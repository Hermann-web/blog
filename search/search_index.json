{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Personal Website","text":"<p>Welcome to Material for MkDocs.</p>"},{"location":"license/","title":"License","text":"<p>MIT License</p> <p>Copyright \u00a9 2016-2023 Martin Donath</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2023/10/02/a-roadmap-for-web-developper/","title":"A Roadmap for Web Development: Lessons and Stories","text":""},{"location":"blog/2023/10/02/a-roadmap-for-web-developper/#chapter-1-html-and-css-crafting-digital-experiences","title":"Chapter 1: HTML and CSS - Crafting Digital Experiences","text":"<p>In the vast landscape of web development, the journey often starts with understanding the language of the web: HTML and CSS. It's more than just syntax and techniques; it's the brush and canvas, where we paint the interfaces of the digital world. I embarked on this journey through an inspiring course that not only taught me the technicalities but also guided me to build my first project, allowing me to witness the magic of transforming code into a visual experience. Link to the course</p>"},{"location":"blog/2023/10/02/a-roadmap-for-web-developper/#chapter-2-javascript-embracing-dynamic-interactions","title":"Chapter 2: JavaScript - Embracing Dynamic Interactions","text":"<p>JavaScript, the language that breathes life into web pages, offers endless possibilities. You can choose to skip it and learn the language in the next course (course 3), as we'll be talking about variables, loops and functions here. But it is always good to learn the basics of a programming language before using any of his frameworks. Link to the course</p>"},{"location":"blog/2023/10/02/a-roadmap-for-web-developper/#chapter-3-javascript-for-web-making-web-pages-dynamic","title":"Chapter 3: JavaScript for Web - Making Web Pages Dynamic","text":"<p>The world of web development thrives on dynamic experiences. This course is the profect continuation in the web dev journey after html, css. On coursera, there is a course that introduces the javascript tools we use to manipulate objects on a web page to make it less static.</p> <p>Towards the end, there's a quiz where you do a project using Node JS (a framework used to run javascript code) but I notice they've adopted a new, simpler format. Link to the course</p>"},{"location":"blog/2023/10/02/a-roadmap-for-web-developper/#the-challenge-testing-time-cloning-a-web-page","title":"The Challenge: Testing Time - Cloning a Web Page","text":""},{"location":"blog/2023/10/02/a-roadmap-for-web-developper/#chapter-4-python-unsupervised-ml-algorithms","title":"Chapter 4: Python: Unsupervised ML Algorithms","text":"<p>My journey expanded beyond web development into the realms of Python. This detour into the world of unsupervised machine learning algorithms was not entirely new. It offered a different perspective, unlocking doors to exploring and analyzing data with a sense of wonder. Link to the course</p>"},{"location":"blog/2023/10/02/a-roadmap-for-web-developper/#chapter-5-php-for-backend-connecting-frontend-to-databases","title":"Chapter 5: PHP for Backend - Connecting Frontend to Databases","text":"<p>The bridge between frontend and databases fascinated me. PHP became my vessel in understanding the interaction between a website and a database. While focusing on MySQL, I explored the installations and syntax, essential for crafting PHP code. Link to reference</p> <p>I must admit, i learning out of pure curiosity, after my first internship in cloud computing. I wanted to know more are a crucial component of the 3-tiers architecture, which is the database and two other reseons as important</p> <ul> <li>i know php is one of the most use tools for backend development</li> <li>i know php integrate very well with html, css</li> <li>i've learned html, css, js before hand</li> </ul> <p>My only experiences with php, for at least years, after learning was about helping every time a student come to be with a problem to fix or a functionnality to add. But after a while, i got into a project where we decided with the team, to use laravel. Laravel is based on php. I got onto it very quick because, even if i have 0 real project experience in php, i've coded in php to help many students. All the tools needed was already on my computer and i've added the first feature assigned to me within a week while the others, took at least 3 weeks to undertand the framework and add the feature assigned to them.</p>"},{"location":"blog/2023/10/02/a-roadmap-for-web-developper/#chapter-6-django-for-backend-in-python-navigating-the-web-development-framework","title":"Chapter 6: Django for Backend in Python - Navigating the Web Development Framework","text":"<p>It's a Python framework for web development. I took part in a course on Coursera to learn about Django architecture (how to create a project, how files are organized, what each file is used for).</p> <p>I learned a lot more about the framework during my internship at Safran than during the apprenticeship. With the bugs to slove and features to be added, I needed to read stackoverflow discussions, try and fail, visit the documentation very often (very rich by the way, cudo to django project team). So the real learning began amidst challenges and practical applications.</p>"},{"location":"blog/2023/10/02/a-roadmap-for-web-developper/#second-internship-web-development","title":"Second internship: Web development","text":"<p>That was the internship i needed to be sure i wasn't just a part time web development learner. My task was to develop a web app that will enable members of a department (i wont say more) to search and access useful document through a simple search engine adpated to their need. The project started as a django project. Before coding, i did design the web app with figma. After, the web app validated, i started the core features.</p> <p>!!!   If i have learn one important thing during the first month, it is to start with the login components if login is a required part of the process.</p> <p>I've developed iteratively the web app. Even though i was told to not waste time on frontend, i could just stand ugly pages. So i was adding css/bootstrap codes of my own. In the second month, i have mainly worked on three backend features</p> <ul> <li>one that give informations about a document. In that part of the app, users can also appli some modifications. By modifications, i'm talking about the classic ones you can make on a word document (change colors of some words, letters, surligne, bold, italic, ...). I've use a js tool as i recall.</li> <li>the frontend of the search engine: As simple of the google search interface. I've removed my styles on this.</li> <li>By the end of the month, i started working on the search engine. Well, the web dev project quickly become a Natural language processing project too. Without going into the details, i was doing NLP with javascript then switched to python to use sklearn methods.</li> </ul>"},{"location":"blog/2023/10/02/a-roadmap-for-web-developper/#chapter-7-bootstrap-streamlining-css-development","title":"Chapter 7: Bootstrap - Streamlining CSS Development","text":"<p>Bootstrap is a framework that makes it easy to add CSS styles to a web page. But, skip to this course if you're working on a project where you need it. You'll learn faster. Same advice as for the next framework: Jquerry. For my part, I needed it to speed up the frontend part of my internship, so I went to the w3schools site. Link to a course I figured long after, tailwing, as a rigid alternative to bootstrap</p>"},{"location":"blog/2023/10/02/a-roadmap-for-web-developper/#chapter-8-jquery-simplifying-javascript","title":"Chapter 8: jQuery - Simplifying JavaScript","text":"<p>While JavaScript is the heart of web interactions, jQuery offered a streamlined path. Mastering it could be an asset in certain scenarios, enhancing the development process. JQuerry is a framework that makes it easy to write javascript code for the web. Some developers don't know Jquerry as well as they know javascript, but in that case it's not always easy to work in a team that doesn't use Jquerry. Link to a course</p>"},{"location":"blog/2023/10/02/a-roadmap-for-web-developper/#chapter-9-ajax-seamlessly-connecting-frontend-and-backend","title":"Chapter 9: Ajax - Seamlessly Connecting Frontend and Backend","text":"<p>Ajax, the invisible thread connecting frontend and backend without the need for page reloads. This technology became essential in my journey, especially in tandem with the Django framework, enabling seamless exchanges and a more interactive web experience. Link to a course</p>"},{"location":"blog/2023/10/02/a-roadmap-for-web-developper/#chapter-10-reactjs-shaping-dynamic-user-experiences","title":"Chapter 10: ReactJS - Shaping Dynamic User Experiences","text":"<p>ReactJS is a NodeJS framework used a lot by frontend developers. A lot of developers learn reactJS, right after html, css and js, which puts them on the right track for front-end development. React Js became a vital part of my skill set during a significant gap year. It was very much in demand and used on the market in 2022. But the real reason I decided to learn ReactJS was for an internship in 2021-2022. My first assignment was to develop a web interface. I had a choice between reactJS and AngularJS. After discussions with the lead dev, we chose ReactJS. Long story short, we wanted the user to have a one-page experience on the application.</p>"},{"location":"blog/2023/10/02/a-roadmap-for-web-developper/#chapter-11-nodejs-unleashing-javascript-on-the-server","title":"Chapter 11: NodeJS - Unleashing JavaScript on the Server","text":"<p>Javascript is well known for the front-end, but was limited to it until the development of NodeJS. NodeJS is a tool for executing JS scripts on the server side, opening doors to multifaceted web development.</p>"},{"location":"blog/2023/10/02/a-roadmap-for-web-developper/#chapter-12-electronjs-exploring-beyond-the-browser","title":"Chapter 12: ElectronJS - Exploring Beyond the Browser","text":""},{"location":"blog/2023/10/02/a-roadmap-for-web-developper/#chapter-13-laravel-embracing-a-new-horizon","title":"Chapter 13: Laravel - Embracing a New Horizon","text":"<p>The journey continues with Laravel, a framework that beckons with new possibilities and a new chapter to explore.</p> <p>Each chapter in this roadmap of web development is more than a course; it's a story, an experience, a transformation that shaped me into the software engineer I am today. The roadmap evolves, the journey continues, and the quest for learning never ceases.</p>"},{"location":"blog/2024/01/02/evolution-of-a-project-from-python-apps-to-a-web-interface/","title":"Evolution of a Project: From Python Apps to a Web Interface","text":"<p>How a solo coder turned a repo full of data scripts into a full-stack web app with beautiful interface in one day ?</p> <p>In the world of development, projects tend to evolve, morphing and reshaping themselves to meet ever-changing needs and preferences. What starts as a collection of Python applications can transform into something entirely different\u2014a cohesive web interface with a backend to match.</p> <p>This evolution was made possible by harnessing a comprehensive tech stack, blending various cutting-edge technologies:</p> <ul> <li>Python: Served as the foundational language for backend logic and functionality.</li> <li>FastAPI: Empowered the backend with rapid server capabilities and smooth API integration.</li> <li>Next.js: Spearheaded the frontend development, providing dynamic and responsive user interfaces.</li> <li>Prisma: Efficiently managed databases, optimizing data operations and queries.</li> <li>Clerk: Handled authentication and user management, ensuring secure and streamlined user experiences.</li> <li>Docker: Ensured modern development practices and facilitated seamless execution across various environments.</li> </ul> <p>This amalgamation of technologies enabled the evolution from fragmented Python apps to a unified, feature-rich web interface, promising an immersive user experience while optimizing efficiency and elegance in development.</p>"},{"location":"blog/2024/01/02/evolution-of-a-project-from-python-apps-to-a-web-interface/#motivation","title":"Motivation","text":"<p>At the end of 2023, in the very last 2 days, I felt a sense of completion. It was time to bring life to a project that had been waiting for attention.</p> <p>Looking back at the same time one year ago \u2014from 2022 to 2023\u2014 I remembered working on statistical tools, learning through coding all methods i've encountered in a dedicated coursera certification. The work is open source on github at Hermann-web/some-common-statistical-methods</p> <p>In the last months of 2023, i've started this blog, as a mean to put on the web what tech tools i learn and how to use them. I rewrite my notes as tutorials i find interesting enough to share, in a way i would want to read them right before having tested them.</p> <p>Before that, i've created an application that has an api, a frontend view then on top of it, a python module that gave birth to a cli tool. I've written a tutorial about it showing how i build it brick by brick and it also open source on github at Hermann-web/simple-file-hosting-with-flask</p> <p>But, as the year ended, a project lingered, a collection of scattered Python apps. This project had evolved into a vision\u2014a web interface combined with a backend, a unified creation born from various ideas.</p>"},{"location":"blog/2024/01/02/evolution-of-a-project-from-python-apps-to-a-web-interface/#consolidating-python-apps","title":"Consolidating Python Apps","text":"<p>Initially, my computer housed several Python applications, each serving a specific purpose. As these apps expanded in functionality, I recognized the potential for a more streamlined experience. That's when I transitioned them into command-line interfaces (CLI apps), offering users a smoother interaction.</p>"},{"location":"blog/2024/01/02/evolution-of-a-project-from-python-apps-to-a-web-interface/#structuring-for-scalability","title":"Structuring for Scalability","text":"<p>However, as the collection grew further, the need for organization became apparent. Inspired by the structured approach of ETL (Extract, Transform, Load) in data engineering, I restructured the repository, employing a folder hierarchy to enhance manageability.</p>"},{"location":"blog/2024/01/02/evolution-of-a-project-from-python-apps-to-a-web-interface/#the-resurrection-of-dev-notes-markdown-mkdocs-and-docker","title":"The Resurrection of Dev Notes: Markdown, MkDocs, and Docker","text":"<p>This restructuring revelation extended beyond just applications. I figured i've gone through a similar for my development notes. These notes\u2014ranging from syntax references to frequently used functions and tutorials\u2014were upgraded significantly. Migrating from plain text to Markdown format, these notes underwent a transformation. Leveraging the prowess of MkDocs and Docker, these notes metamorphosed into a dynamic web application, offering a seamless browsing experience. This gave me an idea for my small python projects too.</p>"},{"location":"blog/2024/01/02/evolution-of-a-project-from-python-apps-to-a-web-interface/#the-expansion-taking-projects-online","title":"The Expansion: Taking Projects Online","text":"<p>Embracing this analogy of transformation, it became evident that bringing these smaller python projects to the web was the next logical step. This expansion wasn't merely about accessibility; it was about a richer, more interactive experience. While I could have chosen familiar tools like Flask and HTML, my inclination toward exploring new frameworks led me elsewhere: Fast and modern tools for both the server (FastAPI/Tornado) and the ui (ReactJS/NextJS)</p>"},{"location":"blog/2024/01/02/evolution-of-a-project-from-python-apps-to-a-web-interface/#framework-selection","title":"Framework Selection","text":"<p>Seeking equilibrium between rapid backend operations and a responsive frontend, this quest led me to opt for FastAPI for the server and Next.js for the frontend. These choices aligned with my desire for speed and versatility, given FastAPI's rapid server capabilities and Next.js's dynamic UI.</p>"},{"location":"blog/2024/01/02/evolution-of-a-project-from-python-apps-to-a-web-interface/#starting-with-the-backend","title":"Starting with the backend","text":"<p>With prior experience in FastAPI, integrating one of my Python projects as starter was a swift process.  </p> <p>I opted for poetry for dependency management, though encountered a hiccup when it failed to create a virtual environment. Resorting to traditional methods to create one, I used poetry to manage packages.</p> <p>Then, i've added my packages with poetry: fastapi and uvicorn. then, black and isort, as dev dependencies. In comparison to pip, poetry or pipenv let you separate dev dependencies and you can ever group dependencies or add command to be run with poetry.</p> <p>See more on python package managers practical comparison here</p> <p>Despite encountering issues, tweaking the <code>pyproject.toml</code> scripts addressed the obstacles.</p> <pre><code># [tool.poetry.scripts]\n# format = \"isort . &amp;&amp; black .\"\n\n# [tool.poetry.scripts]\n# start = \"uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload\"\n</code></pre> <p>I've just made progress on the Swagger docs!</p> <p>Check out the snapshot of the Swagger documentation below:</p> <p></p> <p>The existing endpoints are operational, and to ensure smooth integration before introducing new ones, I've taken a step to containerize the application and connect the backend with the frontend.</p>"},{"location":"blog/2024/01/02/evolution-of-a-project-from-python-apps-to-a-web-interface/#embracing-docker-for-modernization","title":"Embracing Docker for Modernization","text":"<p>Docker-compose emerged as my choice for modern development practices and to ensure seamless project execution across different environments. After locally testing the API with uvicorn, I streamlined the setup, crafting a <code>docker-compose.yml</code> file.</p> <p>Before creating the <code>docker-compose.yml</code>, i've exported my requirements into a file so i could use pip instead of poetry inside the container</p> <pre><code>poetry export -f requirements.txt --output requirements.txt --without-hashes\n</code></pre> <p>Then, i've refactorised <code>docker-compose.yml</code>, into a <code>Dockerfile</code> use by <code>docker-compose.yml</code>. Having a separate Dockerfile allowed me use it in the root folder of the project so i can run the <code>Dockerfile</code>s from client and server in a simple <code>docker-compose.yml</code></p> <p>So, i had an architecture like this</p> <pre><code>project-root/\n\u2502\n\u251c\u2500\u2500 client/                 # Next.js client folder\n\u2502\n\u251c\u2500\u2500 server/                 # FastAPI server folder\n\u2502   \u251c\u2500\u2500 app/                # FastAPI application logic\n\u2502   \u251c\u2500\u2500 requirements.txt    # Server dependencies\n\u2502   \u251c\u2500\u2500 Dockerfile          # Dockerfile for FastAPI server\n\u2502   \u251c\u2500\u2500 docker-compose.yml  # Docker Compose configuration\n\u2502   \u251c\u2500\u2500 .gitignore          # Server-specific .gitignore\n\u2502   \u251c\u2500\u2500 pyproject.toml      # poetry dependency file\n\u2502   \u251c\u2500\u2500 poetry.lock         # poetry lock file\n\u2502   \u2514\u2500\u2500 ...                \n\u2502\n\u251c\u2500\u2500 docker-compose.yml      # Docker Compose configuration\n\u2514\u2500\u2500 readme.md               # Project README or documentation\n</code></pre> <p>I've added a module in my fastapi app and tested on swagger on the browser. It was working fine. So my version 0 will be ready righr after the setup of the nextjs app.</p>"},{"location":"blog/2024/01/02/evolution-of-a-project-from-python-apps-to-a-web-interface/#the-frontend-odyssey","title":"The Frontend Odyssey","text":"<p>Choosing to build a bespoke tool, I commenced with a command to create a Next.js project. The CLI app provided an interactive interface, offering language options (js vs ts), formatting tool choices and more.</p>"},{"location":"blog/2024/01/02/evolution-of-a-project-from-python-apps-to-a-web-interface/#navigating-the-ui-dilemma","title":"Navigating the UI Dilemma","text":"<p>In my quest to fashion a unique frontend, fate introduced me to DevToolboxWeb, a project resonating with my aspirations.</p> <p>Despite deliberations on building from scratch, leveraging their UI became my choice for multiple reasons:</p> <ul> <li>An opportunity to contribute to an interesting open source project</li> <li>If i were to build from scratch, I would have done it like them</li> <li>I was on a clock ! And i prefer build-from-template over build-from-scratch when it comes to frontend</li> </ul>"},{"location":"blog/2024/01/02/evolution-of-a-project-from-python-apps-to-a-web-interface/#merging-frontend-and-backend","title":"Merging Frontend and Backend","text":"<p>While DevToolboxWeb's UI aligned with my vision, it lacked a backend\u2014a void I intended to fill. Forking their repository, I ventured into integrating my FastAPI backend with their feature-rich UI, striving for a synergy of functionality and aesthetics. That's why i've created a fork of their code and add a server into it.</p>"},{"location":"blog/2024/01/02/evolution-of-a-project-from-python-apps-to-a-web-interface/#integrating-fastapi-and-nextjs","title":"Integrating FastAPI and Next.js","text":"<p>To bridge the gap between FastAPI and Next.js, I explored digitros/nextjs-fastapi from the Next.js and FastAPI starter templates on Vercel. This resource offered insights into integrating the next js based frontend and fastapi backend seamlessly.</p>"},{"location":"blog/2024/01/02/evolution-of-a-project-from-python-apps-to-a-web-interface/#culmination-a-glimpse-of-the-horizon","title":"Culmination: A Glimpse of the Horizon","text":"<p>Projects in development constantly change and borrow from each other. My journey from scattered Python apps to a unified web interface has been an ongoing quest for efficiency and elegance.</p> <p>As I combine backend strength with frontend finesse, the vision of a user-friendly toolbox gets closer. And in this pursuit, the journey itself is as rewarding as the destination.</p>"},{"location":"blog/2024/03/21/security-practices-for-authentication-a-guide-for-developers/","title":"Security Practices for Authentication: A Guide for Developers","text":""},{"location":"blog/2024/03/21/security-practices-for-authentication-a-guide-for-developers/#introduction","title":"Introduction","text":"<p>In the realm of digital security, authentication plays a critical role in safeguarding user data and system integrity. However, implementing robust authentication mechanisms is often a daunting task, fraught with potential vulnerabilities and pitfalls. This guide aims to shed light on common authentication practices, highlighting both their strengths and weaknesses, while providing insights into best practices for developers.</p>"},{"location":"blog/2024/03/21/security-practices-for-authentication-a-guide-for-developers/#authentication-methods-overview","title":"Authentication Methods Overview","text":"<p>Authentication methods vary widely in their implementation details, security levels, and use cases. Below, we briefly discuss some common methods before delving into a detailed comparison.</p> <ul> <li> <p>Clear Text Authentication: Involves sending credentials (username and password) without encryption. This method is highly insecure and susceptible to eavesdropping, especially by Man-in-the-Middle (MITM) attackers.</p> </li> <li> <p>Basic Authentication: Utilizes a Base64 encoding scheme to transmit credentials. While slightly better than clear text, it's still easily decodable and not recommended for use without HTTPS.</p> </li> <li> <p>Session Tokens: A server-generated token is sent to the client, which stores it and includes it in subsequent requests. This method requires server-side storage to validate sessions.</p> </li> <li> <p>JSON Web Tokens (JWT): A compact, URL-safe means of representing claims between two parties. JWTs can be encrypted (JWE) for added security and are typically used in stateless authentication scenarios.</p> </li> <li> <p>Bearer Tokens: Essentially a type of access token, \"Bearer\" refers to the method of sending the token in HTTP headers. Bearer tokens can encapsulate various forms of authentication, including JWTs.</p> </li> </ul>"},{"location":"blog/2024/03/21/security-practices-for-authentication-a-guide-for-developers/#detailed-comparison","title":"Detailed Comparison","text":"<p>To better understand the nuances between these methods, refer to the comparison table below:</p> Feature / Method Clear Text Basic Authentication Session Tokens JWT Bearer Tokens Encryption No Base64 (Not secure) Optional Yes* Yes* HTTPS Recommended Yes Yes Yes Yes Yes Client Storage No No Cookies Cookies/Header Header Server Storage No No Yes No No Vulnerability to MITM High High Low (with HTTPS) Low (with HTTPS) Low (with HTTPS) Statefulness Stateless Stateless Stateful Stateless Stateless Expiration Control No No Yes Yes Yes Logout Capability N/A N/A Yes No** Yes** Revocation Capability N/A N/A Yes No** Yes** Complexity Low Low Medium Medium Medium Use Case Avoid Simple Auth Web Applications APIs/Microservices APIs/Microservices <p>* While JWT and Bearer tokens can use encryption, JWT typically involves signing rather than encrypting. Encryption is possible with JWE (JSON Web Encryption).</p> <p>** Logging out or revoking JWTs and Bearer Tokens without server-side tracking involves client-side action to discard the token. However, server-side mechanisms can be implemented for more control, such as token blacklisting or using short-lived tokens with a refresh mechanism.</p> <p>This table serves as a guide to choosing the right authentication method based on your application's needs. For instance:</p> <ul> <li>Clear Text: Highly insecure and should be avoided. Use HTTPS to protect data in transit.</li> <li>Basic Authentication: Simple but requires HTTPS for security. Suitable for simple authentication needs.</li> <li>Session Tokens: Ideal for web applications where state can be maintained on the server.</li> <li>JWT (JSON Web Tokens): Best suited for stateless APIs or microservices, allowing for scalable and flexible authentication mechanisms.</li> <li>Bearer Tokens: Similar to JWT, used for authenticating requests in APIs and microservices, providing a secure method for client-server communication.</li> </ul> <p>Each method has its context where it excels, as well as its drawbacks. The choice among them depends on various factors such as security requirements, the architecture of the application, scalability needs, and user experience considerations.</p> <p>By considering the features outlined in the table, developers and architects can make informed decisions that balance security and functionality, ensuring that their applications are both secure and user-friendly.</p>"},{"location":"blog/2024/03/21/security-practices-for-authentication-a-guide-for-developers/#security-enhancements-and-best-practices","title":"Security Enhancements and Best Practices","text":"<p>Beyond choosing the right authentication method, ensuring the security of the authentication process involves several best practices:</p> <ol> <li> <p>Always Use HTTPS: Regardless of the authentication method, securing the channel with HTTPS is critical to prevent eavesdropping and MITM attacks.</p> </li> <li> <p>Secure Token Storage: When tokens are used (Session, JWT, Bearer), secure storage on the client side is essential. For web applications, HttpOnly cookies can provide added security by preventing client-side script access to the token.</p> </li> <li> <p>Proper Encryption and Hashing: For any method involving passwords or sensitive information, ensure that data is encrypted during transit and securely hashed (using algorithms like bcrypt or Argon2) when stored.</p> </li> <li> <p>Regularly Update Security Measures: With the evolving nature of threats and security standards, regularly reviewing and updating authentication and security practices is vital.</p> </li> </ol> <p>By carefully selecting an appropriate authentication method and adhering to security best practices, you can significantly enhance the security posture of your applications.</p>"},{"location":"blog/2024/03/21/security-practices-for-authentication-a-guide-for-developers/#advanced-techniques-oauth2-and-machine-learning-in-authentication","title":"Advanced Techniques: OAuth2 and Machine Learning in Authentication","text":"<ul> <li>OAuth2: Implements token-based authentication, frequently refreshing tokens to enhance security.</li> <li>Machine Learning: Can differentiate between legitimate users and attackers by analyzing behavioral patterns, thus improving security measures.</li> </ul>"},{"location":"blog/2024/03/21/security-practices-for-authentication-a-guide-for-developers/#mobile-app-considerations","title":"Mobile App Considerations","text":"<ul> <li>Token Storage: Securely store tokens using platform-specific secure storage solutions to protect against unauthorized access, including on rooted devices.</li> <li>Data Encryption: Essential for protecting sensitive information stored within mobile applications.</li> </ul>"},{"location":"blog/2024/03/21/security-practices-for-authentication-a-guide-for-developers/#device-identification-who-vs-what","title":"Device Identification: \"Who\" vs. \"What\"","text":"<ul> <li>Who: Identifies the user, typically through tokens representing their authenticated session.</li> <li>What: Refers to the device, recognized via identifiers like IP and MAC addresses, adding an additional layer of security by validating not just who is accessing the system but also from which device.</li> </ul>"},{"location":"blog/2024/03/21/security-practices-for-authentication-a-guide-for-developers/#conclusion","title":"Conclusion","text":"<p>Secure authentication is a multifaceted challenge that requires a comprehensive approach, including secure transmission of credentials, robust password storage practices, and considerations for both user and device authentication. By implementing the recommendations outlined in this document, developers and system architects can significantly enhance the security of their authentication mechanisms.</p>"},{"location":"blog/2023/10/24/unveiling-the-code-chronicles-navigating-software-licenses/","title":"Unveiling the Code Chronicles: Navigating Software Licenses","text":""},{"location":"blog/2023/10/24/unveiling-the-code-chronicles-navigating-software-licenses/#introduction","title":"Introduction","text":"<p>Ever glanced at those cryptic licenses in software projects\u2014like the <code>MIT License</code>, <code>Apache License</code>, or <code>GPL</code>\u2014and wondered, 'What do they mean for me?'</p> <p>Whether you're a code wizard or just dipping your toes in tech, understanding these licenses is like decoding a secret language. Which one suits your project best? What's the deal when you borrow or tweak someone else's code?</p> <p>This document unravels the mystery, diving into the world of software licenses. Discover their quirks, choose wisely between the MIT, Apache, or GPL licenses, and learn the ropes for handling borrowed or tweaked code. Get ready to crack the code of software licenses!</p> <p>Please note that the information provided here is for educational purposes only and should not be construed as legal advice.</p>"},{"location":"blog/2023/10/24/unveiling-the-code-chronicles-navigating-software-licenses/#motivation-for-choosing-licenses","title":"Motivation for Choosing Licenses","text":"<p>Understanding software licenses is crucial for project development. Choosing a license depends on factors like project goals, desired openness, collaboration, and legal obligations when utilizing or modifying a project.</p>"},{"location":"blog/2023/10/24/unveiling-the-code-chronicles-navigating-software-licenses/#what-to-choose-for-your-project","title":"What to Choose for Your Project?","text":"<ul> <li>Consider the project's goals, community involvement, and desired freedoms for users when selecting a license.</li> <li>Assess the implications of each license on collaboration, distribution, and derivative works.</li> </ul>"},{"location":"blog/2023/10/24/unveiling-the-code-chronicles-navigating-software-licenses/#actions-with-copiedmodified-projects","title":"Actions with Copied/Modified Projects","text":"<ul> <li>Adhering to the original license terms is crucial when using, modifying, or distributing a project.</li> <li>Respect the license obligations to the original authors while exercising your rights under the chosen license.</li> </ul>"},{"location":"blog/2023/10/24/unveiling-the-code-chronicles-navigating-software-licenses/#open-source-licenses","title":"Open Source Licenses","text":"<p>A software is <code>Open source</code> when his code source is available (Not quite the same for llm though). It can be permissive (MIT, Apache, ...), copyleft (you should distribute under the same license and/or keep track of the modification and/or not remove the claim in the license).</p>"},{"location":"blog/2023/10/24/unveiling-the-code-chronicles-navigating-software-licenses/#permissive-licenses","title":"Permissive Licenses","text":"<p>They allow anyone to use, modify, distribute, and sell the software with few restrictions. Though you're not obligated to share modifications, distributing in source code form implies sharing. Here are the most common exemples.</p> <code>MIT License:</code> <code>Apache License / Microsoft Public License (Ms-PL)</code> <code>BSD License</code> <ul> <li>Allows users <code>U1</code> to use, modify, and distribute (release to user <code>U2</code>) the software ( which a part is a derivative from <code>U0</code> work) for any purpose, including commercial use, without having to share ( to user <code>U2</code>) their modifications or contributions</li> <li>Essentially, it offers considerable freedom.</li> </ul> <ul> <li>Permits users <code>U1</code> to use, modify, and distribute the software without having to share their modifications with <code>U2</code></li> <li>Requires users to provide attribution (a mention of the original author' name <code>U0</code>) and a copy of the license when distributing the software to <code>U2</code>.</li> <li>Includes a patent license, ensuring users won't be sued for patents related to the software.</li> <li>So basically, keep the original author's name and no patent (warranties) problem</li> </ul> <ul> <li>Allows redistribution and modification</li> <li>Requires that the original copyright notice and disclaimer be retained.</li> <li>Essentially, it ensures the original author (<code>U0</code> or <code>U1</code>) are not legally responsible for any issues arising from User <code>U2</code>'s use.</li> </ul>"},{"location":"blog/2023/10/24/unveiling-the-code-chronicles-navigating-software-licenses/#copyleft-licenses","title":"Copyleft Licenses","text":"<p>They</p> <ul> <li>Impose restrictions on how the software can be used and distributed.</li> <li>Require that any modifications or derivative works (In programming for example, the part of a software that use/modify a copyleft licenced code) of the software also be released under the same license, maintaining the software's open-source nature.</li> </ul> <p>This helps ensure that the software remains open source and that any improvements made to it are shared with the community. Examples of copyleft licenses include the GNU General Public License (GPL) and the Lesser General Public License (LGPL) or AGPL or  SSPL or BSD (Berkeley Software Distribution) or MPL ( Mozillah Public Licence) or Eclipse Public License (EPL) or CC ( Creative Common)</p> <p>Examples include</p> <code>LGPL (Lesser General Public License)</code> <code>GPL (General Public License)</code> <code>AGPL (Affero General Public License)</code> <code>SSPL (Server Side Public License)</code> <code>MPL (Mozilla Public License) / EPL (Eclipse Public License)</code> <code>Others</code> <ul> <li>Requires users (<code>U1</code>) to share modifications and contributions (only the part of the software that use a LGPL licenced code) to the software under the same license (LGPL).</li> <li>Mandates <code>U1</code> making the source code available to anyone (<code>U2</code>) receiving the software and providing a copy of the license along with it.</li> <li>So basically, requires that modifications to the code be released under the same license.</li> </ul> <ul> <li>Extend LGPL rules</li> <li>Require the entire codebase become GPL licenced if any part uses GPL-licensed code, ensuring the software remains open source.</li> <li>Require the entire codebase should to be shared with user <code>U2</code> when distributing the software</li> <li>So basically, requires that any software that use a GPL licenced code become a GPL licence software that share its codebase.</li> </ul> <ul> <li>Extends GPL's rules to apply even when software is accessed via a network.</li> </ul> <ul> <li>Extend AGPL rules</li> <li>Requires sharing the entire codebase (at least, containing one SSPL-licensed code) with the public (not only user <code>U2</code> whose you distribute it to) when using SSPL-licensed code.</li> <li>Mongo DB uses this license.</li> </ul> <ul> <li>Similar to LGPL, these licenses require modified code portions to be released under the same license but not the entire program. So it allows linking with non-free software</li> <li>Similar to LGPL, requires that modifications to the code be released under the same license</li> </ul> <ul> <li>Microsoft Reciprocal License (Ms-RL)</li> <li>LGPL + disclaimer</li> </ul>"},{"location":"blog/2023/10/24/unveiling-the-code-chronicles-navigating-software-licenses/#proprietary-licenses","title":"Proprietary Licenses","text":"<p>Those are the most restrictive, controlling software use and distribution and may involve fees. Examples include Microsoft Windows EULA, Adobe Photoshop License Agreement, etc.</p>"},{"location":"blog/2023/10/24/unveiling-the-code-chronicles-navigating-software-licenses/#dual-licensing","title":"Dual Licensing","text":"<p>Dual licenses use multiple licenses for different parts of software. Challenges arise when incorporating GPL-licensed components as they may necessitate the entire product to be GPL-licensed and shared accordingly with <code>U2</code> (those who use your product in a compiled form), unless a separation like mere-aggregation is possible. <code>Examples like WordPress, which is GPL-licensed, but premium themes might use different licenses.</code></p>"},{"location":"blog/2023/10/24/unveiling-the-code-chronicles-navigating-software-licenses/#gpl-license-challenges-and-tricks","title":"GPL license: Challenges and Tricks","text":"<p>As a GPL licence requires that any software that use a GPL licenced code become a GPL licence software that share its codebase, these strict obligations can potentially affect the licensing of the entire project.</p> <p>But if you use a non-free program in your codebase or just don't want to share your codebase, a GPL part is a bug. That's why alternatives like MLP or EPL or LGPL are used to \"link\" a GPL.</p> <p>Another trick is to claim a \"mere aggregation.\"  of a (derivative of a gpl licenced component) and (anothers components).     - Like Android OS which use a modified linux kernel but as the kernel runs at the top of a android os, only the modified linux kernel become gpl. So they have a dual ( Apache licenced Android-OS + Gpl licenced modified-linux-kernel )     - Or the difference between worpress ( under gpl) vs the themes ( under gpl) (because they use or modify worpress core) vs anothers that don't use or modify the core</p>"},{"location":"blog/2023/10/24/unveiling-the-code-chronicles-navigating-software-licenses/#dual-licensing-and-open-source-preferences","title":"Dual Licensing and Open Source Preferences","text":"<p>This section delves deeper into the dynamics of dual licensing, open source preferences, and their implications within the software development sphere:</p>"},{"location":"blog/2023/10/24/unveiling-the-code-chronicles-navigating-software-licenses/#dual-licensing-consideration","title":"Dual Licensing Consideration","text":"<p>Some view dual licensing as problematic. Users often lean towards open source due to its security, global expertise utilization, and customization capabilities. However, integrating open-source solutions can pose challenges.</p>"},{"location":"blog/2023/10/24/unveiling-the-code-chronicles-navigating-software-licenses/#open-source-and-security","title":"Open Source and Security","text":"<p>Open-source solutions are favored for security reasons and customization but might involve integration challenges. Notable examples include:     - OpenStack: Uses a permissive Apache license.     - Mozilla Firefox: Utilizes a permissive MPL (Mozilla Public License).     - Linux: Governed by GPL (General Public License), chosen for rapid development and innovation adoption.</p>"},{"location":"blog/2023/10/24/unveiling-the-code-chronicles-navigating-software-licenses/#proprietary-vs-open-source","title":"Proprietary vs. Open Source","text":"<p>While some prefer proprietary solutions for security and maintenance contracts, others argue in favor of copyleft (e.g., LGPL or GPL). They believe copyleft licenses compel companies to share their work, favoring open source in competition.</p>"},{"location":"blog/2023/10/24/unveiling-the-code-chronicles-navigating-software-licenses/#software-composition-analysis-sca-tools","title":"Software Composition Analysis (SCA) Tools","text":"<p>SCA tools automatically scan projects for license compliance, ensuring adherence to licensing requirements.</p>"},{"location":"blog/2023/10/24/unveiling-the-code-chronicles-navigating-software-licenses/#open-source-policy","title":"Open Source Policy","text":"<p>Establishing an open-source policy involves:     - Tracking external code licenses and their respective requirements.     - Utilizing SCA tools for scanning purposes.     - Maintaining records of license purchases, expiration dates, and repository registrations.     - Noting that expired licenses may place code in the public domain (if unaltered), while patents and trademarks remain.     - Keeping up to date with licence change</p>"},{"location":"blog/2023/10/24/unveiling-the-code-chronicles-navigating-software-licenses/#license-management","title":"License Management","text":"<p>Constant vigilance is required to stay updated with license changes and adhere to compliance standards.</p>"},{"location":"blog/2023/10/24/unveiling-the-code-chronicles-navigating-software-licenses/#conclusion-the-significance-of-open-source-and-challenges-with-dual-licensing","title":"Conclusion: The Significance of Open Source and Challenges with Dual Licensing","text":"<p>Open source licenses come in various forms, ranging from permissive ones like MIT and Apache to copyleft licenses where you should distribute under the same license and/or keep track of the modification abd/or not remove the claim in the license.  For example, for GPL, if you distribute ( give a software which one part of the code source is under gpl), you should give the source too. It's crucial to understand their implications for your project and how they impact collaboration, distribution, and legal obligations.</p> <p>Beside open source and proprietary licenses, the dual licences has emerged, combining multiples licenses for differents parts of the same project.</p> <p>So, dual licensing is a strategy used to address different needs within a project, but it can introduce complexities and limitations. While open source fosters collaboration, innovation, and community-driven development, navigating through licensing choices requires careful consideration and understanding.</p> <p>Please remember that the information provided in this document is for educational purposes only and should not be considered legal advice. It's essential to consult with legal professionals for specific legal guidance related to software licenses and intellectual property matters.</p>"},{"location":"blog/2023/10/08/seaborn-in-practice-syntax-and-guide/","title":"Seaborn in Practice: Syntax and Guide","text":"<p>Seaborn is a powerful data visualization library in Python that provides a high-level interface for drawing attractive and informative statistical graphics. One common misconception about Seaborn and programming in general is the necessity to remember all the syntax. In reality, it's more about understanding the tool's capabilities and how to leverage its functions to visualize data effectively.</p> So, what can i do exactly with seaborn ?"},{"location":"blog/2023/10/08/seaborn-in-practice-syntax-and-guide/#import-the-library","title":"import the library","text":"<pre><code>import seaborn as sns\nsns.set(style=\"whitegrid\")\n</code></pre>"},{"location":"blog/2023/10/08/seaborn-in-practice-syntax-and-guide/#load-some-dataset","title":"Load some dataset","text":"<p>we will be using a dataset containing tips from a restaurant. We will know more more about it down the road</p> <pre><code>df = sns.load_dataset(\"tips\")\n</code></pre> <p>Let's see a preview of the dataset</p> <pre><code>df\n</code></pre> total_bill tip sex smoker day time size 0 16.99 1.01 Female No Sun Dinner 2 1 10.34 1.66 Male No Sun Dinner 3 2 21.01 3.50 Male No Sun Dinner 3 3 23.68 3.31 Male No Sun Dinner 2 4 24.59 3.61 Female No Sun Dinner 4 ... ... ... ... ... ... ... ... 239 29.03 5.92 Male No Sat Dinner 3 240 27.18 2.00 Female Yes Sat Dinner 2 241 22.67 2.00 Male Yes Sat Dinner 2 242 17.82 1.75 Male No Sat Dinner 2 243 18.78 3.00 Female No Thur Dinner 2 <p>We have an overview of the data but it is not enough. We will do a broad visualisation of the columns with pairplot.</p>"},{"location":"blog/2023/10/08/seaborn-in-practice-syntax-and-guide/#pairplot","title":"Pairplot","text":"<p>A quick way to visualize relationships in a dataset is by using the method <code>pairplot</code>.</p> <pre><code>sns.pairplot(df)\n</code></pre> Result <p></p> <p>You can see here, we have a table of graphs. The 3 rows and 3 columns correpond to the 3 numerical values in out dataset: <code>tip</code>, <code>total_bill</code> and <code>size</code> In each cell, one column is plot against another:</p> <ul> <li>In the diagonals, a column is plotted againt itselt and you have histograms</li> <li>In the anti-diagonals, 2 columns are plotted against each other and you have a scatterplot</li> </ul> <p>You can also notice only 3 columns of our dataframe is here. It is because they contain numerical values. The 3 others (<code>sex</code>, <code>smoker</code>, <code>day</code> and <code>time</code>) are</p>"},{"location":"blog/2023/10/08/seaborn-in-practice-syntax-and-guide/#histogram","title":"Histogram","text":"<p>Histograms are used both in univariate statistics and multivariate statistics To display the average notes by gender, you can use a bar plot:</p> <pre><code>import seaborn as sns\n# group by gender, then get the column \"notes\" then, compute the mean of notes in a group\ndf1 = df.groupby('gender')[['notes']].mean().reset_index()\nsns.set(style=\"whitegrid\")\n# show a barplot of out new dataframe (mean_notes = fct(gender))\nax = sns.barplot(x=\"gender\", y=\"notes\", data=df1)\n</code></pre> <p>Counting occurrences can be visualized using a categorical plot:</p> <pre><code>sns.catplot(x='gender', kind='count', data=ratings_df)\n</code></pre> <p>You can extend this to visualize counts by gender and skin color:</p> <pre><code>sns.catplot(x='gender', hue='couleur', kind='count', data=ratings_df)\n</code></pre> <p>Further stratifying by region:</p> <pre><code>sns.catplot(x='gender', hue='couleur', row='region', kind='count', data=ratings_df, height=3, aspect=2)\n</code></pre>"},{"location":"blog/2023/10/08/seaborn-in-practice-syntax-and-guide/#scatterplot","title":"Scatterplot","text":"<p>Scatterplots offer a powerful way to visualize relationships between two variables:</p> <p>To represent points based on 'eval' as a function of 'age':</p> <pre><code>ax = sns.scatterplot(x='age', y='eval', data=ratings_df)\n</code></pre> <p>You can distinguish points by gender using different colors:</p> <pre><code>ax = sns.scatterplot(x='age', y='eval', hue='sex', data=ratings_df)\n</code></pre> <p>For more complex visualizations involving multiple categorical variables:</p> <pre><code>sns.relplot(x=\"age\", y=\"eval\", hue=\"sex\", row=\"region\", data=ratings_df, height=3, aspect=2)\n</code></pre> <p>Including regression lines on scatterplots:</p> <pre><code>sns.lmplot(data=ratings_df, x=\"var1\", y=\"var2\", height=5, aspect=1.5)  # Height 5, width 1.5 times larger than height\n</code></pre> <p>Creating scatter plots with histograms for marginal distributions:</p> <pre><code>sns.jointplot(data=df, x=\"var1\", y=\"var2\", height=3.5)\n</code></pre>"},{"location":"blog/2023/10/08/seaborn-in-practice-syntax-and-guide/#boxplot","title":"Boxplot","text":"<p>Boxplots provide a visual summary of the distribution of data:</p> <p>To view the average ages and percentiles at 5% and 95%:</p> <pre><code>sns.boxplot(ratings_df['age'], orient='v')\n</code></pre> <p>Visualizing the average notes and percentiles for each gender:</p> <pre><code>ax = sns.boxplot(x='sexe', y='notes', data=ratings_df)\n</code></pre> <p>Further stratifying data for insights:</p> <pre><code>df[\"xxx_grp\"] = pd.cut(df.xxx, [18, 30, 40, 50, 60, 70, 80])  # Creating age strata\nsns.boxplot(x=\"xxx_grp\", y=\"xxx\", hue=\"yyy\", data=df)  # Optional hue for differentiation\n</code></pre>"},{"location":"blog/2023/10/08/seaborn-in-practice-syntax-and-guide/#distribution-plot","title":"Distribution Plot","text":"<p>Understanding the distribution of data:</p> <pre><code>ax = sns.distplot(ratings_df['notes'], kde=False)\n</code></pre> <p>Analyzing note distribution by gender:</p> <pre><code>sns.distplot(ratings_df[ratings_df['sexe'] == 'female']['eval'], color='green', kde=False)\nsns.distplot(ratings_df[ratings_df['sexe'] == 'male']['eval'], color=\"orange\", kde=False)\nplt.show()\n</code></pre>"},{"location":"blog/2023/10/08/seaborn-in-practice-syntax-and-guide/#heatmap","title":"Heatmap","text":"<p>Utilizing heatmaps to visualize numerical data:</p> <pre><code>corr = new_df.corr()  # Calculating feature correlations\nax = sns.heatmap(corr, vmin=0, vmax=1, cmap=\"YlGnBu\", annot=True)\nplt.savefig('seabornPandas.png')\nplt.show()\n</code></pre>"},{"location":"blog/2023/10/08/seaborn-in-practice-syntax-and-guide/#conclusion","title":"Conclusion","text":"<p>These examples showcase how Seaborn can be effectively utilized for various visualization needs without the necessity to memorize all the syntax.</p> <p>Understanding the basic syntax and functionality of Seaborn allows you to explore various plots and graphs that suit your data analysis requirements. Through simple examples and by focusing on the visual representation of data, you can gain deeper insights without the burden of remembering intricate details.</p> <p>Remember, Seaborn is designed to assist in the visual exploration of your data, offering a wide range of options for customizing and fine-tuning plots to suit your specific needs.</p> <p>Experiment with different plot types and functionalities to better understand the story your data has to tell. And don't hesitate to refer to the documentation and various online resources available to enrich your understanding and application of Seaborn.</p> <p>Let the visualization journey begin, and may your data tell its story vividly through Seaborn!</p>"},{"location":"blog/2023/11/14/flask-based-file-hosting-web-app--api--python-module--cli-app/","title":"Flask based File Hosting (web app & api & python module & cli app)","text":"<p>This guide will walk you through creating a basic file hosting web application using Flask, a lightweight web framework for Python. The application will include features such as user login, file uploads, and file listing. We'll also explore adding a simple API for interacting with the application.</p>"},{"location":"blog/2023/11/14/flask-based-file-hosting-web-app--api--python-module--cli-app/#prerequistes","title":"Prerequistes","text":"<ul> <li>python &gt;=3.9</li> </ul>"},{"location":"blog/2023/11/14/flask-based-file-hosting-web-app--api--python-module--cli-app/#setup-environment","title":"Setup Environment","text":"<ol> <li>Create a <code>requirements.txt</code> file:</li> </ol> <pre><code>python-slugify\npython-dotenv\nFlask~=2.0.1\n</code></pre> <ol> <li>Set up a Python virtual environment:</li> </ol> <p>Open your terminal and follow these steps</p> <code>For Linux &amp; Mac</code> <code>For Windows</code> <pre><code>cd path/to/folder\n\n# check the python version and localisation\npython -V\nwhich python\n\n# create the env\npython -m venv venv\n\n# activate the env\n./venv/bin/activate\n\n# check the python version and localisation\npython -V\nwhich python\n\n# install requirements\npip install -r requirements.txt\n</code></pre> <pre><code>cd path/to/folder\n\n# check the python version and localisation\npython -V\nwhere.exe python\n\n# create the env\npython -m venv venv\n\n# activate the env\n./venv/Scripts/activate\n\n# check the python version and localisation\npython -V\nwhere.exe python\n\n# install requirements\npip install -r requirements.txt\n</code></pre>"},{"location":"blog/2023/11/14/flask-based-file-hosting-web-app--api--python-module--cli-app/#constants-configuration","title":"Constants Configuration","text":"<p>Create a <code>constants.py</code> file:</p> <pre><code>import os\nfrom pathlib import Path\nfrom dotenv import load_dotenv\n\nUPLOAD_FOLDER = Path('uploads')\nUPLOAD_FOLDER.mkdir(exist_ok=True)\n\nDATA_FILE = Path('data.json')\n\nDEBUG = False  # Set this to True in the development environment\n\n# Load environment variables from .env file\nload_dotenv()\n\nif not DEBUG:\n    USERNAME = os.getenv(\"S_USERNAME\")\n    PASSWORD = os.getenv(\"S_PASSWORD\")\n    FLASK_SECRET_KEY = os.getenv(\"FLASK_SECRET_KEY\")\nelse:\n    USERNAME = \"my-username\"\n    PASSWORD = \"my-password\"\n    FLASK_SECRET_KEY = 'my-secret-key'\n\nassert USERNAME\nassert PASSWORD\nassert FLASK_SECRET_KEY\n</code></pre>"},{"location":"blog/2023/11/14/flask-based-file-hosting-web-app--api--python-module--cli-app/#creating-the-flask-app","title":"Creating the Flask App","text":"<p>Create a file named <code>app.py</code> and set up the initial Flask app:</p> <pre><code>from datetime import datetime, timedelta\nfrom functools import wraps\nimport json\nimport os\nimport mimetypes\nfrom slugify import slugify\nfrom flask import Flask, render_template, request, redirect, send_from_directory\nfrom flask import session, jsonify\nimport constants\n\napp = Flask(__name__)\napp.config['UPLOAD_FOLDER'] = constants.UPLOAD_FOLDER\napp.config['DATA_FILE'] = constants.DATA_FILE\napp.config['SECRET_KEY'] = constants.FLASK_SECRET_KEY\napp.config['DEBUG'] = constants.DEBUG\napp.config['USERNAME'] = constants.USERNAME\napp.config['PASSWORD'] = constants.PASSWORD\n</code></pre>"},{"location":"blog/2023/11/14/flask-based-file-hosting-web-app--api--python-module--cli-app/#optional-simple-hello-world-app","title":"(Optional) Simple Hello World App","text":"<p>Replace the contents of <code>app.py</code> with a basic \"Hello, World!\" Flask app:</p> <pre><code># ... (Previous Code: Initial Setup)\n\n@app.route('/')\ndef hello():\n    return 'Hello, World!'\n\nif __name__ == '__main__':\n    app.run(debug=True)\n</code></pre> <p>Run the app using:</p> <pre><code>python app.py\n</code></pre> <p>Visit http://127.0.0.1:5000/ to see the \"Hello, World!\" message.</p>"},{"location":"blog/2023/11/14/flask-based-file-hosting-web-app--api--python-module--cli-app/#adding-a-web-page-to-list-uploaded-files","title":"Adding a Web Page to List Uploaded Files","text":"<ol> <li> <p>Create a <code>template</code> folder and download the index.html file into it.</p> </li> <li> <p>Modify the Flask app in <code>app.py</code> to include the file listing:</p> </li> </ol> <pre><code># ... (Initial Setup)\n\n@app.route('/')\ndef index():\n    list_files = os.listdir(app.config['UPLOAD_FOLDER'])\n    files = [(filename, \"\") for filename in list_files]\n    return render_template('index.html', files=files)\n\nif __name__ == '__main__':\n    if not os.path.exists(app.config['UPLOAD_FOLDER']):\n        os.makedirs(app.config['UPLOAD_FOLDER'])\n    app.run()\n</code></pre> <p>Ensure the 'uploads' folder contains some files, then run the app to see the list.</p>"},{"location":"blog/2023/11/14/flask-based-file-hosting-web-app--api--python-module--cli-app/#adding-a-login-page","title":"Adding a Login Page","text":"<ol> <li> <p>Download the login.html file into the templates folder.</p> </li> <li> <p>Add login-related functions to <code>app.py</code>:</p> </li> </ol> <pre><code># ... (Previous code)\n\ndef validate_credentials(username, password):\n    res = (username == app.config['USERNAME'] and password == app.config['PASSWORD'])\n    session['logged_in'] = res\n    return res\n\ndef is_logged_in():\n    return 'logged_in' in session and session['logged_in']\n\ndef login_required(f):\n    @wraps(f)\n    def decorated_function(*args, **kwargs):\n        if not is_logged_in():\n            session['previous_url'] = request.url\n            return redirect('/login')\n        return f(*args, **kwargs)\n    return decorated_function\n</code></pre> Here is how it works <ul> <li> <p><code>login_required</code> is a wrapper that use the function <code>is_logged_in</code> to check if a user is logged in</p> </li> <li> <p><code>validate_credentials</code> check if the <code>username</code> and <code>password</code> sent by the user match those we have from <code>constants.py</code></p> </li> </ul> <p>Now, we will create the login page and add the login wrapper to the home page</p> <pre><code>@app.route('/')\n@login_required\ndef index():\n    list_files = os.listdir(app.config['UPLOAD_FOLDER'])\n    files = [(filename, \"\") for filename in list_files]\n    return render_template('index.html', files=files)\n\n@app.route('/login', methods=['GET', 'POST'])\ndef login():\n\n    sucessful_login_redirect = lambda : redirect(session.pop('previous_url') if 'previous_url' in session else \"\\\\\")\n    default_login_render = lambda : render_template('login.html')\n\n    if is_logged_in():\n        return sucessful_login_redirect()\n\n    if request.method != 'POST':\n        return default_login_render()\n\n    username = request.form['username']\n    password = request.form['password']\n\n    if validate_credentials(username, password):\n        return sucessful_login_redirect()\n\n    return default_login_render()\n</code></pre> Here is how it works <ul> <li>The login page use the template <code>index.html</code>, a form with two fields: <code>username</code> and <code>password</code>. But if he is already logged in, he may be redirected to another page.</li> <li>When he add his credentials and send them, we will get <code>username</code> and <code>password</code> from the form.</li> <li>Then we will use the function <code>validate_credentials</code> to check them.</li> <li>If the credentials match, the user is redirected to the page he asks for. For example, in an unauthenticated user go the the home page, he is redirected to the login page. And if his crede,tials match, he is redirected back to the home page.</li> </ul>"},{"location":"blog/2023/11/14/flask-based-file-hosting-web-app--api--python-module--cli-app/#create-your-credentials-to-access-the-app","title":"create your credentials to access the app","text":"<p>As i've 've said, i use the most basic authentication for this simple login protected web app.To add the credentials for the web app, create a file <code>.env</code> like .env.example</p> <p><code>.env</code></p> <pre><code>USERNAME=myuser\nPASSWORD=mypassword\n</code></pre> <p>Warning</p> <p>Modify it to match the credentials for your app</p> <p>Also, go into the constants.py file and make sure <code>DEBUG</code> is set to <code>False</code> to use it</p>"},{"location":"blog/2023/11/14/flask-based-file-hosting-web-app--api--python-module--cli-app/#optional-session-timeout-and-logout-page","title":"(Optional) Session Timeout and Logout Page","text":"<p>You can add a timeout of the session. So a user will not stay logged in forever. It is important for data sentivive related apps. You can also let the user logout if he wants. There is a logout bouton in the home page.</p> <ol> <li> <p>Set the session timeout:</p> <pre><code># ... (Previous code)\n\n# Set the session timeout to 30 minutes (1800 seconds)\napp.config['PERMANENT_SESSION_LIFETIME'] = timedelta(minutes=30)\n</code></pre> </li> <li> <p>Add a logout page:</p> <pre><code># ... (Previous code)\n\n@app.route('/logout')\ndef logout():\n    session.clear()\n    return redirect('/login')\n</code></pre> </li> </ol>"},{"location":"blog/2023/11/14/flask-based-file-hosting-web-app--api--python-module--cli-app/#adding-file-download-and-upload-features","title":"Adding File Download and Upload Features","text":"<ol> <li> <p>Add functions for file download:</p> <pre><code># ... (Previous code)\n\n@app.route('/uploads/&lt;path:filename&gt;')\ndef download(filename):\n    return send_from_directory(app.config['UPLOAD_FOLDER'], filename)\n</code></pre> </li> <li> <p>Add functions for file upload:</p> <pre><code># ... (Previous code)\n\n@app.route('/uploads/&lt;path:filename&gt;')\ndef download(filename):\n    return send_from_directory(app.config['UPLOAD_FOLDER'], filename)\n\n\ndef slugify_filename(filename):\n    # Split the filename and extension\n    _ = filename.rsplit('.', 1)\n    if len(_)&lt;2: return \n    base, extension = _\n    # Slugify the base part\n    slug_base = slugify(base)\n    # Join the slugified base with the original extension\n    slug_filename = f\"{slug_base}.{extension}\"\n    return slug_filename\n\ndef handle_file_saving(file):\n    filename = slugify_filename(file.filename)\n    file_save = app.config['UPLOAD_FOLDER'] / filename\n    print(f\"saving {file_save.resolve()}\")\n    file.save(file_save)\n    return filename\n\n@app.route('/upload', methods=['POST'])\n@login_required\ndef upload():\n    file = request.files['file']\n    if file:\n        filename = handle_file_saving(file)\n    return redirect('/')\n</code></pre> </li> </ol> Here is how it works <p>We have added a upload endpoint</p> <ul> <li>that will receive user files and save them using the <code>handle_file_saving</code> function</li> <li>that is protected with <code>login_required</code></li> <li>The function <code>slugify_filename</code> will rewrite the filename to use only lowercase alphanumeric characters an <code>-</code> as separators instead of space</li> <li><code>handle_file_saving</code> will save the file in the <code>uploads</code> directory</li> </ul> <p>The complete code with file download and upload features can be found in the GitHub repository.</p>"},{"location":"blog/2023/11/14/flask-based-file-hosting-web-app--api--python-module--cli-app/#optional-adding-endpoints-for-open-file-raw-content-and-api","title":"(Optional) Adding Endpoints for Open File, Raw Content, and API","text":"<ol> <li> <p>Add endpoints for opening a file, displaying raw content, and API:</p> <pre><code># ... (Previous code)\n\n@app.route('/open/&lt;path:filename&gt;')\n@login_required\ndef open_file(filename):\n    file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)\n\n    if not os.path.exists(file_path):\n        return \"File not found\"\n\n    mime_type = get_content_type(file_path)\n\n    # Map .md and .mmd extensions to text/plain\n    if mime_type == 'text/markdown' or mime_type == 'text/x-markdown':\n        mime_type = 'text/plain'\n\n    if mime_type:\n        with open(file_path, 'rb') as file:\n            file_content = file.read()\n        return Response(file_content, content_type=mime_type)\n\n    return \"Unknown file type\"\n\n@app.route('/raw/&lt;path:filename&gt;')\n@login_required\ndef raw_file(filename):\n    file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)\n\n    if not os.path.exists(file_path):\n        return \"File not found\"\n\n    with open(file_path, 'rb') as file:\n        file_content = file.read()\n    return file_content\n</code></pre> </li> </ol> <p>The code for these features is available in the GitHub repository.</p>"},{"location":"blog/2023/11/14/flask-based-file-hosting-web-app--api--python-module--cli-app/#optional-modifying-file-upload-to-filter-files","title":"(Optional) Modifying File Upload to Filter Files","text":"<p>To filter the files, you can use a database to add which files to show. To be simple, i've used a json file.</p> <p>So the</p> <ul> <li>home page will look into the json file to show the files</li> <li> <p>the upload page will save the file on the server and also add it in the json file</p> </li> <li> <p>Modify the listing feature to filter files using a JSON file:</p> <pre><code>def load_data_from_json():\n    if os.path.exists(app.config['DATA_FILE']):\n        with open(app.config['DATA_FILE'], 'r') as file:\n            try:\n                return json.load(file)\n            except json.JSONDecodeError:\n                pass\n    return {}\n\ndef get_files_with_dates():\n    data = load_data_from_json()\n    return [(filename, data[filename]) for filename in sorted(data, key=data.get) if (app.config['UPLOAD_FOLDER']/filename).exists()]\n\n@app.route('/')\n@login_required\ndef index():\n    files = get_files_with_dates()\n    return render_template('index.html', files=files)\n</code></pre> </li> <li> <p>Modify the upload feature to filter files using a JSON file:</p> <pre><code>def update_data_file(filename):\n    data = load_data_from_json()\n    data[filename] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    with open(app.config['DATA_FILE'], 'w') as file:\n        json.dump(data, file)\n\ndef handle_file_saving(file):\n    filename = slugify_filename(file.filename)\n    file_save = app.config['UPLOAD_FOLDER'] / filename\n    print(f\"saving {file_save.resolve()}\")\n    file.save(file_save)\n    update_data_file(filename)\n    return filename\n\n@app.route('/upload', methods=['POST'])\n@login_required\ndef upload():\n    file = request.files['file']\n    if file:\n        filename = handle_file_saving(file)\n    return redirect('/')\n</code></pre> </li> </ul> <p>The complete code with file filtering and other features is available in the GitHub repository.</p>"},{"location":"blog/2023/11/14/flask-based-file-hosting-web-app--api--python-module--cli-app/#optional-adding-an-api-along-the-web-page","title":"(Optional) Adding an api along the web page","text":"<ol> <li> <p>login</p> <pre><code>@app.route('/api/login', methods=['POST'])\ndef api_login():\n    username = request.json.get('username')\n    password = request.json.get('password')\n\n    if validate_credentials(username, password):\n        return jsonify({'message': 'Login successful'})\n    else:\n        return jsonify({'message': 'Invalid credentials'}), 401\n</code></pre> </li> <li> <p>get all the files</p> <pre><code>@app.route('/api')\ndef api_index():\n    if not is_logged_in():\n        return jsonify({'message': 'Unauthorized'}), 401\n\n    files = get_files()\n    return jsonify({'files': files})\n</code></pre> </li> <li> <p>upload a file</p> <pre><code>@app.route('/api/upload', methods=['POST'])\ndef api_upload():\n    if not is_logged_in():\n        return jsonify({'message': 'Unauthorized'}), 401\n\n    file = request.files['file']\n    if file:\n        filename = handle_file_saving(file)\n        return jsonify({'message': f'File uploaded: {filename}'})\n    else:\n        return jsonify({'message': 'No file provided'}), 400\n</code></pre> </li> <li> <p>download a file</p> <pre><code>@app.route('/api/uploads/&lt;path:filename&gt;')\ndef api_download(filename):\n    if not is_logged_in():\n        return jsonify({'message': 'Unauthorized'}), 401\n\n    return send_from_directory(app.config['UPLOAD_FOLDER'], filename)\n</code></pre> </li> </ol>"},{"location":"blog/2023/11/14/flask-based-file-hosting-web-app--api--python-module--cli-app/#bonus-how-to-use-the-api","title":"(Bonus) How to use the api","text":"<ul> <li>You can access the api with the routes <code>http://localhost:5000/api/*</code></li> <li>The file cli_app/cli_app.py to access the api along with a context manager to handle sessions</li> <li>you can read the api documentation</li> </ul>"},{"location":"blog/2023/11/14/flask-based-file-hosting-web-app--api--python-module--cli-app/#bonus-how-to-use-the-cli-app","title":"(Bonus) How to use the cli app","text":"<ul> <li>The script cli_app/sharefile.py provides a cli app to access the api context manager</li> <li>Using your cli, you can list, upload and download files. The api will be called behind the hood by cli_app/cli_app.py</li> <li>you can read the cli-app documentation</li> </ul>"},{"location":"blog/2023/11/14/flask-based-file-hosting-web-app--api--python-module--cli-app/#bonus-serving-static-files","title":"(Bonus) Serving Static Files","text":"<p>If you want to serve static files, add the following endpoint:</p> <pre><code>@app.route('/&lt;path:filename&gt;')\ndef static_files(filename):\n    return send_from_directory('static', filename)\n</code></pre> <p>Create a 'static' folder and place your static files inside it.</p> <p>It can be interesting for custom css/js files and others</p> <p>You can find all this code in the repository https://github.com/Hermann-web/simple-file-hosting-with-flask</p>"},{"location":"blog/2024/03/09/laravel-pwa-integration-guide-for-mobile-views/","title":"Laravel PWA Integration Guide for Mobile Views","text":""},{"location":"blog/2024/03/09/laravel-pwa-integration-guide-for-mobile-views/#introduction","title":"Introduction","text":"<p>Have you ever wanted to provide mobile views for your web applications without the hassle of native mobile app development?</p> <p>With the Laravel PWA package, you can effortlessly create mobile views for your Laravel application, catering to both Android and iOS users.</p> <p>This documentation serves as a step-by-step guide to help you integrate the Laravel PWA package into your Laravel application effortlessly. Whether you're a seasoned developer or just starting your journey with Laravel, this guide will walk you through the process, making PWA implementation a breeze.</p>"},{"location":"blog/2024/03/09/laravel-pwa-integration-guide-for-mobile-views/#motivation-pwa-vs-native","title":"Motivation: PWA vs Native","text":"<p>Before diving into the details, let's explore the motivation behind using Progressive Web Apps (PWAs) compared to native mobile app development:</p> <code>Pros of PWA</code> <code>Cons of PWA</code> <ul> <li> <p>Cross-Platform Compatibility: PWAs work seamlessly across various platforms, including Android, iOS, and desktop browsers, eliminating the need for separate development efforts for each platform.</p> </li> <li> <p>Cost-Effectiveness: Developing a PWA is often more cost-effective than building separate native apps for different platforms, as it requires less time and resources.</p> </li> <li> <p>Easy Maintenance: With a single codebase for both web and mobile, maintaining a PWA is simpler and more efficient compared to managing separate native apps.</p> </li> </ul> <ul> <li> <p>Limited Device Access: While PWAs offer broad platform compatibility, they may have limited access to certain device features compared to native apps.</p> </li> <li> <p>Performance: Although PWAs have made significant strides in performance, they may still lag behind native apps in terms of speed and responsiveness, especially for complex applications.</p> </li> <li> <p>App Store Distribution: Unlike native apps, PWAs do not have direct access to app stores, potentially limiting their discoverability and distribution.</p> </li> </ul> <p>Now that we've explored the pros and cons, let's proceed with the integration of the Laravel PWA package into your Laravel application for creating mobile views.</p>"},{"location":"blog/2024/03/09/laravel-pwa-integration-guide-for-mobile-views/#installation","title":"Installation","text":"<p>To begin, you'll need to install the Laravel PWA package into your Laravel application using Composer. Open your terminal and run the following command:</p> <pre><code>composer require silviolleite/laravelpwa --prefer-dist\n</code></pre>"},{"location":"blog/2024/03/09/laravel-pwa-integration-guide-for-mobile-views/#vendor-publishing","title":"Vendor Publishing","text":"<p>After installing the package, you'll need to publish its assets to your application. Use the following Artisan command to publish the assets:</p> <pre><code>php artisan vendor:publish --provider=\"LaravelPWA\\Providers\\LaravelPWAServiceProvider\"\n</code></pre>"},{"location":"blog/2024/03/09/laravel-pwa-integration-guide-for-mobile-views/#implementation","title":"Implementation","text":"<p>Once the package's assets are published, you can start implementing PWA features in your Laravel application.</p> <ol> <li>Open your Blade template file (e.g., <code>resources/views/layouts/app.blade.php</code>).</li> <li>Inside the <code>&lt;head&gt;</code> section of your template, add the <code>@laravelPWA</code> directive:</li> </ol> <pre><code>&lt;html&gt;\n&lt;head&gt;\n    &lt;title&gt;My Laravel PWA&lt;/title&gt;\n    &lt;!-- Other meta tags and stylesheets --&gt;\n    @laravelPWA\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;!-- Your application content --&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>Note: Ensure that you include the <code>@laravelPWA</code> directive in every Blade template where you want to enable PWA features.</p>"},{"location":"blog/2024/03/09/laravel-pwa-integration-guide-for-mobile-views/#customization-options","title":"Customization Options","text":"<p>Certain aspects of your PWA can be customized to match your application's branding and identity. These include:</p> <ul> <li>App Name: Customize the name of your PWA.</li> <li>Description: Add a description to provide users with information about your PWA.</li> <li>Icons and Splashes: Customize the icons and splash screens displayed when launching the PWA.</li> </ul> <p>These customization options can be adjusted in the <code>config/laravelpwa.php</code> file as mentioned in the module repo</p>"},{"location":"blog/2024/03/09/laravel-pwa-integration-guide-for-mobile-views/#conclusion","title":"Conclusion","text":"<p>Congratulations! You've successfully integrated the Laravel PWA package into your Laravel application, enabling Progressive Web App functionality. Your users can now enjoy a seamless web experience with features like offline access, push notifications, and more.</p> <p>With this easy-to-follow guide, you've unlocked the potential of PWA in your Laravel application, enhancing user engagement and satisfaction.</p> <p>Explore the possibilities of PWA further and stay tuned for updates and enhancements to the Laravel PWA package.</p>"},{"location":"blog/2024/03/09/laravel-pwa-integration-guide-for-mobile-views/#related-pages","title":"Related pages","text":"<ul> <li>Setting Up Laravel Environment on Linux</li> <li>Setting Up Laravel Environment on Windows</li> </ul>"},{"location":"blog/2023/11/18/setting-up-laravel-environment-on-ubuntu/","title":"Setting Up Laravel Environment on Ubuntu","text":"<p>This guide will help you set up the necessary environment to run a Laravel application on an Ubuntu system.</p> <p>So this document provides a step-by-step guide to set up Apache, PHP, MySQL/MariaDB, Composer, and phpMyAdmin for managing databases, while also ensuring MySQL root user password setup for a Laravel environment on Ubuntu.</p>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/2023/11/18/setting-up-laravel-environment-on-ubuntu/#install-apache-and-php","title":"Install Apache and PHP","text":"<ul> <li>Install Apache</li> </ul> <pre><code>sudo apt update \nsudo apt install apache2\n</code></pre> <ul> <li>Enable the Apache service and start it:</li> </ul> <pre><code>sudo systemctl enable apache2\nsudo systemctl start apache2\n</code></pre> When systemd is not running in this container <p>If you encounter the error message <code>systemd\" is not running in this container due to its overhead. Use the \"service\" command to start services instead. e.g.: service --status-all</code>, use the <code>service</code> command instead of <code>systemctl</code>.</p> <p>For example, instead of <code>sudo systemctl enable apache2 &amp; sudo systemctl start apache2</code>, use <code>sudo service apache2 start</code>. To stop or reload the service, use <code>sudo service apache2 stop</code> or <code>sudo service apache2 reload</code>, respectively.</p> <p>Similarly, for stopping or reloading other services, use the <code>service</code> command instead of <code>systemctl</code>.</p> <ul> <li>Install php</li> </ul> <pre><code>sudo apt install php php-cli php-common php-mbstring php-xml php-zip php-mysql php-pgsql php-sqlite3 php-json php-bcmath php-gd php-tokenizer php-xmlwriter\n</code></pre> <p>Check <code>localhost</code> in your browser to ensure Apache is running.</p>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/2023/11/18/setting-up-laravel-environment-on-ubuntu/#install-mariadbmysql","title":"Install MariaDB/MySQL","text":"<ul> <li>Install MariaDB MariaDB is an open-source relational database management system. Install it by running the following command:</li> </ul> <pre><code>sudo apt install mariadb-server\n</code></pre> <ul> <li>Install MySQL</li> </ul> <pre><code>sudo mysql_secure_installation\n</code></pre> <ul> <li>Enable and start MySQL:</li> </ul> <pre><code>sudo systemctl enable mysql\nsudo systemctl start mysql\nsudo systemctl status mysql\n</code></pre>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/2023/11/18/setting-up-laravel-environment-on-ubuntu/#install-composer","title":"Install Composer","text":"<p>Composer is a dependency management tool for PHP. Install with the command below</p> <pre><code>sudo apt install composer\n</code></pre> <ul> <li>here is an altenative</li> </ul> <pre><code>curl -sS https://getcomposer.org/installer | php\nsudo mv composer.phar /usr/local/bin/composer\nsudo chmod +x /usr/local/bin/composer\n</code></pre>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/2023/11/18/setting-up-laravel-environment-on-ubuntu/#install-phpmyadmin-for-database-management","title":"Install phpMyAdmin for Database Management","text":"<pre><code>sudo apt update\nsudo apt install phpmyadmin\nsudo ln -s /etc/phpmyadmin/apache.conf /etc/apache2/conf-available/phpmyadmin.conf\nsudo a2enconf phpmyadmin\nsudo systemctl reload apache2\n</code></pre> <p>Access phpMyAdmin at <code>http://localhost/phpmyadmin</code> to manage your databases.</p>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/2023/11/18/setting-up-laravel-environment-on-ubuntu/#configure-mysql-root-user-password","title":"Configure MySQL Root User Password","text":"<p>because phpmyadmin refuse the passwordless login and also for security purposes</p> <p>Modify the MySQL configuration file:</p> <pre><code>sudo nano /etc/mysql/mariadb.conf.d/50-server.cnf\n</code></pre> <p>In some setups or older versions, this file might exist as the main configuration file for the MySQL/MariaDB server</p> <pre><code>sudo nano /etc/mysql/mysql.conf.d/mysqld.cnf\n</code></pre> <p>Add the following line under the <code>[mysqld]</code> section:</p> <pre><code>skip-grant-tables\n</code></pre> <p>Restart MySQL in safe mode to update the config:</p> <pre><code>sudo systemctl stop mysql\nsudo mysqld_safe --skip-grant-tables --skip-networking &amp;\n</code></pre> <p>If there's a process conflict, kill the processes and restart MySQL.</p> <p>In another terminal:</p> <pre><code>mysql -u root -p\n</code></pre> <pre><code>use mysql;\nupdate user set authentication_string=PASSWORD(\"new_password\") where User='root';\nflush privileges;\nquit;\n</code></pre> <p>Restart MySQL:</p> <pre><code>sudo systemctl stop mysql\nsudo systemctl start mysql\n</code></pre> <p>Access phpMyAdmin at <code>http://localhost/phpmyadmin</code> and use the updated root password.</p> <p>This environment should now be ready for running your Laravel application.</p>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/2023/11/18/setting-up-laravel-environment-on-ubuntu/#bonus-create-a-new-laravel-project","title":"(Bonus) Create a New Laravel Project","text":"<p>Let's conclude this guide with a practical example by demonstrating how to create a new Laravel project using Composer.</p> <p>So we will apply the newly configured environment by creating a new Laravel project and accessing it through a web browser</p> <p>Now that you have set up your Laravel environment, let's create a new Laravel project using Composer.</p> <p>Run the following command in your terminal:</p> <pre><code>composer create-project --prefer-dist laravel/laravel my-laravel-app\n</code></pre> <p>This command will create a new Laravel project named <code>my-laravel-app</code> in the current directory. Replace <code>my-laravel-app</code> with your preferred project name.</p> <p>Navigate to the project directory:</p> <pre><code>cd my-laravel-app\n</code></pre> <p>Then, start the Laravel development server:</p> <pre><code>php artisan serve\n</code></pre> <p>Access your Laravel application by visiting <code>http://localhost:8000</code> in your web browser. You should see the default Laravel welcome page, confirming that your new Laravel project is up and running!</p>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/2023/11/18/setting-up-laravel-environment-on-ubuntu/#setup-a-laravel-project-case-of-lavsms","title":"Setup a laravel project: case of lavsms","text":"","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/2023/11/18/setting-up-laravel-environment-on-ubuntu/#run-xampp","title":"Run XAMPP","text":"<p>XAMPP is used to provide a local server environment to run your Laravel application.</p> <ul> <li>Start the XAMPP GUI application.</li> <li>Launch the Apache web server and MySQL database server.</li> <li> <p>Create a new database named <code>lavsms</code> using phpMyAdmin or another MySQL client.</p> <pre><code>mysql -u your_username -p -e \"CREATE DATABASE lavsms;\"\n</code></pre> </li> </ul>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/2023/11/18/setting-up-laravel-environment-on-ubuntu/#database-configuration","title":"Database Configuration","text":"<p>Configure the database settings for your Laravel application.</p> <ul> <li> <p>Create an environment (<code>.env</code>) file by making a copy of the example file:</p> <pre><code>cp .env.example .env\n</code></pre> </li> <li> <p>Modify the database connection settings in the <code>.env</code> file to match your XAMPP setup:</p> <pre><code>DB_DATABASE=lavsms\nDB_USERNAME=root\nDB_PASSWORD=\n</code></pre> </li> </ul>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/2023/11/18/setting-up-laravel-environment-on-ubuntu/#install-project-dependencies","title":"Install Project Dependencies","text":"<p>Install the necessary dependencies for your Laravel project.</p> <pre><code># Navigate to the project directory\ncd path/to/project\n\n# Update Composer dependencies (if needed)\ncomposer update\n\n# Install Composer dependencies\ncomposer install\n\n# Install Node.js dependencies\nnpm install\n</code></pre>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/2023/11/18/setting-up-laravel-environment-on-ubuntu/#build","title":"Build","text":"<p>Perform necessary build steps for your Laravel application.</p> <pre><code># Generate an application key\nphp artisan key:generate\n\n# Clear the configuration cache\nphp artisan config:clear\n\n# create a symbolic link (`/public/storage`) from the storage directory (`/storage/app/public`) to the public directory (`/public/`)\nphp artisan storage:link\n</code></pre>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/2023/11/18/setting-up-laravel-environment-on-ubuntu/#build-db","title":"Build Db","text":"<p>Prepare and set up your database for the Laravel application.</p> <pre><code># Run database migrations to create database tables\nphp artisan migrate\n\n# Seed the database with initial data (if needed)\nphp artisan db:seed   \n</code></pre>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/2023/11/18/setting-up-laravel-environment-on-ubuntu/#run-development","title":"Run Development","text":"<p>Start the development server for your Laravel application.</p> <pre><code># Start the Laravel development server\nphp artisan serve\n</code></pre>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/2023/11/18/setting-up-laravel-environment-on-ubuntu/#bonus-a-comparison-laravel-php-vs-django-python-mvc-like-architecture","title":"(Bonus) A Comparison: Laravel (PHP) vs. Django (Python) MVC-like Architecture","text":"<p>A brief comparison of Laravel's architecture to Django's MVC pattern.</p> <p>Both Laravel (PHP) and Django (Python) frameworks use a MVC-like architecture. Here are the analogies.</p> <code>Laravel (PHP)</code> <code>Django (Python)</code> <ol> <li>Routes: Defined in <code>routes/web.php</code>.<ul> <li>Invokes PHP controllers.</li> <li>Calls <code>resources\\views\\partials\\js\\custom_js.blade.php</code> (JavaScript) on form submission, writing to the console.</li> </ul> </li> <li>Serializers: Located in <code>app/http/requests</code>.<ul> <li>Used in controllers for data validation.</li> </ul> </li> <li>Controllers: Found in <code>app/http/controllers</code>.<ul> <li>Utilizes serializers automatically for data validation.</li> <li>Uses models for CRUD operations.</li> <li>Returns <code>parse(a_view, data_for_client)</code> similar to Django.</li> </ul> </li> <li>Models: Reside in <code>app/models</code>.<ul> <li>Utilized in controllers for CRUD operations.</li> </ul> </li> <li>Views (Blade): Located in <code>resources/views</code>.<ul> <li>Similar to PHP-client in Django, handling the presentation layer.</li> </ul> </li> </ol> <ol> <li>URL Patterns: Defined in <code>urls.py</code>.<ul> <li>Maps to Python views.</li> <li>Handles HTTP requests and defines the view functions.</li> </ul> </li> <li>Serializers: Often part of Django REST framework in Python.<ul> <li>Used for serialization and deserialization of data.</li> </ul> </li> <li>Views: Python files corresponding to the application's logic.<ul> <li>Utilizes serializers for data validation.</li> <li>Performs database operations and returns rendered templates.</li> </ul> </li> <li>Models: Represented as Python classes in <code>models.py</code>.<ul> <li>Represents the application's data structure.</li> <li>Interacts with the database via Django's ORM.</li> </ul> </li> <li>Templates: HTML files residing in <code>templates</code> directory.<ul> <li>Renders the user interface based on data provided by views.</li> </ul> </li> </ol>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/2023/11/18/setting-up-laravel-environment-on-ubuntu/#related-pages","title":"Related pages","text":"<ul> <li>Guide to Installing MySQL and Connecting to Databases</li> <li>Setting Up Laravel Environment on Windows</li> <li>Laravel PWA Integration Guide for Mobile Views</li> </ul>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/2023/11/17/setting-up-laravel-environment-on-windows/","title":"Setting Up Laravel Environment on Windows","text":"<p>This guide provides step-by-step instructions to set up a Laravel project on your local environment using XAMPP. If you encounter any issues, please refer to the version details provided below for context and troubleshooting.</p>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/2023/11/17/setting-up-laravel-environment-on-windows/#introduction","title":"Introduction","text":"<p>Welcome to the \"Setting Up Laravel Environment on Windows\" tutorial! This guide helps you set up a strong development environment on your Windows system for effortless Laravel web application creation. You'll navigate through configuring Apache, PHP, MySQL/MariaDB, Composer, and phpMyAdmin, making your Windows system a powerful platform for Laravel development. Whether you're new or experienced in web development, this step-by-step tutorial ensures a smooth setup, enabling you to dive into Laravel effortlessly.</p> <p>In this tutorial, we'll cover:</p> <ol> <li> <p>Installation of Essential Tools: We'll start by installing XAMPP, Composer, and Node.js. These tools are the building blocks of your Laravel environment.</p> </li> <li> <p>XAMPP Configuration: We'll guide you through launching the Apache web server, setting up the MySQL database server, and configuring a database using phpMyAdmin.</p> </li> </ol> <ol> <li> <p>Database Setup for Laravel: You'll learn how to configure your Laravel application to interact with the database through the <code>.env</code> file.</p> </li> <li> <p>Dependency Management: We'll explore installing project dependencies using Composer and Node.js for your Laravel project.</p> </li> <li> <p>Project Setup and Run: You'll run essential commands to generate keys, perform database migrations, seed initial data, and finally, start the Laravel development server.</p> </li> <li> <p>Bonus - Laravel Pattern Overview: We'll provide a brief comparison of Laravel's architecture to the MVC pattern, similar to Django's structure.</p> </li> </ol> <p>Each section is designed to streamline your Laravel setup process, ensuring you have a robust environment ready to build dynamic web applications. Let's dive in and set up your Windows system for Laravel development!</p>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/2023/11/17/setting-up-laravel-environment-on-windows/#version-details","title":"Version Details","text":"<p>from xammp installation</p> <ul> <li>PHP Version: PHP 8.2.4 (cli) (built: Mar 14 2023)</li> <li>XAMPP Control Panel Version: XAMPP for Windows 8.2.4</li> <li>MySQL Version: MariaDB 10.4.28, for Win64 (AMD64)</li> </ul> <p>from composer intallation</p> <ul> <li>Composer Version: Composer version 2.6.2 (2023-09-03)</li> </ul> <p>from node intallation</p> <ul> <li>Node Version: Node version v20.5.1 (2023-09-03)</li> </ul>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/2023/11/17/setting-up-laravel-environment-on-windows/#installation-steps","title":"Installation Steps","text":"","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/2023/11/17/setting-up-laravel-environment-on-windows/#install-xampp","title":"Install XAMPP","text":"<p>XAMPP is a free and open-source cross-platform web server solution stack package developed by Apache Friends, consisting mainly of the Apache HTTP Server, MariaDB database, and interpreters for scripts written in the PHP and Perl programming languages.</p> <ul> <li>Download from the website: XAMPP Download Page</li> <li>Put the folder <code>C:\\xampp\\php</code> (or equivalent) in the variables environment</li> </ul>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/2023/11/17/setting-up-laravel-environment-on-windows/#install-composer-php-dependency-manager","title":"Install Composer (PHP Dependency Manager)","text":"<p>Composer is a tool for dependency management in PHP. It allows you to declare the libraries your project depends on and it will manage them for you.</p> <pre><code># Download the Composer installer for Windows: link found at https://getcomposer.org/doc/00-intro.md\ncurl -O https://getcomposer.org/Composer-Setup.exe\n\n# Run the Composer installer (this will open a GUI installer)\nstart Composer-Setup.exe\n</code></pre>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/2023/11/17/setting-up-laravel-environment-on-windows/#install-nodejs-and-npm-node-package-manager","title":"Install Node.js and npm (Node Package Manager)","text":"<p>Node.js is an open-source, cross-platform, JavaScript runtime environment that executes JavaScript code outside a web browser. npm is the default package manager for Node.js.</p> <ul> <li>Download the Node.js (<code>node 20</code> preferably) installer for Windows from the official website: https://nodejs.org/</li> <li>Run the Node.js installer (this will also install npm).</li> </ul>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/2023/11/17/setting-up-laravel-environment-on-windows/#setup-a-laravel-project-case-of-lavsms","title":"Setup a laravel project: case of lavsms","text":"","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/2023/11/17/setting-up-laravel-environment-on-windows/#run-xampp","title":"Run XAMPP","text":"<p>XAMPP is used to provide a local server environment to run your Laravel application.</p> <ul> <li>Start the XAMPP GUI application.</li> <li>Launch the Apache web server and MySQL database server.</li> <li> <p>Create a new database named <code>lavsms</code> using phpMyAdmin or another MySQL client.</p> <pre><code>mysql -u your_username -p -e \"CREATE DATABASE lavsms;\"\n</code></pre> </li> </ul>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/2023/11/17/setting-up-laravel-environment-on-windows/#database-configuration","title":"Database Configuration","text":"<p>Configure the database settings for your Laravel application.</p> <ul> <li> <p>Create an environment (<code>.env</code>) file by making a copy of the example file:</p> <pre><code>cp .env.example .env\n</code></pre> </li> <li> <p>Modify the database connection settings in the <code>.env</code> file to match your XAMPP setup:</p> <pre><code>DB_DATABASE=lavsms\nDB_USERNAME=root\nDB_PASSWORD=\n</code></pre> </li> </ul>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/2023/11/17/setting-up-laravel-environment-on-windows/#install-project-dependencies","title":"Install Project Dependencies","text":"<p>Install the necessary dependencies for your Laravel project.</p> <pre><code># Navigate to the project directory\ncd path/to/project\n\n# Update Composer dependencies (if needed)\ncomposer update\n\n# Install Composer dependencies\ncomposer install\n\n# Install Node.js dependencies\nnpm install\n</code></pre>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/2023/11/17/setting-up-laravel-environment-on-windows/#build","title":"Build","text":"<p>Perform necessary build steps for your Laravel application.</p> <pre><code># Generate an application key\nphp artisan key:generate\n\n# Clear the configuration cache\nphp artisan config:clear\n\n# create a symbolic link (`/public/storage`) from the storage directory (`/storage/app/public`) to the public directory (`/public/`)\nphp artisan storage:link\n</code></pre>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/2023/11/17/setting-up-laravel-environment-on-windows/#build-db","title":"Build Db","text":"<p>Prepare and set up your database for the Laravel application.</p> <pre><code># Run database migrations to create database tables\nphp artisan migrate\n\n# Seed the database with initial data (if needed)\nphp artisan db:seed   \n</code></pre>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/2023/11/17/setting-up-laravel-environment-on-windows/#run-development","title":"Run Development","text":"<p>Start the development server for your Laravel application.</p> <pre><code># Start the Laravel development server\nphp artisan serve\n</code></pre>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/2023/11/17/setting-up-laravel-environment-on-windows/#bonus-a-comparison-laravel-php-vs-django-python-mvc-like-architecture","title":"(Bonus) A Comparison: Laravel (PHP) vs. Django (Python) MVC-like Architecture","text":"<p>A brief comparison of Laravel's architecture to Django's MVC pattern.</p> <p>Both Laravel (PHP) and Django (Python) frameworks use a MVC-like architecture. Here are the analogies.</p> <code>Laravel (PHP)</code> <code>Django (Python)</code> <ol> <li>Routes: Defined in <code>routes/web.php</code>.<ul> <li>Invokes PHP controllers.</li> <li>Calls <code>resources\\views\\partials\\js\\custom_js.blade.php</code> (JavaScript) on form submission, writing to the console.</li> </ul> </li> <li>Serializers: Located in <code>app/http/requests</code>.<ul> <li>Used in controllers for data validation.</li> </ul> </li> <li>Controllers: Found in <code>app/http/controllers</code>.<ul> <li>Utilizes serializers automatically for data validation.</li> <li>Uses models for CRUD operations.</li> <li>Returns <code>parse(a_view, data_for_client)</code> similar to Django.</li> </ul> </li> <li>Models: Reside in <code>app/models</code>.<ul> <li>Utilized in controllers for CRUD operations.</li> </ul> </li> <li>Views (Blade): Located in <code>resources/views</code>.<ul> <li>Similar to PHP-client in Django, handling the presentation layer.</li> </ul> </li> </ol> <ol> <li>URL Patterns: Defined in <code>urls.py</code>.<ul> <li>Maps to Python views.</li> <li>Handles HTTP requests and defines the view functions.</li> </ul> </li> <li>Serializers: Often part of Django REST framework in Python.<ul> <li>Used for serialization and deserialization of data.</li> </ul> </li> <li>Views: Python files corresponding to the application's logic.<ul> <li>Utilizes serializers for data validation.</li> <li>Performs database operations and returns rendered templates.</li> </ul> </li> <li>Models: Represented as Python classes in <code>models.py</code>.<ul> <li>Represents the application's data structure.</li> <li>Interacts with the database via Django's ORM.</li> </ul> </li> <li>Templates: HTML files residing in <code>templates</code> directory.<ul> <li>Renders the user interface based on data provided by views.</li> </ul> </li> </ol>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/2023/11/17/setting-up-laravel-environment-on-windows/#related-pages","title":"Related pages","text":"<ul> <li>Guide to Installing MySQL and Connecting to Databases</li> <li>Setting Up Laravel Environment on Linux</li> <li>Laravel PWA Integration Guide for Mobile Views</li> </ul>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/2024/03/10/integrating-requirementstxt-with-poetry/","title":"Integrating Requirements.txt with Poetry","text":"<p>Managing dependencies is a crucial aspect of any software project. Whether you're starting a new project or inheriting an existing one, handling dependencies effectively can greatly impact your workflow. Often, projects utilize a <code>requirements.txt</code> file to specify their dependencies, but when it comes to Python projects, integrating these dependencies seamlessly with a package manager like Poetry can streamline the process.</p> <p>So, when working with Poetry, you might need to integrate your existing <code>requirements.txt</code> file into your project. This document outlines how to achieve that efficiently.</p> <p>It's essential to ensure that the file contains only abstract requirements, akin to manual maintenance. Conversely, if the <code>requirements.txt</code> file is generated from a <code>pip freeze</code>, it includes all packages, not just high-level requirements.</p>"},{"location":"blog/2024/03/10/integrating-requirementstxt-with-poetry/#removing-spaces-comments-and-empty-lines","title":"Removing Spaces, Comments, and Empty Lines","text":"<p>To streamline the process, we'll remove unnecessary elements from the <code>requirements.txt</code> file before adding packages to Poetry.</p> <pre><code># Add requirements.txt as dependencies for Poetry\ncat requirements.txt | sed 's/ //g' | sed 's/#.*$//g' | grep -v \"^$\" | xargs -n 1 poetry add\n</code></pre> <p>Explanation:</p> <ul> <li><code>sed 's/ //g'</code>: Removes all spaces.</li> <li><code>sed 's/#.*$//g'</code>: Removes everything after <code>#</code>, effectively removing comments.</li> <li><code>grep -v \"^$\"</code>: Excludes empty lines.</li> <li><code>xargs -n 1 poetry add</code>: Adds each package listed in <code>requirements.txt</code> to Poetry.</li> </ul> Example <p>Input: <pre><code>requests==2.26.0  # HTTP library\ndjango&gt;=3.2.0\nnumpy==1.21.0\n</code></pre></p> <p>Output: <pre><code>Adding requests (2.26.0)\nAdding django (3.2.0)\nAdding numpy (1.21.0)\n</code></pre></p>"},{"location":"blog/2024/03/10/integrating-requirementstxt-with-poetry/#removing-package-versions","title":"Removing Package Versions","text":"<p>If you want to add packages without their versions, you can use the following command:</p> <pre><code>cat requirements.txt | sed 's/ //g' | sed 's/#.*$//g' | grep -v \"^$\" | cut -d= -f1 | xargs -n 1 poetry add\n</code></pre> <p>This command strips version information from the package names before adding them to Poetry.</p> Example <p>Input: <pre><code>requests==2.26.0  # HTTP library\ndjango&gt;=3.2.0\nnumpy==1.21.0\n</code></pre></p> <p>Output: <pre><code>Adding requests\nAdding django\nAdding numpy\n</code></pre></p> <p>Be Cautious!</p> <p>Ensure that your <code>requirements.txt</code> file contains abstract requirements. If it's generated from a <code>pip freeze</code>, it lists all packages, which may not align with high-level requirements.</p>"},{"location":"blog/2024/03/10/integrating-requirementstxt-with-poetry/#conclusion","title":"Conclusion","text":"<p>Integrating an existing <code>requirements.txt</code> file into a Poetry project can be a straightforward process with the right approach. By ensuring that your requirements are abstract and by following the provided steps to preprocess your file, you can seamlessly manage dependencies in your Python projects using Poetry. Embracing modern tools like Poetry not only simplifies dependency management but also enhances the overall development experience, allowing you to focus more on building and refining your software.</p>"},{"location":"blog/2024/03/10/integrating-requirementstxt-with-poetry/#related-links","title":"Related Links","text":"<ul> <li>How to import an existing requirements.txt into a Poetry project? - Stack Overflow</li> <li>Cheat on Python Package Managers</li> </ul>"},{"location":"blog/2023/10/28/managing-python-dependencies-navigating-pip-pipenv-poetry-conda-and-more/","title":"Managing Python Dependencies: Navigating pip, pipenv, poetry, conda and more","text":""},{"location":"blog/2023/10/28/managing-python-dependencies-navigating-pip-pipenv-poetry-conda-and-more/#introduction","title":"Introduction","text":"<p>In the realm of Python development, a crucial aspect is managing project dependencies effectively.</p> <p>This guide delves into four prominent tools\u2014pip, pipenv, poetry, conda and more\u2014each offering distinct approaches to dependency management. Grasping their strengths, weaknesses, and use cases empowers you to make informed decisions for your projects.</p>"},{"location":"blog/2023/10/28/managing-python-dependencies-navigating-pip-pipenv-poetry-conda-and-more/#key-considerations","title":"Key Considerations","text":""},{"location":"blog/2023/10/28/managing-python-dependencies-navigating-pip-pipenv-poetry-conda-and-more/#choosing-the-right-tool","title":"Choosing the Right Tool","text":"<ul> <li>Scope and Size: Consider project scale and complexity.</li> <li>Team Collaboration: Assess the extent of teamwork involved.</li> <li>Environment Isolation: Determine the need for isolated environments.</li> <li>Dependency Management: Evaluate desired features like dependency locking and version conflict resolution.</li> <li>Project Requirements: Factor in specific project needs or constraints.</li> </ul>"},{"location":"blog/2023/10/28/managing-python-dependencies-navigating-pip-pipenv-poetry-conda-and-more/#tools-overview","title":"Tools Overview","text":"<code>pip</code> <code>pipenv</code> <code>poetry</code> <code>conda</code> <code>uv</code> <ul> <li>Core Package Installer: The foundation for Python package management.</li> <li>Global Installations: Installs packages system-wide by default.</li> <li>Virtual Environments: Can be used within virtual environments for isolation.</li> <li>Basic Dependency Management: Relies on <code>requirements.txt</code> for specifying dependencies.</li> </ul> <ul> <li>Virtual Environment Creation and Management: Automatically creates and manages virtual environments.</li> <li>Dependency Locking: Locks dependency versions for reproducibility.</li> <li>Integration with Pipfile: Uses Pipfile and Pipfile.lock for dependency management.</li> </ul> <ul> <li>Dependency Management and Packaging: Comprehensive dependency management and packaging tool.</li> <li>Virtual Environment Handling: Manages virtual environments effectively.</li> <li>Dependency Locking and Version Handling: Offers robust dependency locking and version conflict resolution.</li> <li>Declarative Syntax: Uses pyproject.toml for configuration.</li> </ul> <ul> <li>Cross-Platform Package and Environment Management: Manages packages and environments across multiple languages (Python, R, etc.).</li> <li>Large Ecosystem of Packages: Accesses a vast repository of packages through Anaconda repositories.</li> <li>Environment Isolation: Creates isolated environments for project-specific dependencies.</li> <li>Non-Python Dependencies: Handles non-Python dependencies as well.</li> </ul> <ul> <li>Extremely Fast Installation: Faster than traditional pip and pip-tools.</li> <li>Drop-in Replacement: Provides a familiar interface for common pip commands.</li> <li>Supports Advanced Features: Handles editable installs, Git dependencies, URL dependencies, etc.</li> <li>Dependency Management: Offers features like dependency overrides and conflict resolution.</li> <li>Limitations: Not a complete replacement for pip; some features are still missing.</li> <li>Platform-specific Requirements File Generation: Generates requirements files tailored to specific platforms.</li> </ul>"},{"location":"blog/2023/10/28/managing-python-dependencies-navigating-pip-pipenv-poetry-conda-and-more/#comparison-table","title":"Comparison Table","text":"Feature pip pipenv poetry conda uv Installation Built-in <code>pip install pipenv</code> <code>pip install poetry</code> Download and install Anaconda or Miniconda <code>pip install uv</code> Environments Implicit Automatic Automatic Automatic Implicit Locking Manual Automatic Automatic Automatic Manual Configuration requirements.txt Pipfile, Pipfile.lock pyproject.toml Environment files N/A Official packages PyPI (Python Package Index) PyPI PyPI Conda Forge (conda-forge channel) PyPI (Python Package Index) Multi-python-version Resolution No No Yes No Yes"},{"location":"blog/2023/10/28/managing-python-dependencies-navigating-pip-pipenv-poetry-conda-and-more/#common-commands","title":"Common Commands","text":"Feature pip pipenv poetry conda uv Create a project <code>python -m venv &lt;env-name&gt;</code> <code>pipenv --python &lt;python-version&gt;</code> <code>poetry init</code> <code>conda create -n &lt;env-name&gt; python=&lt;python-version&gt;</code> <code>uv venv</code> (creates virtualenv at: <code>.venv/</code>) Use a virtual environment <code>source &lt;env-name&gt;/bin/activate</code> (linux) or <code>&lt;env-name&gt;/Scripts/activate</code> (windows) <code>pipenv shell</code> <code>poetry shell</code> <code>conda activate &lt;env-name&gt;</code> Same as pip Install a package <code>pip install &lt;package-name&gt;</code> <code>pipenv install &lt;package-name&gt;</code> <code>poetry add &lt;package-name&gt;</code> <code>conda install &lt;package-name&gt;</code> <code>uv pip install &lt;package-name&gt;</code> Remove a package <code>pip uninstall &lt;package-name&gt;</code> <code>pipenv uninstall &lt;package-name&gt;</code> <code>poetry remove &lt;package-name&gt;</code> <code>conda uninstall &lt;package-name&gt;</code> <code>uv pip uninstall &lt;package-name&gt;</code> Install from requirements <code>pip install -r requirements.txt</code> <code>pipenv install</code> (reads from Pipfile) <code>poetry install</code> (reads from pyproject.toml) <code>conda install --file requirements.txt</code> <code>uv pip sync requirements.txt</code> or <code>uv pip install -r requirements.txt</code> Dev packages vs others No explicit distinction <code>--dev</code> flag for development dependencies <code>[tool.poetry.dev-dependencies]</code> section in pyproject.toml No explicit distinction Not currently supported (planned for future) List requirements <code>pip freeze &gt; requirements.txt</code> or use <code>pipreqs</code> <code>pipenv lock -r &gt; requirements.txt</code> <code>poetry export -f requirements.txt &gt; requirements.txt</code> <code>conda list --export &gt; requirements.txt</code> Same as pip (e.g., <code>uv pip freeze &gt; requirements.txt</code>) Transport project Manually copy files or create a setup.py Copy Pipfile and Pipfile.lock Copy pyproject.toml and pyproject.toml.lock Export environment to .yml file (conda env export &gt; environment.yml) Not directly supported (requires rebuilding with UV) Install from lock file N/A <code>pipenv install --ignore-pipfile</code> <code>poetry install --no-dev</code> (reads from pyproject.toml.lock) <code>conda install --file environment.yml</code> N/A (Dependency resolution handled by UV directly) Delete the venv Remove directory manually <code>pipenv --rm</code> <code>poetry env remove &lt;env-name&gt;</code> <code>conda remove --name &lt;env-name&gt; --all</code> N/A <p>Remember !</p> <p>As you wrap up your exploration of Python's dependency management tools, remember:</p> <ul> <li>Consistency: Maintain a uniform approach within a project for easier maintenance.</li> <li>Descriptive Naming: Use clear and informative names for environments and dependencies.</li> <li>Thorough Testing: Ensure compatibility and functionality after dependency changes.</li> <li>Regular Updates: Keep tools and dependencies up-to-date for security and performance.</li> </ul> More on UV dependencies resolution <ul> <li>Generate Requirements from pyproject.toml: <pre><code>uv pip-compile pyproject.toml -o requirements.txt\n</code></pre></li> <li> <p>Platform-specific Requirements Generation: <pre><code>uv pip-compile requirements.in -o requirements.txt\n</code></pre></p> </li> <li> <p>Command Line Compatibility:   uv's <code>pip-install</code> (<code>uv pip</code>) and <code>pip-compile</code> (<code>uv compile</code>) commands support many familiar command-line arguments, such as <code>-r requirements.txt</code>, <code>-c constraints.txt</code>, <code>-e .</code> (for editable installs), <code>--index-url</code>, and more.</p> </li> <li> <p>Resolution Strategy Customization:   uv allows customization of its resolution strategy. With <code>--resolution=lowest</code>, uv installs the lowest compatible versions for all dependencies, while <code>--resolution=lowest-direct</code> focuses on lowest compatible versions for direct dependencies and latest compatible versions for transitive dependencies.</p> </li> </ul> read this docu about how to add dependencies from a <code>requirements.txt</code> file"},{"location":"blog/2023/10/28/managing-python-dependencies-navigating-pip-pipenv-poetry-conda-and-more/#conclusion","title":"Conclusion","text":"<p>Congratulations! You've now equipped yourself with insights into managing Python dependencies effectively using pip, pipenv, poetry, conda and more. By understanding their strengths and use cases, you're better equipped to make informed decisions for your projects.</p> <p>Stay proactive in exploring and adapting these tools to optimize your Python development experience!</p>"},{"location":"blog/2023/10/28/managing-python-dependencies-navigating-pip-pipenv-poetry-conda-and-more/#related-links","title":"Related Links","text":""},{"location":"blog/2023/12/20/pathlib-tutorial-transitioning-to-simplified-file-and-directory-handling-in-python/","title":"Pathlib Tutorial: Transitioning to Simplified File and Directory Handling in Python","text":""},{"location":"blog/2023/12/20/pathlib-tutorial-transitioning-to-simplified-file-and-directory-handling-in-python/#introduction","title":"Introduction","text":"<p>Are you still using <code>import os</code> for file handling after 2020 ? Use <code>pathlib</code> instead !</p> <p>If you're moving away from command line operations or 'os' module to Python's <code>pathlib</code>, you're at the right place.</p> <p>Well, in this tutorial, we'll dive into the powerful <code>pathlib</code> module in Python. It offers a clean transition for users accustomed to CLI or 'os' for file and directory handling, providing an elegant and intuitive approach.</p>"},{"location":"blog/2023/12/20/pathlib-tutorial-transitioning-to-simplified-file-and-directory-handling-in-python/#key-considerations-for-choosing-os-or-pathlib","title":"Key Considerations for Choosing os or pathlib","text":"<p>While <code>os</code> and <code>pathlib</code> both handle file and directory operations, they differ significantly in their approach:</p> <code>os</code> <code>pathlib</code> <ul> <li>Procedural: Primarily functions for path operations.</li> <li>Built-in: A part of Python's standard library.</li> <li>String-based: Paths represented as strings.</li> </ul> <ul> <li>Object-oriented: Paths as Path objects.</li> <li>Introduced in Python 3.4: Not available in older Python versions.</li> <li>Enhanced functionality: Offers extensive methods for path manipulation.</li> <li>Cross-platform: Works well across different operating systems.</li> </ul>"},{"location":"blog/2023/12/20/pathlib-tutorial-transitioning-to-simplified-file-and-directory-handling-in-python/#making-the-transition","title":"Making the Transition","text":"<ul> <li>Preference for Object-Oriented Approach: <code>pathlib</code> provides a more intuitive and readable experience.</li> <li>Compatibility and Legacy Code: <code>os</code> may be necessary for older Python versions or existing code.</li> <li>Specific Functionality: Some advanced operations might be easier with one module over the other.</li> <li>Project Style and Conventions: Consider the overall project style and best practices.</li> </ul>"},{"location":"blog/2023/12/20/pathlib-tutorial-transitioning-to-simplified-file-and-directory-handling-in-python/#best-practices-for-smooth-transition","title":"Best Practices for Smooth Transition","text":"<ul> <li>Consistency: Maintain a uniform approach within a project for easier maintenance.</li> <li>Descriptive Naming: Use clear variable names for paths to enhance readability.</li> <li>Error Handling: Implement robust error handling for potential issues.</li> <li>Thorough Testing: Ensure correctness in file and directory operations.</li> </ul>"},{"location":"blog/2023/12/20/pathlib-tutorial-transitioning-to-simplified-file-and-directory-handling-in-python/#why-transition-to-pathlib","title":"Why Transition to <code>pathlib</code>?","text":"<p>Traditionally, file handling in Python often relied on the command line or the 'os' module. However, <code>pathlib</code> introduces an object-oriented paradigm, offering an intuitive and platform-independent solution.</p> <p>The transition to <code>pathlib</code> allows for:</p> <ul> <li>Simplified Path Representation: Paths as Path objects offer enhanced readability and functionality.</li> <li>Streamlined Syntax: Code becomes concise and more understandable using <code>pathlib</code>'s methods.</li> <li>Expanded Methodology: <code>pathlib</code> covers common path operations comprehensively.</li> </ul> <p>Let's explore the functionalities of <code>pathlib</code> step by step.</p> <p>Usage Examples</p> <pre><code>from pathlib import Path\n\n# Get the current working directory\ncwd = Path.cwd()\n\n# Create a new directory\nnew_dir = Path(\"my_new_directory\")\nnew_dir.mkdir()\n\n# Create a nested directory structure\nnested_dir = Path(\"data/processed/results\")\nnested_dir.mkdir(parents=True, exist_ok=True)  # Create all parent directories if needed\n\n# Check if a file exists\nfile_path = Path(\"my_file.txt\")\nif filepath.exists():\n    print(\"The file exists!\")\n\n# Read the contents of a file\ntext = filepath.read_text()\n\n# Write content to a file\nfilepath.write_text(\"New content for the file\")\n\n# Remove an empty directory\nempty_dir = Path(\"empty_dir\")\nempty_dir.rmdir()\n\n# Remove a non-empty directory and its contents\nnon_empty_dir = Path(\"non_empty_dir\")\nshutil.rmtree(non_empty_dir)\n</code></pre>"},{"location":"blog/2023/12/20/pathlib-tutorial-transitioning-to-simplified-file-and-directory-handling-in-python/#equivalence-os-vs-pathlib-vs-cli-for-common-operations","title":"Equivalence os vs pathlib vs cli for Common Operations","text":"<p>This table provides a comparison between Python's <code>os</code> module and the <code>pathlib</code> library operations alongside their Linux command equivalents for file and directory manipulation, information retrieval, traversal, file input/output (I/O), and path validation. It offers a comprehensive reference for developers familiar with Python who want to understand corresponding operations in Linux command line interfaces. The table is categorized by groups, making it easy to find specific functionalities and their corresponding commands in Python and Linux.</p> More on Table description <p>This table is a comprehensive guide detailing various file and directory operations along with path manipulation using Python's <code>os</code> and <code>pathlib</code> modules, alongside their Linux command equivalents.</p> <ul> <li> <p>Path Manipulation: Operations for obtaining the current working directory, joining paths, and checking path existence are provided.</p> </li> <li> <p>File and Directory Information: It includes functions to get file/directory size, list directory contents, and retrieve file-related timestamps like creation, modification, and access times.</p> </li> <li> <p>File and Directory Operations: This section covers creating and removing directories, renaming files/directories, copying files/directories, as well as commands for creating and deleting files directly.</p> </li> <li> <p>Path Information and Validation: Functions to check if a path points to a file or directory, checking if a path is absolute, and extracting file extensions are included.</p> </li> <li> <p>Path Traversal and Exploration: Operations to iterate through files matching a specific pattern, resolve absolute paths, and extract the parent directory or file/directory name are outlined.</p> </li> <li> <p>File I/O: Covers reading, writing, and appending contents to a file.</p> </li> <li> <p>Path Accessibility: How to check for path accessibility (read, write, execute permissions) using Python's <code>os</code> module alongside Linux command equivalents is explained.</p> </li> </ul> <p>Each operation is represented under its relevant group, detailing the equivalent Pythonic approach using <code>os</code> or <code>pathlib</code>, alongside their corresponding Linux command line alternatives. This table serves as a quick reference for performing common file system-related tasks using Python and Linux commands.</p> Group Operation os pathlib Linux Command Equivalent Path Manipulation Get current working directory <code>os.getcwd()</code> <code>Path.cwd()</code> <code>pwd</code> Join paths <code>os.path.join('/path', 'to', 'join')</code> <code>Path('/path') / 'to' / 'join'</code> <code>joinpath</code> Check path existence <code>os.path.exists('/path')</code> <code>Path('/path').exists()</code> <code>test -e /path</code> File and Directory Information Get file/directory size <code>os.path.getsize('/path')</code> <code>Path('/path').stat().st_size</code> <code>du -b /path</code> List directory contents <code>os.listdir('/path')</code> <code>[item.name for item in Path('/path').iterdir()]</code> <code>ls /path</code> Get file creation time <code>os.path.getctime('/path')</code> <code>Path('/path').stat().st_ctime</code> <code>stat -c %W /path</code> Get file last modification time <code>os.path.getmtime('/path')</code> <code>Path('/path').stat().st_mtime</code> <code>stat -c %Y /path</code> Get file last access time <code>os.path.getatime('/path')</code> <code>Path('/path').stat().st_atime</code> <code>stat -c %X /path</code> File and Directory Operations Create a directory <code>os.makedirs('/path')</code> <code>Path('/path').mkdir()</code> (for single directory), <code>Path('/path').mkdir(parents=True)</code> (for nested directories) <code>mkdir /path</code> Remove an empty directory <code>os.rmdir('/path')</code> <code>Path('/path').rmdir()</code> <code>rmdir /path</code> Remove a non-empty directory <code>shutil.rmtree('/path')</code> <code>shutil.rmtree(Path('/path'))</code> <code>rm -r /path</code> Rename a file/directory <code>os.rename('path/to/source', 'path/to/dest')</code> <code>Path('path/to/source').rename('path/to/dest')</code> <code>mv path/to/source path/to/dest</code> Copy a file <code>shutil.copy('/source/file', '/destination/file')</code> <code>Path('/source/file').replace('/destination/file')</code> <code>cp /source/file /destination/file</code> Copy a directory <code>shutil.copytree('/source/dir', '/destination/dir')</code> <code>shutil.copytree('/source/dir', '/destination/dir')</code> <code>cp -r /source/dir /destination/dir</code> Create a file <code>open('/file/path', 'w').close()</code> <code>Path('/file/path').touch()</code> <code>touch /file/path</code> Remove a file <code>os.remove('/file/path')</code> <code>Path('/file/path').unlink()</code> <code>rm /file/path</code> Path Information and Validation Check if the path is a file <code>os.path.isfile('/path')</code> <code>Path('/path').is_file()</code> <code>test -f /path</code> Check if the path is a directory <code>os.path.isdir('/path')</code> <code>Path('/path').is_dir()</code> <code>test -d /path</code> Check if the path is absolute <code>os.path.isabs('/path')</code> <code>Path('/path').is_absolute()</code> <code>readlink -f /path</code> Get the file extension <code>os.path.splitext('/path')[1]</code> <code>Path('/path').suffix</code> <code>echo /path | grep -o -P '\\.\\K.*'</code> Path Traversal and Exploration Iterate through files matching a pattern <code>glob.glob('/path')</code> <code>Path('/path').glob()</code> or <code>Path('/path').rglob()</code> for recursive search <code>find /path -name pattern</code> Resolve the absolute path <code>os.path.abspath('/path')</code> <code>Path('/path').resolve()</code> <code>realpath /path</code> Get the parent directory <code>os.path.dirname('/path')</code> <code>Path('/path').parent</code> <code>dirname /path</code> Get the file/directory name <code>os.path.basename('/path')</code> <code>Path('/path').name</code> <code>basename /path</code> File I/O Read the contents of a file <code>open('/file/path'', 'r').read()</code> <code>Path('/file/path'').read_text()</code> <code>cat /file/path</code> Write contents to a file <code>open('/file/path'', 'w').write(content)</code> <code>Path('/file/path'').write_text(content)</code> <code>echo content &gt; /file/path</code> Append contents to a file <code>open('/file/path', 'a').write(content)</code> <code>Path('/file/path'').open('a').write(content)</code> <code>echo content &gt;&gt; /file/path</code> Path Accessibility Check path accessibility <code>os.access()</code> Use <code>Path</code> methods in combination with <code>os.access()</code> or <code>os.stat()</code> <code>test -rwx /path</code> <p>Remember !</p> <ul> <li>Always handle potential errors with <code>try-except</code> blocks for file and directory operations.</li> <li>Test your code thoroughly to ensure correctness and handle edge cases.</li> <li>Embrace the object-oriented nature of pathlib for a more intuitive and readable approach to file handling in Python.</li> </ul>"},{"location":"blog/2023/12/20/pathlib-tutorial-transitioning-to-simplified-file-and-directory-handling-in-python/#conclusion","title":"Conclusion","text":"<p>Congratulations! You've now gained a foundational understanding of <code>pathlib</code> and its capabilities for file and directory handling in Python. Experiment further by combining these methods and explore additional functionalities for your file manipulation needs.</p> <p>Stay curious and keep exploring to harness the full potential of <code>pathlib</code> in your Python projects!</p>"},{"location":"blog/2023/12/20/pathlib-tutorial-transitioning-to-simplified-file-and-directory-handling-in-python/#relating-links","title":"relating links","text":"<ul> <li>Removing Directories in Python</li> </ul>"},{"location":"blog/2024/01/21/managing-python-projects-with-poetry-a-comprehensive-guide/","title":"Managing Python Projects with Poetry","text":"<p>Poetry simplifies Python project management and dependency handling. It's beginner-friendly and offers advantages like streamlined dependency management, integrated virtual environments, and simplified workflow. Give it a try to experience efficient Python development.</p> <p>Follow these steps to initialize, add dependencies, and manage your project effortlessly:</p>"},{"location":"blog/2024/01/21/managing-python-projects-with-poetry-a-comprehensive-guide/#1-install-poetry","title":"1. Install Poetry","text":"<p>If Poetry is not installed, use the following command:</p> <pre><code>pip install poetry\n</code></pre> <p></p>"},{"location":"blog/2024/01/21/managing-python-projects-with-poetry-a-comprehensive-guide/#2-create-a-new-project","title":"2. Create a New Project","text":"<p>Navigate to your desired project directory and run:</p> <pre><code>poetry new project_name\n</code></pre> <p>Replace <code>project_name</code> with your desired project name.</p>"},{"location":"blog/2024/01/21/managing-python-projects-with-poetry-a-comprehensive-guide/#3-add-dependencies","title":"3. Add Dependencies","text":"<p>Move into your project directory:</p> <pre><code>cd project_name\n</code></pre> <p>Add dependencies with:</p> <pre><code>poetry add package_name\n</code></pre> <p>Replace <code>package_name</code> with the desired package. Poetry manages dependencies in <code>pyproject.toml</code>.</p>"},{"location":"blog/2024/01/21/managing-python-projects-with-poetry-a-comprehensive-guide/#4-install-dependencies","title":"4. Install Dependencies","text":"<p>Install dependencies or update existing ones:</p> <pre><code>poetry install\n</code></pre> <p>This creates a virtual environment for your project.</p>"},{"location":"blog/2024/01/21/managing-python-projects-with-poetry-a-comprehensive-guide/#5-run-your-project","title":"5. Run Your Project","text":"<p>Execute Python scripts within the Poetry-managed environment:</p> <pre><code>poetry run python your_script.py\n</code></pre> <p>Replace <code>your_script.py</code> with your script's name.</p>"},{"location":"blog/2024/01/21/managing-python-projects-with-poetry-a-comprehensive-guide/#6-other-commands","title":"6. Other Commands","text":"<ul> <li>Update a package:</li> </ul> <pre><code>poetry update package_name\n</code></pre> <ul> <li>Access the virtual environment shell:</li> </ul> <pre><code>poetry shell\n</code></pre>"},{"location":"blog/2024/01/21/managing-python-projects-with-poetry-a-comprehensive-guide/#adding-a-script","title":"Adding a Script","text":"<p>Modify the <code>pyproject.toml</code> file to include a script:</p> <pre><code>...\n\n[tool.poetry.scripts]\nstart = \"uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload\"\n\n...\n</code></pre> <p>Run the development server with:</p> <pre><code>poetry run start\n</code></pre>"},{"location":"blog/2024/01/21/managing-python-projects-with-poetry-a-comprehensive-guide/#additional-dependencies","title":"Additional Dependencies","text":"<p>Add Jupyter as a development dependency:</p> <pre><code>poetry add jupyter --group dev\n</code></pre> <p>Modify <code>pyproject.toml</code>:</p> <pre><code>...\n\njupyter = \"notebook\"\n\n...\n</code></pre>"},{"location":"blog/2024/01/21/managing-python-projects-with-poetry-a-comprehensive-guide/#code-formatting-with-isort-and-black","title":"Code Formatting with isort and black","text":"<p>Install <code>isort</code> and <code>black</code>:</p> <pre><code>poetry add --dev isort black\n</code></pre> <p>Configure in <code>pyproject.toml</code>:</p> <pre><code>[tool.isort]\nprofile = \"black\"\n\n[tool.black]\nline-length = 88\n</code></pre> <p>Format code with:</p> <pre><code>poetry run format\n</code></pre>"},{"location":"blog/2024/01/21/managing-python-projects-with-poetry-a-comprehensive-guide/#grouped-package-installation","title":"Grouped Package Installation","text":"<p>Install a package in a group:</p> <pre><code>poetry add \"laspy[lazrs,laszip]==2.4.1\" --group laspy\n</code></pre>"},{"location":"blog/2024/01/21/managing-python-projects-with-poetry-a-comprehensive-guide/#modifying-python-version","title":"Modifying Python Version","text":"<p>Change Python version requirement in <code>pyproject.toml</code>:</p> <pre><code>[tool.poetry.dependencies]\npython = \"^3.9\"\nmatplotlib = \"3.7.1\"\n</code></pre> <p>Update dependencies:</p> <pre><code>poetry update\n</code></pre>"},{"location":"blog/2024/01/21/managing-python-projects-with-poetry-a-comprehensive-guide/#running-the-project","title":"Running the Project","text":"<p>Execute your project with:</p> <pre><code>poetry run your_command_here\n</code></pre> <p>Replace <code>your_command_here</code> with your actual command.</p>"},{"location":"blog/2024/01/21/managing-python-projects-with-poetry-a-comprehensive-guide/#generate-a-requirementstxt-file-using-poetry","title":"generate a <code>requirements.txt</code> file using Poetry","text":"<p>Poetry doesn't directly generate a <code>requirements.txt</code> file, but you can generate an equivalent file using the following command:</p> <pre><code>poetry export -f requirements.txt --output requirements.txt --without-hashes\n</code></pre> <p>This command exports your project's dependencies into a <code>requirements.txt</code> file. Here's what each part of the command does:</p> <ul> <li><code>-f requirements.txt</code>: Specifies the format of the output file.</li> <li><code>--output requirements.txt</code>: Sets the output file name.</li> <li><code>--without-hashes</code>: Exports the dependencies without including the hash information for each package.</li> </ul> <p>This command essentially creates a <code>requirements.txt</code> file that lists all the project dependencies along with their versions, making it similar to a standard <code>requirements.txt</code> file commonly used with <code>pip</code>.</p> <p>Remember that this file generated by Poetry won't include any packages managed exclusively by Poetry (like development dependencies or direct URLs to packages). It mainly generates a simplified version of requirements that can be used with <code>pip</code>.</p> <p>Run this command in the root directory of your Poetry-managed project, and it will generate the <code>requirements.txt</code> file based on your project's dependencies.</p> <p>Enjoy seamless Python project management with Poetry! Happy coding!</p>"},{"location":"blog/2024/03/16/publishing-your-python-project-with-poetry/","title":"Publishing Your Python Project with Poetry","text":"<p>Are you ready to share your Python project with the world and make it accessible to users and developers alike?</p> <p>Publishing your work not only showcases your creation but also enables others to benefit from and contribute to your project. Discover the simplest way to publish Python projects using Poetry, a powerful dependency management and packaging tool.</p> <p>In this guide, we'll walk you through setting up your project with Poetry and publishing it on both test.pypi and production PyPI.</p> <p>Get ready to make your project accessible for testing and distribution. Let's get started!</p>"},{"location":"blog/2024/03/16/publishing-your-python-project-with-poetry/#getting-started-with-poetry","title":"Getting Started with Poetry","text":"<p>Poetry is one of the simplest and most powerful tools for managing Python dependencies and packaging your projects. If you're new to Poetry, don't worry! We'll guide you through the process step by step.</p> <p>First, ensure you have Poetry installed on your system:</p> <pre><code>pip install poetry\n</code></pre> <p>Next, create your project using Poetry:</p> <pre><code>poetry new project-folder-name\n</code></pre> <p>Certainly! Here's the modified section with a visual representation of the folder structure:</p> <pre><code>Next, create your project using Poetry:\n\n```bash\npoetry new project-folder-name\n</code></pre> <p>When you run this command, Poetry will generate a project structure with the following folders and files:</p> <pre><code>project-folder-name/\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 your_main_python_file.py\n\u251c\u2500\u2500 tests/\n\u2502   \u2514\u2500\u2500 test_your_main_python_file.py\n\u251c\u2500\u2500 poetry.lock\n\u2514\u2500\u2500 .gitignore\n</code></pre> python project file architecture for packaging <ul> <li>project-folder-name/: This is the root directory of your project. It contains all the files and folders related to your project.</li> <li>pyproject.toml: This file is the Poetry configuration file where you define your project's dependencies, Python version, and other settings.</li> <li>README.rst: This file contains the documentation for your project. You can use reStructuredText format for writing documentation.</li> <li>src/: This folder is where your Python source code files reside. Placing your source files in a subfolder like this is important for proper project management. It helps keep your project organized and separate from other files, such as tests or configuration files.</li> <li>tests/: This folder is where you write tests for your project. Writing tests is crucial for ensuring your code works as expected and for maintaining code quality.</li> <li>poetry.lock: This file is automatically generated by Poetry and contains information about the exact versions of your project's dependencies.</li> <li>.gitignore: This file specifies which files and directories should be ignored by version control systems like Git. It helps keep irrelevant files out of your repository.</li> </ul> <p>Organizing your project files in this way helps maintain a clean and structured project layout, making it easier to manage, collaborate on, and maintain your codebase.</p> <p>For Python project management with Poetry, refer to this documentation. If you're not familiar with Poetry, you can check out this beginner's guide to get started or this this python dependency management cheat for usual commands.</p>"},{"location":"blog/2024/03/16/publishing-your-python-project-with-poetry/#publishing-your-project-on-test-pypi","title":"Publishing Your Project on Test PyPI","text":""},{"location":"blog/2024/03/16/publishing-your-python-project-with-poetry/#account-setup-one-time-process","title":"Account Setup (One-Time Process)","text":"<p>Before publishing your project on test.pypi, ensure your account setup is complete:</p> <ol> <li> <p>Create an account: Visit test.pypi.org and create an account using your email address.</p> </li> <li> <p>Verify your email: Check your inbox for a verification email from test.pypi.org and follow the instructions to verify your email address.</p> </li> <li> <p>Set up Two-Factor Authentication (2FA): Navigate to your account settings on test.pypi.org and enable 2FA for added security. Follow the instructions to set up 2FA using an authenticator app or SMS.</p> </li> </ol> <p>Once your account setup is complete, proceed with configuring Poetry for publishing your project.</p>"},{"location":"blog/2024/03/16/publishing-your-python-project-with-poetry/#configuring-poetry-for-test-pypi-one-time-setup","title":"Configuring Poetry for Test PyPI (One-Time Setup)","text":"<p>Follow these steps to configure Poetry for publishing on test.pypi:</p> <ol> <li> <p>Add the test.pypi repository to your Poetry config:</p> <pre><code>poetry config repositories.test-pypi https://test.pypi.org/legacy/\n</code></pre> </li> <li> <p>Obtain a token from test.pypi.org and store it:</p> <pre><code>poetry config pypi-token.test-pypi pypi-YYYYYYYY\n</code></pre> </li> </ol>"},{"location":"blog/2024/03/16/publishing-your-python-project-with-poetry/#publishing-to-test-pypi","title":"Publishing to Test PyPI","text":"<p>When you're ready to publish your project for testing:</p> <ol> <li> <p>Bump the version using Poetry:</p> <pre><code>poetry version prerelease\n# or\npoetry version patch\n</code></pre> </li> <li> <p>Publish your project to test.pypi:</p> <pre><code>poetry publish -r test-pypi\n</code></pre> </li> </ol>"},{"location":"blog/2024/03/16/publishing-your-python-project-with-poetry/#publishing-your-project-on-pypi","title":"Publishing Your Project on PyPI","text":"<p>Once you've tested your project on test.pypi and are ready to release it to the public, follow these steps:</p> <ol> <li> <p>Obtain a token from pypi.org and store it:</p> <pre><code>poetry config pypi-token.pypi pypi-XXXXXXXX\n</code></pre> </li> <li> <p>Bump the version using Poetry:</p> <pre><code>poetry version patch\n</code></pre> </li> <li> <p>Publish your project to PyPI:</p> <pre><code>poetry publish\n</code></pre> </li> </ol>"},{"location":"blog/2024/03/16/publishing-your-python-project-with-poetry/#conclusion","title":"Conclusion","text":"<p>Congratulations! You've learned how to publish your Python project using Poetry. Whether you're testing your package on test.pypi or releasing it to the production PyPI, Poetry simplifies the process and ensures smooth distribution of your projects.</p> <p>For further reference, you can also check out this Stack Overflow answer demonstrating the publishing process.</p> <p>Now, go ahead and share your amazing Python projects with the world!</p> <p>Related resources:</p> <ul> <li>Test PyPI, PyPI, Python Packaging</li> <li>beginner's guide to poetry, python dependency management cheat</li> <li>Introducing Two New Packages for Streamlining File Conversions in Python</li> </ul>"},{"location":"blog/2024/02/12/introduction-to-python-code-formatters-and-sorters/","title":"Introduction to Python Code Formatters and Sorters","text":""},{"location":"blog/2024/02/12/introduction-to-python-code-formatters-and-sorters/#introduction","title":"Introduction","text":"<p>Are you struggling to maintain consistent formatting in your Python code? Do you find yourself spending too much time organizing imports or adjusting code style manually?</p> <p>Navigating the landscape of Python code formatters and sorters can be overwhelming, especially for beginners.</p> <p>This guide serves as your roadmap to mastering Python code formatters and sorters, simplifying the process and providing practical examples for effective code formatting and organization.</p> <p>Whether you're a novice Python developer or an experienced programmer looking to streamline your workflow, this document is tailored to demystify Python code formatters and sorters, offering insights into popular tools like Black, isort, and YAPF. By the end, you'll have a solid understanding of how to leverage these tools to enhance the readability and maintainability of your Python codebase.</p>"},{"location":"blog/2024/02/12/introduction-to-python-code-formatters-and-sorters/#overview","title":"Overview","text":"<p>Python code formatters and sorters are tools designed to improve the readability and maintainability of Python code by enforcing consistent formatting and organization standards. These tools automatically format your code according to predefined rules, saving time and ensuring adherence to best practices. In this document, we'll explore some popular Python code formatters and sorters, including Black, isort, YAPF, and more.</p>"},{"location":"blog/2024/02/12/introduction-to-python-code-formatters-and-sorters/#black","title":"Black","text":"<p>Black is a highly opinionated Python code formatter that reformats your code to follow a consistent style guide. It offers a \"one true way\" approach to code formatting, ensuring that all code formatted with Black adheres to the same style. Black is known for its simplicity and ability to produce clean, readable code without configuration.</p>"},{"location":"blog/2024/02/12/introduction-to-python-code-formatters-and-sorters/#installation","title":"Installation","text":"<p>You can install Black using pip:</p> <pre><code>pip install black\n</code></pre>"},{"location":"blog/2024/02/12/introduction-to-python-code-formatters-and-sorters/#usage","title":"Usage","text":"<p>To format a Python file with Black, simply run:</p> <pre><code>black your_file.py\n</code></pre> More Options for Black <p>Black provides several options for formatting Python code, including:</p> <ul> <li> <p>Format a specific file: <pre><code>black myfile.py\n</code></pre></p> </li> <li> <p>Format all Python files in the current directory: <pre><code>black *.py\n</code></pre></p> </li> <li> <p>Format all Python files in a directory and its subdirectories: <pre><code>black myfolder\n</code></pre></p> </li> <li> <p>Formatting with Black Using a Maximum Line Length of 100 Characters     <pre><code>black . -l 100\n</code></pre></p> </li> </ul>"},{"location":"blog/2024/02/12/introduction-to-python-code-formatters-and-sorters/#isort","title":"isort","text":"<p>isort is a Python utility that sorts import statements within your Python code. It organizes import statements alphabetically and groups them according to predefined categories. isort ensures a consistent import order across your codebase, improving readability and maintainability.</p>"},{"location":"blog/2024/02/12/introduction-to-python-code-formatters-and-sorters/#installation_1","title":"Installation","text":"<p>You can install isort using pip:</p> <pre><code>pip install isort\n</code></pre>"},{"location":"blog/2024/02/12/introduction-to-python-code-formatters-and-sorters/#usage_1","title":"Usage","text":"<p>To sort import statements within a Python file with isort, run:</p> <pre><code>isort your_file.py\n</code></pre> More Options for isort <ul> <li> <p>Sorting Imports with isort Using the Black Profile</p> <p>To sort import statements using isort with the Black profile, run:</p> <pre><code>isort --profile black\n</code></pre> </li> </ul>"},{"location":"blog/2024/02/12/introduction-to-python-code-formatters-and-sorters/#yapf-yet-another-python-formatter","title":"YAPF (Yet Another Python Formatter)","text":"<p>YAPF is a Python code formatter developed by Google. It aims to provide a highly configurable code formatting solution while maintaining simplicity and ease of use. YAPF offers a wide range of formatting options, allowing you to customize its behavior according to your project's needs.</p>"},{"location":"blog/2024/02/12/introduction-to-python-code-formatters-and-sorters/#installation_2","title":"Installation","text":"<p>You can install YAPF using pip:</p> <pre><code>pip install yapf\n</code></pre>"},{"location":"blog/2024/02/12/introduction-to-python-code-formatters-and-sorters/#usage_2","title":"Usage","text":"<p>To apply YAPF to a single file, use the following command:</p> <pre><code>yapf -i your_file.py\n</code></pre> <p>Replace <code>your_file.py</code> with the name of the Python file you want to format.</p> <p>To apply YAPF to multiple files, specify each file separated by spaces:</p> <pre><code>yapf -i file1.py file2.py file3.py\n</code></pre> <p>To apply YAPF recursively to all Python files in a folder, navigate to the folder containing your Python files and run:</p> <pre><code>yapf -i ./*.py\n</code></pre> <p>This command will format all <code>.py</code> files in the current directory.</p> More Options for YAPF <ul> <li> <p>Specify Line Length: You can specify the line length for formatting using the <code>-l</code> or <code>--style</code> option:</p> <pre><code>yapf -i --style=\"{based_on_style: google, column_limit: 120}\" your_file.py\n</code></pre> <p>This command formats <code>your_file.py</code> with a line length limit of 120 characters, based on the Google style guide.</p> </li> <li> <p>Preserve Existing Formatting: YAPF allows you to preserve the existing formatting to some extent using the <code>--style</code> option:</p> <pre><code>yapf -i --style=\"{based_on_style: pep8, indent_width: 4}\" your_file.py\n</code></pre> <p>This command formats <code>your_file.py</code> while preserving the existing indentation width of 4 spaces, based on the PEP 8 style guide.</p> </li> <li> <p>Dry Run Mode: You can use the <code>--diff</code> option to perform a dry run and see the proposed changes without actually modifying the files:</p> <pre><code>yapf --diff your_file.py\n</code></pre> <p>This command displays the proposed changes to <code>your_file.py</code> without modifying the file itself.</p> </li> <li> <p>Recursive Formatting: To format Python files in the current directory and all subdirectories, use the <code>--recursive</code> flag:</p> <pre><code>yapf --in-place --recursive .\n</code></pre> <p>This command recursively formats all <code>.py</code> files starting from the current directory and traversing through all subdirectories.</p> </li> </ul>"},{"location":"blog/2024/02/12/introduction-to-python-code-formatters-and-sorters/#other-python-code-formatters-and-sorters","title":"Other Python Code Formatters and Sorters","text":"<p>In addition to Black, isort, and YAPF, there are several other Python code formatters and sorters available, each with its own set of features and capabilities. Some notable mentions include:</p> <ul> <li>autopep8: A tool that automatically formats Python code to conform to the PEP 8 style guide.</li> <li>pyfmt: A code formatter that aims to balance flexibility and readability, offering customizable formatting options.</li> <li>pycodestyle: A tool that checks Python code against the PEP 8 style guide and provides suggestions for improvements.</li> </ul>"},{"location":"blog/2024/02/12/introduction-to-python-code-formatters-and-sorters/#supercharge-your-ci-with-formatters","title":"Supercharge Your CI with Formatters","text":"<p>Combining isort and Black</p> <p>You can combine isort and Black to format and sort your Python files efficiently:</p> <pre><code>#!/bin/bash\nisort .\nblack . -l 100\n</code></pre> <p>This script first sorts the import statements with isort and then formats the code with Black, ensuring a consistent style and organization.</p> <p>Example with find + isort and Black</p> <p>To recursively format and sort Python files in a directory and its subdirectories, while excluding certain directories like 'env' and '.git', you can use the following commands:</p> <pre><code>#!/bin/bash\nfind . -type f -name \"*.py\" ! -path \"*env/*\" ! -path \"*.git/*\" -exec isort {} +\nfind . -type f -name \"*.py\" ! -path \"*env/*\" ! -path \"*.git/*\" -exec black -l 100 {} +\n</code></pre> <p>These commands will find all Python files in the current directory and its subdirectories, excluding specific directories like 'env' and '.git', and then apply isort and Black with a maximum line length of 100 characters, ensuring clean and well-organized code. Skipping these directories helps avoid unnecessary file processing and ensures a faster and more efficient formatting process.</p>"},{"location":"blog/2024/02/12/introduction-to-python-code-formatters-and-sorters/#conclusion","title":"Conclusion","text":"<p>Python code formatters and sorters are valuable tools for improving the consistency, readability, and maintainability of Python code. By using tools like Black, isort, YAPF, and others, developers can ensure that their code adheres to established standards and best practices, leading to more efficient and collaborative development processes.</p>"},{"location":"blog/2024/02/12/introduction-to-python-code-formatters-and-sorters/#related-links","title":"Related links","text":"<ul> <li>Some notes on the right formater to use for your project</li> </ul>"},{"location":"blog/2023/11/17/removing-directories-in-python/","title":"Removing Directories in Python","text":""},{"location":"blog/2023/11/17/removing-directories-in-python/#remove-empty-directories-in-python","title":"Remove Empty Directories in Python","text":"<p>To remove an empty directory, you can use <code>os.rmdir()</code> with os and <code>Path.rmdir()</code> with pathlib.</p> <code>index.py</code> <code>index.py</code> <pre><code>import os\n\ndirectory_path = '/path/to/empty_directory'\n\ntry:\n    os.rmdir(directory_path)\n    print(f\"The directory '{directory_path}' has been successfully removed.\")\nexcept OSError as e:\n    print(f\"Error: {directory_path} : {e.strerror}\")\n</code></pre> <pre><code>from pathlib import Path\n\ndirectory_path = Path('/path/to/empty_directory')\n\ntry:\n    directory_path.rmdir()\n    print(f\"The directory '{directory_path}' has been successfully removed.\")\nexcept OSError as e:\n    print(f\"Error: {directory_path} : {e.strerror}\")\n</code></pre>"},{"location":"blog/2023/11/17/removing-directories-in-python/#remove-non-empty-directories-in-python","title":"Remove Non-Empty Directories in Python","text":"<p>For directories that contain files or other directories, use <code>shutil.rmtree()</code> to remove them along with their contents.</p> <code>index.py</code> <code>index.py</code> <pre><code>import shutil\n\ndirectory_path = '/path/to/non_empty_directory'\n\ntry:\n    shutil.rmtree(directory_path)\n    print(f\"The directory '{directory_path}' and its contents have been successfully removed.\")\nexcept OSError as e:\n    print(f\"Error: {directory_path} : {e.strerror}\")\n</code></pre> <pre><code>from pathlib import Path\nimport shutil\n\ndirectory_path = Path('/path/to/non_empty_directory')\n\ntry:\n    shutil.rmtree(directory_path)\n    print(f\"The directory '{directory_path}' and its contents have been successfully removed.\")\nexcept OSError as e:\n    print(f\"Error: {directory_path} : {e.strerror}\")\n</code></pre> <p>Ensure to confirm the directory paths before removal operations to prevent accidental deletion of important data. When using shutil.rmtree(), exercise caution as it permanently deletes directories and their contents. The choice between os and pathlib can depend on your preference for object-oriented or procedural-style programming when handling paths and directories in Python.</p>"},{"location":"blog/2023/11/17/removing-directories-in-python/#relating-links","title":"relating links","text":"<ul> <li>Pathlib Tutorial: Transitioning to Simplified File and Directory Handling in Python</li> </ul>"},{"location":"blog/2024/02/10/simplified-logging-in-python-a-practical-guide-to-effective-debugging-and-monitoring/","title":"Simplified Logging in Python: A Practical Guide to Effective Debugging and Monitoring","text":""},{"location":"blog/2024/02/10/simplified-logging-in-python-a-practical-guide-to-effective-debugging-and-monitoring/#introduction","title":"Introduction","text":"<p>Are you still using print() statements for debugging in Python? Upgrade your logging game with Python's built-in logging module or the Loguru library!</p> <p>If you're tired of scattered print statements cluttering your codebase, it's time to embrace the power of logging in Python. Whether you're a beginner or an experienced developer, mastering logging techniques is essential for effective debugging, monitoring, and troubleshooting of your Python applications.</p> <p>Logging in Python allows you to set different levels of logging, such as DEBUG, INFO, WARNING, ERROR, and CRITICAL. With these levels, you can control the verbosity of log messages and focus on the information relevant to your current task.</p> <p>In this comprehensive guide, we'll explore both the built-in logging module and the Loguru library, offering practical examples and best practices for seamless integration into your projects. Say goodbye to ad-hoc debugging and hello to structured and manageable logs!</p>"},{"location":"blog/2024/02/10/simplified-logging-in-python-a-practical-guide-to-effective-debugging-and-monitoring/#advantages-of-logging-in-python","title":"Advantages of Logging in Python","text":"<p>Logging in Python offers several advantages over ad-hoc debugging using print statements. Here are some key benefits:</p>"},{"location":"blog/2024/02/10/simplified-logging-in-python-a-practical-guide-to-effective-debugging-and-monitoring/#structured-output","title":"Structured Output","text":"<p>Unlike print statements, which often result in unstructured output scattered throughout the codebase, logging allows developers to generate structured logs with predefined formats. This structured output makes it easier to parse and analyze log data, leading to improved debugging and troubleshooting.</p>"},{"location":"blog/2024/02/10/simplified-logging-in-python-a-practical-guide-to-effective-debugging-and-monitoring/#granular-control","title":"Granular Control","text":"<p>With logging, developers can control the verbosity of log messages by setting different logging levels (e.g., DEBUG, INFO, WARNING, ERROR, CRITICAL). This granular control enables developers to filter log messages based on their severity, allowing them to focus on relevant information and ignore less critical messages.</p>"},{"location":"blog/2024/02/10/simplified-logging-in-python-a-practical-guide-to-effective-debugging-and-monitoring/#modularization","title":"Modularization","text":"<p>Logging encourages modularization of code by promoting the separation of concerns. By logging messages within specific modules or components, developers can track the flow of execution and identify potential issues more effectively. Additionally, modular logging facilitates collaboration among team members by providing insight into each component's behavior.</p>"},{"location":"blog/2024/02/10/simplified-logging-in-python-a-practical-guide-to-effective-debugging-and-monitoring/#runtime-configuration","title":"Runtime Configuration","text":"<p>Logging in Python allows for runtime configuration, meaning developers can adjust logging settings without modifying the source code. This flexibility is particularly useful in scenarios where different logging configurations are required for development, testing, and production environments.</p>"},{"location":"blog/2024/02/10/simplified-logging-in-python-a-practical-guide-to-effective-debugging-and-monitoring/#performance-monitoring","title":"Performance Monitoring","text":"<p>Logging is essential for performance monitoring and profiling of Python applications. By logging performance-related metrics such as execution times, memory usage, and resource consumption, developers can identify bottlenecks and optimize the performance of their applications.</p>"},{"location":"blog/2024/02/10/simplified-logging-in-python-a-practical-guide-to-effective-debugging-and-monitoring/#error-handling","title":"Error Handling","text":"<p>Effective error handling is crucial for robust Python applications. Logging provides a centralized mechanism for capturing and reporting errors, allowing developers to track the occurrence of exceptions and trace their origins. This helps in identifying and addressing potential issues before they impact the application's functionality.</p>"},{"location":"blog/2024/02/10/simplified-logging-in-python-a-practical-guide-to-effective-debugging-and-monitoring/#integration-with-monitoring-tools","title":"Integration with Monitoring Tools","text":"<p>Logging seamlessly integrates with monitoring and alerting tools, enabling developers to monitor the health and performance of their applications in real-time. By integrating logging with tools like Prometheus, Grafana, or ELK Stack, developers can gain valuable insights into their application's behavior and take proactive measures to maintain its reliability.</p> <p>In the following sections, we'll delve deeper into the features and usage of both the built-in logging module and the Loguru library, showcasing practical examples and best practices for effective logging in Python.</p>"},{"location":"blog/2024/02/10/simplified-logging-in-python-a-practical-guide-to-effective-debugging-and-monitoring/#built-in-logging","title":"Built-in Logging","text":""},{"location":"blog/2024/02/10/simplified-logging-in-python-a-practical-guide-to-effective-debugging-and-monitoring/#basic-setup","title":"Basic Setup","text":"<p>Basic Setup with Built-in Logging</p> <pre><code>import logging\n\n# Creating a logger\nlogger = logging.getLogger()\n\n# Setting up a namespace (e.g., grpc)\nlogging.getLogger(\"grpc\")\n</code></pre>"},{"location":"blog/2024/02/10/simplified-logging-in-python-a-practical-guide-to-effective-debugging-and-monitoring/#using-handlers","title":"Using Handlers","text":"<p>Handlers in logging are responsible for taking the log records (created by loggers) and outputting them to the desired destination, such as the console, files, or network sockets. Using handlers allows developers to control where log messages are sent and how they are formatted. For example, a console handler might be used for debugging during development, while a file handler could be used to store logs for later analysis.</p> <p>Using Handlers with Built-in Logging</p> <pre><code># Adding a console handler\nconsole_handler = logging.StreamHandler()\nformatter = logging.Formatter(fmt=\"%(asctime)s: %(levelname)-8s %(message)s\")\nconsole_handler.setFormatter(formatter)\n\n# Clearing any existing handlers and adding the console handler\nlogger.handlers = []\nlogger.addHandler(console_handler)\n\n# Setting the logging level\nlogger.setLevel(logging.WARNING)  # Adjust the level as needed\n</code></pre>"},{"location":"blog/2024/02/10/simplified-logging-in-python-a-practical-guide-to-effective-debugging-and-monitoring/#logging-levels","title":"Logging Levels","text":"<p>Logging levels provide a way to categorize log messages based on their severity. By setting the logging level, developers can control which messages are emitted by the logger. This is essential for managing the volume of log output and focusing on relevant information. For instance, during development, setting the level to DEBUG allows developers to see detailed debugging information, while in production, it might be set to WARNING or higher to only capture critical issues.</p> <p>Logging Levels with Built-in Logging</p> <pre><code>logger.debug(\"Debug message\")\nlogger.info(\"Info message\")\n# Other logging levels such as WARNING, ERROR, and CRITICAL can also be used\n</code></pre>"},{"location":"blog/2024/02/10/simplified-logging-in-python-a-practical-guide-to-effective-debugging-and-monitoring/#suppressing-logging","title":"Suppressing Logging","text":"<p>Sometimes during testing or specific scenarios, it's beneficial to suppress logging within certain namespaces or contexts to keep the output clean and focused on relevant information. This is especially useful when running automated tests or building processes where excessive logging could clutter the output or interfere with the test results.</p> <p>To achieve this, use the <code>suppress_logging</code> context manager, which temporarily disables logging</p> <p>within the specified namespace.</p> <p>Suppressing Logging with Built-in Logging</p> <pre><code>from contextlib import contextmanager\n\n@contextmanager\ndef suppress_logging(namespace):\n    \"\"\"\n    Suppress logging within a specific namespace to keep output clean during testing or build processes.\n    \"\"\"\n    logger = logging.getLogger(namespace)\n    old_value = logger.disabled\n    logger.disabled = True\n    try:\n        yield\n    finally:\n        logger.disabled = old_value\n\n# Usage example:\nwith suppress_logging(\"your_namespace\"):\n    # Your code using logging goes here\n    pass\n</code></pre>"},{"location":"blog/2024/02/10/simplified-logging-in-python-a-practical-guide-to-effective-debugging-and-monitoring/#loguru","title":"Loguru","text":"<p>Loguru is a powerful logging library for Python that simplifies logging configuration and provides advanced features such as automatic inclusion of file, function, and line information in log messages. It offers a more intuitive and flexible logging experience compared to the built-in logging module, making it a popular choice among developers.</p> <p></p>"},{"location":"blog/2024/02/10/simplified-logging-in-python-a-practical-guide-to-effective-debugging-and-monitoring/#installation","title":"Installation","text":"<pre><code>pip install loguru\n</code></pre>"},{"location":"blog/2024/02/10/simplified-logging-in-python-a-practical-guide-to-effective-debugging-and-monitoring/#usage","title":"Usage","text":"<p>Usage with Loguru</p> <pre><code>from loguru import logger\n\n# Logging messages with Loguru\nlogger.debug(\"A debug message\")\nlogger.info(\"An info message\")\nlogger.warning(\"A warning message\")\nlogger.error(\"An error message\")\nlogger.critical(\"A critical message\")\n\n# Loguru output automatically includes by default module name, function name as well as line information\n</code></pre>"},{"location":"blog/2024/02/10/simplified-logging-in-python-a-practical-guide-to-effective-debugging-and-monitoring/#conclusion","title":"Conclusion","text":"<p>Logging plays a crucial role in Python development by providing a structured and manageable approach to debugging, monitoring, and troubleshooting applications. Whether using the built-in logging module or the Loguru library, developers can leverage logging to improve code quality, streamline development workflows, and enhance application reliability.</p> <p>By mastering logging techniques and best practices outlined in this guide, developers can effectively manage log output, control verbosity, modularize code, monitor performance, handle errors, and integrate with monitoring tools. With structured and informative logs, Python developers can gain valuable insights into their applications, leading to more efficient development cycles and improved overall software quality.</p>"},{"location":"blog/2023/11/15/how-to-deploy-a-streamlit-application-on-hugging-face/","title":"How to deploy a Streamlit Application on Hugging Face","text":"<p>In this guide, we'll walk through the steps to deploy a Streamlit app using the Hugging Face platform. For demonstration purposes, we'll create an app that utilizes the Python module Pix2Tex. Users will be able to upload an image and get the corresponding LaTeX formula along with a rendered version.</p>"},{"location":"blog/2023/11/15/how-to-deploy-a-streamlit-application-on-hugging-face/#prerequisites","title":"Prerequisites","text":"<ol> <li>Create a virtual environment:</li> </ol> <code>For Linux &amp; Mac</code> <code>For Windows</code> <pre><code>python3 -m venv myenv\nsource myenv/bin/activate\n</code></pre> <pre><code>python -m venv myenv\n./myenv/Scripts/activate\n</code></pre> <ol> <li> <p>Install dependencies:</p> <pre><code>pip install streamlit pix2tex pillow\n</code></pre> </li> </ol>"},{"location":"blog/2023/11/15/how-to-deploy-a-streamlit-application-on-hugging-face/#creating-the-streamlit-app","title":"Creating the Streamlit App","text":"<ol> <li> <p>Create the <code>app.py</code> file with your Streamlit app code.</p> <pre><code># app.py\nimport streamlit as st\nfrom PIL import Image\nfrom pix2tex.cli import LatexOCR\n\ndef main():\n    st.title(\"Image to LaTeX Formula Parser\")\n\n    # Upload image through Streamlit\n    uploaded_image = st.file_uploader(\"Upload an image\", type=[\"jpg\", \"jpeg\", \"png\"])\n\n    if uploaded_image is not None:\n        # Display the uploaded image\n        st.image(uploaded_image, caption=\"Uploaded Image\", use_column_width=True)\n\n        # Perform LaTeX OCR on the uploaded image\n        latex_formula = process_image(uploaded_image)\n\n        # Display LaTeX formula\n        st.subheader(\"LaTeX Formula:\")\n        st.text(latex_formula)\n\n        # Display parsed Markdown\n        parsed_md = parse_to_md(latex_formula)\n        st.subheader(\"Parsed Markdown:\")\n        st.latex(f\"\\n{latex_formula}\\n\")\n\ndef process_image(image):\n    # Perform LaTeX OCR on the image\n    img = Image.open(image)\n    model = LatexOCR()\n    latex_formula = model(img)\n    return latex_formula\n\ndef parse_to_md(latex_formula):\n    # You can implement your own logic to parse LaTeX to Markdown\n    # Here's a simple example for demonstration purposes\n    parsed_md = f\"**Parsed Formula:** *{latex_formula}*\"\n    return parsed_md\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> </li> <li> <p>Test the app locally:</p> <pre><code>streamlit run app.py\n</code></pre> </li> </ol>"},{"location":"blog/2023/11/15/how-to-deploy-a-streamlit-application-on-hugging-face/#deploying-on-hugging-face","title":"Deploying on Hugging Face","text":"<ol> <li> <p>Create a new Hugging Face space:</p> <ul> <li>Visit Hugging Face Spaces.</li> <li>Name your repository and choose the Streamlit option.</li> </ul> <p></p> </li> <li> <p>Create a Hugging Face token:</p> <ul> <li>Visit Hugging Face Token Settings.</li> <li>Create a new token. the format is <code>hf_****</code></li> </ul> <p> </p> </li> <li> <p>Clone the repository and push your app:</p> <ul> <li> <p>clone the repo</p> <pre><code>git clone https://&lt;user_name&gt;:&lt;token&gt;@huggingface.co/&lt;user_name&gt;/&lt;repo_name&gt;\ncd &lt;repo_name&gt;\n</code></pre> </li> </ul> <p>the token is from step 6 - add <code>app.py</code> and <code>requirements.txt</code> into the folder - push the changes to huggingface</p> <pre><code>```bash\ngit add .\ngit commit -m \"Add application\"\ngit push\n```\n</code></pre> <ul> <li> <p>example</p> <pre><code>git clone https://hermann-web:hf_****@huggingface.co/spaces/hermann-web/pix2tex \ncd pix2tex\nmv ../app.py app.py\npip freeze &gt; requirements.txt\ngit add .\ngit commit -m \"Add application\"\ngit push \n</code></pre> </li> </ul> </li> <li> <p>Check your deployed application online:</p> <ul> <li>Visit Hugging Face Repository <code>https://huggingface.co/&lt;user_name&gt;/&lt;repo_name&gt;</code>.</li> </ul> <p></p> </li> </ol> <p>Congratulations! Your Streamlit app is now deployed on Hugging Face. You can find a live example on https://huggingface.co/spaces/hermann-web/pix2tex.</p>"},{"location":"blog/2023/11/13/simple-guide-to-using-docker-on-windows-10-and-access-from-wsl-2/","title":"Simple guide to using Docker on Windows 10 and access from WSL 2","text":""},{"location":"blog/2023/11/13/simple-guide-to-using-docker-on-windows-10-and-access-from-wsl-2/#introduction","title":"Introduction","text":"<p>For a heavy linux user like me, using windows also mean find a door to work with a linux distro. There are several options to gauge from the situation. I usually need both docker and wsl2 on the computer. And i've installed this many times.</p> <p>So, this is a very straight forward tutorial on docker and wsl2 installation and configuration on windows10 or 11.</p>"},{"location":"blog/2023/11/13/simple-guide-to-using-docker-on-windows-10-and-access-from-wsl-2/#prerequisites","title":"Prerequisites","text":"<ul> <li>Windows 10 Pro, Enterprise, or Education edition.</li> <li>WSL 2 enabled on your Windows machine.</li> <li>Docker Desktop for Windows installed.</li> </ul>"},{"location":"blog/2023/11/13/simple-guide-to-using-docker-on-windows-10-and-access-from-wsl-2/#step-by-step-guide","title":"Step-by-Step Guide","text":"<ol> <li> <p>Install Docker Desktop for Windows:</p> <ul> <li>Download Docker Desktop for Windows from the official Docker website: https://www.docker.com/products/docker-desktop</li> <li>Run the installer and follow the instructions to complete the installation.</li> </ul> </li> <li> <p>Enable WSL 2:</p> Enable docker for wsl2 <p></p> </li> <li> <p>Set wsl default version to 2:</p> <ul> <li>Open PowerShell as an administrator and run the following command:</li> </ul> <pre><code>wsl --set-default-version 2\n</code></pre> </li> </ol> <ol> <li> <p>Install a Linux Distribution:</p> <ul> <li>Open the Microsoft Store app on your Windows machine.</li> <li>Search for a Linux distribution (e.g., Ubuntu, Debian, or Alpine) and install it.</li> </ul> wsl installer <p></p> </li> <li> <p>Configure WSL 2 as the Default WSL Version:</p> <ul> <li>see the list of distro you have</li> </ul> <pre><code>wsl.exe -l -v\n</code></pre> <ul> <li>Open PowerShell as an administrator and run the following command:</li> </ul> <pre><code>wsl --set-version &lt;distribution_name&gt; 2\n</code></pre> <p>Replace <code>&lt;distribution_name&gt;</code> with the name of the Linux distribution you installed.</p> </li> <li> <p>Start the Linux Distribution:</p> <ul> <li>Launch the Linux distribution you installed from the Start menu or by running its executable.</li> <li>Follow the initial setup instructions to create a user account and set a password.</li> </ul> </li> <li> <p>Add the Docker host configuration:</p> <ul> <li>Scroll to the end of the file using the arrow keys.</li> <li>Add the following line:</li> </ul> <pre><code>echo \"export DOCKER_HOST=unix:///var/run/docker.sock\" &gt;&gt; ~/.bashrc\n</code></pre> <ul> <li>I didn't need this one but who knows:</li> </ul> <pre><code># open the bashrc\nnano ~/.bashrc\n# add this line in the file\nexport DOCKER_HOST=tcp://localhost:2375\n</code></pre> </li> <li> <p>Reload the updated <code>.bashrc</code> file:</p> <ul> <li>Run the following command to apply the changes:</li> </ul> <pre><code>source ~/.bashrc\n</code></pre> </li> <li> <p>Verify Docker connectivity:</p> <ul> <li>Run the following command to check if the Docker client in WSL can connect to Docker Desktop:</li> </ul> <pre><code>docker version\n</code></pre> </li> <li> <p>Enable WSL Integration in Docker Desktop:</p> <ul> <li>Open Docker Desktop on your Windows machine.</li> <li>Right-click on the Docker Desktop icon in the system tray (notification area) and select \"Settings\".</li> <li>In the \"Settings\" window, navigate to the \"Resources\" section.</li> <li>Click on \"WSL Integration\" in the left sidebar.</li> <li>Toggle the switch next to your WSL distribution (e.g., Ubuntu) to enable integration.</li> <li>Click \"Apply &amp; Restart\" to save the changes and restart Docker Desktop.</li> </ul> </li> <li> <p>Verify Docker Connectivity in WSL:</p> <ul> <li>Open the WSL terminal (Ubuntu).</li> <li>Run the following command to check Docker connectivity:</li> </ul> <pre><code>docker version\n</code></pre> </li> </ol> <p>That's it! You should now have Docker successfully configured and running on Windows 10 with WSL 2. You can now use Docker commands within your Ubuntu WSL terminal to manage containers, images, and other Docker resources.</p> <p>If you encounter any issues or have further questions, feel free to ask. Happy Dockerizing!</p>"},{"location":"blog/2023/11/13/simple-guide-to-using-docker-on-windows-10-and-access-from-wsl-2/#basic-commands-for-working-with-wsl","title":"basic commands for working with wsl <sup>2</sup>","text":"<ul> <li>install wsl: <code>wsl --install</code></li> <li>check list of distro (name and version): <code>wsl.exe -l -v</code></li> <li>change from v1 to v2: <code>wsl.exe --set-version Ubuntu 2</code> <sup>1</sup></li> <li>unsinstall the wsl: <code>wsl.exe --unregister Ubuntu-22.04</code> where <code>Ubuntu-22.04</code> is my distro name i got</li> <li>see images <code>docker images</code></li> </ul>"},{"location":"blog/2023/11/13/simple-guide-to-using-docker-on-windows-10-and-access-from-wsl-2/#related-links","title":"Related links","text":"<ul> <li>Mastering Docker: A Comprehensive Guide to Efficient Container Management</li> </ul> <ol> <li> <p>Running Docker on WSL2 without Docker Desktop (the right way), Felipe Santos, 11 oct. 2022 \u21a9</p> </li> <li> <p>basic commands for working with wsl \u21a9</p> </li> </ol>"},{"location":"blog/2024/03/21/mastering-docker-a-comprehensive-guide-to-efficient-container-management/","title":"Mastering Docker: A Comprehensive Guide to Efficient Container Management","text":"<p>Tired of complicated installation processes, inconsistent environments, and tedious deployment procedures in your development workflow?</p> <p>Whether you're an experienced developer or just starting out, Docker offers a game-changing approach when it comes to application deployment, scalability, and consistency.</p> <p>Dive into the world of Docker and utilise its full potential to simplify your container management and increase your productivity!</p> <p>In this comprehensive guide, we will introduce you to Docker's essential commands, best practices, and advanced techniques to help you harness the power of containerisation effectively.</p> <p>Ready to embark on a journey towards transparent development, easy deployment, and unrivalled efficiency? Let's dive in and master Docker from A to Z!</p>"},{"location":"blog/2024/03/21/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#understanding-docker-a-primer-on-containerization","title":"Understanding Docker: A Primer on Containerization","text":"<p>Before diving into Docker's intricacies, let's establish a foundational understanding of containerization:</p>"},{"location":"blog/2024/03/21/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#what-is-docker","title":"What is Docker?","text":"<p>Docker is a platform designed to make it easier to create, deploy, and run applications by using containers. Containers allow developers to package an application with all of its dependencies into a standardized unit for software development.</p>"},{"location":"blog/2024/03/21/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#key-concepts","title":"Key Concepts","text":"<ul> <li>Images: Lightweight, standalone, and executable packages that contain everything needed to run a piece of software, including the code, runtime, libraries, and dependencies.</li> <li>Containers: Runnable instances of Docker images, encapsulating the software and its dependencies, ensuring consistency across different environments.</li> <li>Dockerfile: A text document that contains all the commands a user could call on the command line to assemble an image.</li> <li>Docker Hub: A cloud-based repository provided by Docker for finding, sharing, and distributing container images.</li> </ul> <p>With this foundation in place, let's delve deeper into Docker's functionalities and explore .</p>"},{"location":"blog/2024/03/21/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#essential-docker-commands-navigating-the-docker-universe","title":"Essential Docker Commands: Navigating the Docker Universe","text":"<p>Creating a container from an image or using a specific distribution like Ubuntu involves a few steps. Here are the basic commands to create a Docker container:</p>"},{"location":"blog/2024/03/21/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#create-a-container-from-an-image","title":"Create a Container from an Image","text":""},{"location":"blog/2024/03/21/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#pull-an-image-from-docker-hub","title":"Pull an Image from Docker Hub","text":"<p>If you haven't already downloaded the image you want to use, you can pull it from Docker Hub using <code>docker pull</code>:</p> <pre><code>docker pull &lt;image_name&gt;\n</code></pre> <p>Replace <code>&lt;image_name&gt;</code> with the name of the image you want to pull. For example, to pull the Ubuntu image:</p> <pre><code>docker pull ubuntu\n</code></pre>"},{"location":"blog/2024/03/21/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#run-a-container-from-an-image","title":"Run a Container from an Image","text":"<p>Once you have the image, you can create a container by running it with <code>docker run</code>:</p> <pre><code>docker run -it --name &lt;container_name&gt; &lt;image_name&gt; /bin/bash\n</code></pre> <p>Explanation of the flags used:</p> <ul> <li><code>-it</code>: Starts the container in interactive mode with a terminal.</li> <li><code>--name &lt;container_name&gt;</code>: Assigns a specific name to your container.</li> <li><code>&lt;image_name&gt;</code>: Specifies the image to use for creating the container.</li> <li><code>/bin/bash</code> (or another command): Starts a specific command or shell in the container. For Ubuntu, using <code>/bin/bash</code> opens a Bash shell.</li> </ul> <p>For instance, to create a container named <code>my-ubuntu-container</code> from the Ubuntu image:</p> <pre><code>docker run -it --name my-ubuntu-container ubuntu /bin/bash\n</code></pre> <p>This will start a new container based on the Ubuntu image and give you access to the Bash shell within that container.</p>"},{"location":"blog/2024/03/21/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#install-software-or-customize-the-container","title":"Install Software or Customize the Container","text":"<p>Once inside the container, you can install software, make changes, or configure it as needed using standard commands as you would on a regular Linux system. For example, within the container:</p> <pre><code>apt update\napt install &lt;package_name&gt;\n</code></pre> <p>Replace <code>&lt;package_name&gt;</code> with the name of the package you want to install.</p>"},{"location":"blog/2024/03/21/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#commit-changes-to-a-new-image-optional","title":"Commit Changes to a New Image (Optional)","text":"<p>If you make changes within the container and want to save them as a new image, you can commit the container's current state:</p> <pre><code>docker commit &lt;container_id&gt; &lt;new_image_name&gt;\n</code></pre> <p>Replace <code>&lt;container_id&gt;</code> with the ID of your container, and <code>&lt;new_image_name&gt;</code> with the name you want to give to the new image.</p> <p>For example:</p> <pre><code>docker commit my-ubuntu-container my-custom-ubuntu\n</code></pre> <p>This will create a new image named <code>my-custom-ubuntu</code> based on the changes made in the <code>my-ubuntu-container</code> container.</p> <p>Remember, changes made inside a container will be lost if you remove the container without committing those changes to a new image.</p> <p>These steps should help you create, customize, and manage containers based on Docker images, allowing you to work with specific distributions like Ubuntu or any other image available on Docker Hub.</p>"},{"location":"blog/2024/03/21/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#get-all-container-along-with-their-id-and-status","title":"Get all container along with their ID and status","text":"<p>To access an existing container from another terminal, you need its container ID or name. You can find this information looking for all containers.</p> <p>To get the container ID, you can use the following command in the terminal:</p> <pre><code>$ docker ps\n# to see all containers, running and stopped\n$ docker ps -a\n</code></pre> <p>This command lists all the running containers along with their IDs, names, and other details.</p> <p>Example output:</p> <pre><code>ID            IMAGE                                COMMAND    ...   PORTS\necce33b30ebf  &lt;your username&gt;/node-web-app:latest  npm start  ...   49160-&gt;8080\n</code></pre>"},{"location":"blog/2024/03/21/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#print-app-output","title":"Print app output","text":"<p>To print the application output, you can use the following command:</p> <pre><code>docker logs &lt;container id&gt;\n</code></pre> <p>Example output:</p> <pre><code>Running on http://localhost:8080\n</code></pre>"},{"location":"blog/2024/03/21/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#access-the-application","title":"Access the application","text":"<p>To access the application, you can use the <code>curl</code> command:</p> <pre><code>curl -i localhost:49160\n</code></pre> <p>Example output:</p> <pre><code>HTTP/1.1 200 OK\nX-Powered-By: Express\nContent-Type: text/html; charset=utf-8\nContent-Length: 12\nETag: W/\"c-M6tWOb/Y57lesdjQuHeB1P/qTV0\"\nDate: Mon, 13 Nov 2017 20:53:59 GMT\nConnection: keep-alive\n\nHello world\n</code></pre>"},{"location":"blog/2024/03/21/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#access-cmd-line-of-the-running-container","title":"Access cmd line of the running container","text":"<p>Once you have the container ID or name, you can use the <code>docker exec</code> command to access it from another terminal. The command syntax is:</p> <pre><code>docker exec -it &lt;container_id_or_name_&gt; /bin/bash\n</code></pre> <p>Replace <code>&lt;container_id_or_name&gt;</code> with the actual ID or name of your container.</p> <p>For example, if your container's name is <code>my-ubuntu-container</code>, you'd run:</p> <pre><code>docker exec -it my-ubuntu-container /bin/bash\n</code></pre> <p>This command will open a new terminal session inside the running container, allowing you to interact with it just like you did from the initial terminal.</p> <p>Remember, you can access a running container from multiple terminals simultaneously using <code>docker exec</code>. Each terminal session will have its own instance of a shell within the same container, enabling parallel interactions and commands execution.</p>"},{"location":"blog/2024/03/21/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#kill-the-running-container","title":"Kill the running container","text":"<p>To stop the running container, you can use the following command:</p> <pre><code>docker kill &lt;container id&gt;\n</code></pre> <p>Example output:</p> <pre><code>&lt;container id&gt;\n</code></pre>"},{"location":"blog/2024/03/21/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#restart-a-stoppedkilled-container","title":"Restart a stopped/killed container","text":"<p>To run the killed container, you can use the following command:</p> <pre><code># see all the containers\n$ docker ps -a \n# restart the one\n$ docker start &lt;container id&gt;\n</code></pre> <p>Example output:</p> <pre><code>&lt;container id&gt;\n</code></pre>"},{"location":"blog/2024/03/21/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#confirm-that-the-app-has-stopped","title":"Confirm that the app has stopped","text":"<p>To confirm that the application has stopped, you can use the <code>curl</code> command again:</p> <pre><code>curl -i localhost:49160\n</code></pre> <p>Example output:</p> <pre><code>curl: (7) Failed to connect to localhost port 49160: Connection refused\n</code></pre>"},{"location":"blog/2024/03/21/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#delete-a-container","title":"delete a container","text":"<p>To delete a Docker container by its ID, you can use the <code>docker rm</code> command followed by the container ID. Here's the syntax:</p> <pre><code>docker rm &lt;container_id&gt;\n</code></pre> <p>Replace <code>&lt;container_id&gt;</code> with the actual ID of the container you want to delete.</p> <p>For example, if your container ID is <code>a460131e5352</code>, you'd run:</p> <pre><code>docker rm a460131e5352\n</code></pre> <p>This command will remove the specified container. Make sure the container is stopped before attempting to remove it. If the container is running, you can stop it using <code>docker stop &lt;container_id&gt;</code> before removing it.</p> <p>Please note that deleting a container is a permanent action, and its associated data (unless stored in a separate volume) will be lost.</p>"},{"location":"blog/2024/03/21/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#share-files-between-your-pc-and-a-docker-container","title":"Share files between your PC and a Docker container","text":"<p>To share files between your PC and a Docker container using the container ID, you can use Docker's <code>docker cp</code> command. This command allows you to copy files or directories between the host system and a container. Here's how you can use it:</p>"},{"location":"blog/2024/03/21/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#copying-files-from-host-to-container","title":"Copying Files from Host to Container","text":"<pre><code>docker cp /path/on/host/file_or_directory &lt;container_id&gt;:/path/in/container\n</code></pre> <p>Replace <code>/path/on/host/file_or_directory</code> with the path to the file or directory on your host system that you want to copy into the container.</p> <p>Replace <code>&lt;container_id&gt;</code> with the ID of the container where you want to copy the files.</p> <p>Replace <code>/path/in/container</code> with the path inside the container where you want to place the files.</p> <p>For instance, if you have a file <code>/home/user/file.txt</code> on your host system and you want to copy it into a container with ID <code>a460131e5352</code> into the container's <code>/app/data</code> directory, you'd use:</p> <pre><code>docker cp /home/user/file.txt a460131e5352:/app/data\n</code></pre>"},{"location":"blog/2024/03/21/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#copying-files-from-container-to-host","title":"Copying Files from Container to Host","text":"<p>Similarly, you can copy files from a container back to your host system:</p> <pre><code>docker cp &lt;container_id&gt;:/path/in/container/file_or_directory /path/on/host\n</code></pre> <p>Replace <code>&lt;container_id&gt;</code> with the ID of the container from which you want to copy files.</p> <p>Replace <code>/path/in/container/file_or_directory</code> with the path inside the container of the file or directory you want to copy.</p> <p>Replace <code>/path/on/host</code> with the path on your host system where you want to place the copied files.</p> <p>For example, if you want to copy a file named <code>output.log</code> from a container with ID <code>a460131e5352</code> located at <code>/var/logs</code> to your host's <code>/home/user/logs</code> directory, you'd run:</p> <pre><code>docker cp a460131e5352:/var/logs/output.log /home/user/logs\n</code></pre> <p>This way, you can share files between your PC and a Docker container using the <code>docker cp</code> command.</p>"},{"location":"blog/2024/03/21/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#vscode-tools","title":"VSCode Tools","text":"<p>open a container folder in vscode</p> <p>consult this link</p>"},{"location":"blog/2024/03/21/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#create-an-image-from-an-application","title":"Create an Image from an Application","text":"<p>Managing Docker images and their containers involves a structured process. Here's an organized version detailing the steps:</p>"},{"location":"blog/2024/03/21/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#organize-files","title":"Organize Files:","text":"<p>Organizing files is pivotal for smooth Docker image creation. Ensure your application code, including the Dockerfile, <code>.dockerignore</code>, and necessary configuration files, resides within the same directory. For instance, a typical Node.js app might have the following structure:</p> <pre><code>your-app-directory/\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 .dockerignore\n\u251c\u2500\u2500 package.json\n\u251c\u2500\u2500 index.js (or main application file)\n\u251c\u2500\u2500 ... (other application files and directories)\n</code></pre> <p>Keeping all pertinent files together simplifies referencing in the Dockerfile and ensures that only essential application files are included in the image.</p>"},{"location":"blog/2024/03/21/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#example-dockerfile-for-an-express-app","title":"Example Dockerfile for an Express App:","text":"<p>Here's a simple Dockerfile suitable for an Express application running on Node.js 14:</p> <pre><code># Use Node.js 14 as the base image\nFROM node:14\n\n# Create and set the working directory in the container\nWORKDIR /usr/src/app\n\n# Copy package.json and package-lock.json to the working directory\nCOPY package*.json ./\n\n# Install application dependencies\nRUN npm install\n\n# Copy the entire application directory into the container\nCOPY . .\n\n# Expose port 3001 to the outside world\nEXPOSE 3001\n\n# Define the command to start the application\nCMD [\"npm\", \"start\"]\n</code></pre>"},{"location":"blog/2024/03/21/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#create-dockerignore","title":"Create .dockerignore:","text":"<p>A sample <code>.dockerignore</code> might contain exclusions like:</p> <pre><code>node_modules\nnpm-debug.log\nimages\ndoc-api\nlogs\n.github\n.git\n.env\n.prod.env\ntests\n</code></pre> <p>The <code>.dockerignore</code> file excludes unnecessary files and directories from being copied into the Docker image during the build process. Including <code>.git</code> is generally advised to prevent large, unneeded directories from being included.</p> <p>Additionally, consider excluding:</p> <ul> <li>Sensitive Data: Files containing sensitive information.</li> <li>Development Configs: Configuration specific to local development.</li> <li>Build Artifacts: Generated files not needed for the running application.</li> <li>IDE/Editor Files: Editor-specific files not crucial for the app's runtime.</li> <li>Documentation/Images: Non-essential assets irrelevant to the app's functionality.</li> </ul>"},{"location":"blog/2024/03/21/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#quick-trick-using-a-modified-gitignore","title":"Quick trick: Using a modified .gitignore","text":"<p>Consider using a modified <code>.gitignore</code> as <code>.dockerignore</code>. However, ensure it's not overly restrictive, as some files ignored in version control might be necessary for the app to run correctly within a Docker container.</p> <p>In my example, <code>.test.env</code> was in .gitignore but important. However, <code>tests</code> is not in .gitignore but useless in production</p>"},{"location":"blog/2024/03/21/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#build-the-image","title":"Build the Image:","text":"<p>Navigate to the directory containing your Dockerfile and execute:</p> <pre><code>docker build -t your-image-name .\n</code></pre> <p>This command constructs a Docker image from the Dockerfile and tags it with the specified name.</p>"},{"location":"blog/2024/03/21/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#run-a-container-from-the-new-image","title":"Run a Container from the New Image:","text":"<p>Start a container using the newly built image:</p> <pre><code>docker run -p 49160:3001 your-image-name\n</code></pre> <p>Replace <code>your-image-name</code> with the image's name. If your app runs on port 3001 in the container, access it at <code>localhost:49160</code> on your machine.</p>"},{"location":"blog/2024/03/21/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#managing-images-and-associated-containers","title":"Managing Images and Associated Containers","text":""},{"location":"blog/2024/03/21/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#list-all-images","title":"List All Images","text":"<p>To view all Docker images on your system, execute:</p> <pre><code>docker images\n</code></pre> <p>Use <code>-q</code> for a compact list displaying only image IDs:</p> <pre><code>docker images -q\n</code></pre>"},{"location":"blog/2024/03/21/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#list-containers-using-a-specific-image","title":"List Containers Using a Specific Image","text":"<p>To list containers using a particular image:</p> <pre><code>docker ps -a --filter ancestor=your-image-name\n</code></pre> <p>Use <code>--format</code> to customize output (e.g., <code>{{.ID}}</code> for IDs):</p> <pre><code>docker ps -a --filter ancestor=your-image-name --format \"{{.ID}}\"\n</code></pre>"},{"location":"blog/2024/03/21/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#stop-containers-using-an-image","title":"Stop Containers Using an Image","text":"<p>Stop all containers associated with an image:</p> <pre><code>docker stop $(docker ps -a -q --filter ancestor=your-image-name)\n</code></pre>"},{"location":"blog/2024/03/21/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#remove-an-image-and-associated-containers","title":"Remove an Image and Associated Containers","text":"<p>To rebuild an image, follow these steps:</p> <ol> <li>Stop and Remove Containers:</li> </ol> <pre><code>docker stop $(docker ps -a -q --filter ancestor=your-image-name)\ndocker rm $(docker ps -a -q --filter ancestor=your-image-name)\n</code></pre> <p>Replace <code>your-image-name</code> with the specific image name or ID.</p> <ol> <li>Remove the Image:</li> </ol> <pre><code>docker rmi your-image-name\n</code></pre> <p>Replace <code>your-image-name</code> with the name of your Docker image.</p> <ol> <li>Rebuild the Image:</li> </ol> <p>After removing the container and image, rebuild the image:</p> <pre><code>docker build -t your-image-name .\n</code></pre> <p>This rebuilds the Docker image using the Dockerfile in the directory and tags it with the specified name.</p> <ol> <li>Run a New Container:</li> </ol> <p>Start a new container from the rebuilt image:</p> <pre><code>docker run -p 49160:3001 your-image-name\n</code></pre> <p>Replace <code>your-image-name</code> with the name of your newly built Docker image.</p> <p>By following these steps, you'll manage existing containers, remove associated images, and rebuild a fresh image for running a new container with updated changes.</p>"},{"location":"blog/2024/03/21/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#clean-env","title":"clean env","text":""},{"location":"blog/2024/03/21/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#remove-all-images-that-have-none-as-img-name","title":"remove all images that have none as img name","text":"<pre><code>#!/bin/bash\n\n# Get a list of image IDs for images with the tag \"none\"\nimages=$(docker images | grep \"none\" | awk '{print $3}')\n\n# Loop through each image ID and execute the commands\nfor image_id in $images\ndo\n # Stop containers based on the image\n docker stop $(docker ps -a -q --filter ancestor=$image_id)\n\n # Remove containers based on the image\n docker rm $(docker ps -a -q --filter ancestor=$image_id)\n\n # Remove the image itself\n docker rmi $image_id\ndone\n</code></pre>"},{"location":"blog/2024/03/21/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#remove-all-images-that-dont-have-a-container","title":"remove all images that dont have a container","text":"<pre><code># Get a list of image IDs for all images\nimages=$(docker images | awk '{print $3}')\n\n# Loop through each image ID and execute the commands\nfor image_id in $images\ndo\n # Remove the image itself if possible, which mean there's no associated container\n docker rmi $image_id\ndone\n</code></pre>"},{"location":"blog/2024/03/21/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#a-great-removal","title":"a great removal","text":"<pre><code>docker system prune -a\n</code></pre> <p>This command will remove all unused containers, networks, images (both dangling and unreferenced), and optionally, volumes.</p>"},{"location":"blog/2024/03/21/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#syncing-changes-between-local-directory-and-docker-container","title":"Syncing Changes between Local Directory and Docker Container","text":"<p>When working with projects like FastAPI, Django, ReactJS, MkDocs, NestJs, Flask or similar applications running inside Docker containers, syncing changes between your local development environment and the container becomes crucial for an efficient workflow. This synchronization ensures that any modifications made locally reflect inside the running Docker container, allowing real-time updates without the need for manual file transfers or container restarts.</p> <p>For instance, when using MkDocs to build documentation or working with FastAPI and Django for web development, syncing changes enables immediate feedback on updates, edits, or additions made to the project files.</p> <p>You can use the <code>--mount</code> option with <code>type=bind</code> in Docker accomplishes this synchronization:</p> <pre><code>docker run -p 49160:8000 --mount type=bind,source=$DOCS_ABS_PATH,target=/app/docs project_docu\n</code></pre> <p>This command binds the local <code>docs</code> directory (specified by <code>$DOCS_ABS_PATH</code>) to the <code>/app/docs</code> directory within the running <code>project_docu</code> Docker container. Any changes made within the local <code>docs</code> directory will be instantly reflected in the container, and vice versa.</p> <p>This synchronization mechanism ensures that as you modify files within your local development environment\u2014be it Markdown files for MkDocs, source code for FastAPI, or Django\u2014the changes are immediately accessible and reflected within the Docker container. This allows for seamless development, testing, and previewing of changes without interruptions caused by manual file transfers or container restarts.</p> <p>Integrating this sync approach into your development workflow significantly streamlines the process, enhancing productivity and enabling swift iterations during project development and testing phases.</p>"},{"location":"blog/2024/03/21/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#optimizing-container-performance","title":"Optimizing Container Performance","text":"<p>To maximize the performance and efficiency of your containers, consider the following tips:</p> <ul> <li>Keep Images Lean: Minimize the size of your Docker images by removing unnecessary dependencies and files.</li> <li>Use Docker Compose: Streamline multi-container applications by defining them in a single file with Docker Compose.</li> <li>Monitor Resource Usage: Monitor CPU, memory, and disk usage of your containers using Docker stats to identify performance bottlenecks.</li> <li>Implement Container Orchestration: Explore container orchestration tools like Kubernetes or Docker Swarm for managing large-scale container deployments.</li> </ul> <p>By incorporating these advanced techniques into your Docker workflow, you can elevate your container management to new heights and unlock the full potential of Docker.</p>"},{"location":"blog/2024/03/21/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#conclusion-embracing-the-future-of-containerization","title":"Conclusion: Embracing the Future of Containerization","text":"<p>Congratulations! You've embarked on a transformative journey through the world of Docker, mastering essential commands, best practices, and advanced techniques along the way. Armed with this knowledge, you're well-equipped to revolutionize your development workflow, streamline deployment processes, and unleash unparalleled efficiency with Docker.</p> <p>As you continue to explore and experiment with Docker, remember that containerization is not just a technology\u2014it's a mindset. Embrace the principles of consistency, scalability, and efficiency, and let Docker empower you to build, deploy, and scale applications like never before.</p>"},{"location":"blog/2024/03/21/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#related-links","title":"Related links","text":"<ul> <li>Simple guide to using Docker on Windows 10 and access from WSL 2</li> </ul>"},{"location":"blog/2023/10/26/step-by-step-guide-to-identifying-and-terminating-processes-on-specific-ports/","title":"Step-by-Step Guide to Identifying and Terminating Processes on Specific Ports","text":""},{"location":"blog/2023/10/26/step-by-step-guide-to-identifying-and-terminating-processes-on-specific-ports/#introduction","title":"Introduction","text":"<p>This markdown provides a step-by-step guide to identify and terminate processes running on a specific port, catering to both Unix-based and Windows systems.</p>"},{"location":"blog/2023/10/26/step-by-step-guide-to-identifying-and-terminating-processes-on-specific-ports/#handling-processes-on-a-port","title":"Handling Processes on a Port","text":"<p>Suppose you encounter an <code>OSError: [Errno 98] Address already in use</code> error while trying to run an application that requires port 8000. This commonly happens when another process is already using the same port.</p>"},{"location":"blog/2023/10/26/step-by-step-guide-to-identifying-and-terminating-processes-on-specific-ports/#method-1-using-curl-to-test-the-port","title":"Method 1: Using <code>curl</code> to Test the Port","text":"<p>One way to check if a process is using port 8000 is by attempting to access it:</p> <pre><code>curl 127.0.0.1:8000\n</code></pre> <p>If you encounter an error or a response different from what you expect, it may indicate a running application using that port.</p>"},{"location":"blog/2023/10/26/step-by-step-guide-to-identifying-and-terminating-processes-on-specific-ports/#method-2-using-ps-and-grep-command","title":"Method 2: Using <code>ps</code> and <code>grep</code> Command","text":"<p>The <code>ps</code> command, in conjunction with <code>grep</code>, can display processes associated with a specific port. However, this method might not precisely show processes bound to port 8000; rather, it lists processes containing \"8000\" in their information.</p> <pre><code>ps aux | grep 8000\n</code></pre>"},{"location":"blog/2023/10/26/step-by-step-guide-to-identifying-and-terminating-processes-on-specific-ports/#method-3-using-lsof-to-identify-processes-by-port","title":"Method 3: Using <code>lsof</code> to Identify Processes by Port","text":"<p>The <code>lsof</code> command is specifically designed to list processes using a particular port. Execute the following command to identify processes running on port 8000:</p> <pre><code>lsof -i :8000\n</code></pre> <p>This command displays detailed information about processes using port 8000, including their Process ID (PID) and associated program.</p>"},{"location":"blog/2023/10/26/step-by-step-guide-to-identifying-and-terminating-processes-on-specific-ports/#method-4-windows-equivalent-netstat","title":"Method 4: Windows Equivalent (<code>netstat</code>)","text":"<p>For Windows users, the <code>netstat</code> command helps identify active connections and associated processes using port 8000:</p> <pre><code>netstat -ano | findstr :8000\n</code></pre>"},{"location":"blog/2023/10/26/step-by-step-guide-to-identifying-and-terminating-processes-on-specific-ports/#additional-methods","title":"Additional Methods","text":""},{"location":"blog/2023/10/26/step-by-step-guide-to-identifying-and-terminating-processes-on-specific-ports/#using-ps-l-for-detailed-process-information","title":"Using <code>ps -l</code> for Detailed Process Information","text":"<p>The <code>ps -l</code> command provides detailed information about processes, including the process state, start time, and more. Use it in combination with <code>grep</code> to filter processes for port 8000:</p> <pre><code>ps -l | grep 8000\n</code></pre>"},{"location":"blog/2023/10/26/step-by-step-guide-to-identifying-and-terminating-processes-on-specific-ports/#forcefully-terminating-a-process-with-kill-9","title":"Forcefully Terminating a Process with <code>kill -9</code>","text":"<p>In some cases, a process may not respond to a regular <code>kill</code> command. The <code>kill -9</code> command forcefully terminates a process. Use it with caution, as it does not give the process a chance to clean up resources:</p> <pre><code>kill -9 &lt;PID&gt;\n</code></pre>"},{"location":"blog/2023/10/26/step-by-step-guide-to-identifying-and-terminating-processes-on-specific-ports/#terminating-the-identified-process","title":"Terminating the Identified Process","text":"<p>Once you've identified the Process ID (PID) of the process using port 8000, you can terminate it using the <code>kill</code> or <code>kill -9</code> command.</p> <ol> <li> <p>Identify the PID: Use <code>lsof</code> or <code>netstat</code> to find the PID associated with port 8000.</p> <p>Example with <code>lsof</code>:</p> <pre><code>lsof -i :8000\n</code></pre> </li> <li> <p>Kill the Process: Once you have the PID, use the <code>kill</code> command followed by the PID to terminate the process.</p> <p>Example, if the PID is 1234:</p> <pre><code>kill 1234\n</code></pre> <p>If needed, and the process is unresponsive to a regular <code>kill</code>, you can use <code>kill -9</code>:</p> <pre><code>kill -9 1234\n</code></pre> </li> </ol> <p>Always exercise caution when terminating processes, especially with <code>kill -9</code>, as it may impact running applications or services. Ensure proper permissions and confirm that you're terminating the correct process to avoid unintended consequences.</p>"},{"location":"blog/2023/12/30/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/","title":"Mastering Essential Linux Commands: Your Path to File and Directory Mastery","text":""},{"location":"blog/2023/12/30/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#introduction","title":"Introduction","text":"<p>This documentation aims to offer a comprehensive understanding of essential commands and techniques for file and directory management in a Linux environment. Mastering these commands is crucial for efficient navigation, manipulation, and analysis of files and directories.</p> <p>We'll embark on a journey by delving into the foundational usage of key commands like <code>wc</code>, <code>du</code>, <code>grep</code>, <code>awk</code>, and <code>find</code>, uncovering their individual functionalities. Additionally, we'll explore how these commands can be combined using powerful methods such as pipes (<code>|</code>), <code>-exec {} \\;</code>, or <code>-exec {} +</code>, unlocking their synergistic potential.</p> <p>Moreover, to solidify your understanding, real-life examples showcasing practical applications will be demonstrated.</p> <p>The hands-on experience gained through testing and implementing these commands will be pivotal in comprehending their nuanced usage and unleashing their practical utility.</p> <p>Let the learning begin !</p>"},{"location":"blog/2023/12/30/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#1-basic-commands-overview","title":"1. Basic Commands Overview","text":""},{"location":"blog/2023/12/30/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#wc-word-count","title":"wc (Word Count)","text":"<p>The <code>wc</code> command is used to count lines, words, and characters in files.</p> <ul> <li>Counting lines in a file:</li> </ul> <pre><code>wc -l file.txt\n</code></pre> <p>This command displays the number of lines in <code>file.txt</code>.</p>"},{"location":"blog/2023/12/30/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#du-disk-usage","title":"du (Disk Usage)","text":"<p>The <code>du</code> command estimates file and directory space usage.</p> <ul> <li>Getting the size of a directory:</li> </ul> <pre><code>du -h /path/to/directory\n</code></pre> <p>This command provides the disk usage of the specified directory (<code>/path/to/directory</code>) in a human-readable format (<code>-h</code>).</p>"},{"location":"blog/2023/12/30/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#grep-global-regular-expression-print","title":"grep (Global Regular Expression Print)","text":"<p>The <code>grep</code> command searches for patterns in files.</p> <ul> <li>Searching for lines containing a pattern in a file:</li> </ul> <pre><code>grep \"pattern\" file.txt\n</code></pre> <p>This command displays lines in <code>file.txt</code> that contain the specified <code>pattern</code>.</p> <ul> <li>Searching for lines containing multiple patterns in a file:</li> </ul> <pre><code>grep -e \"pattern1\" -e \"pattern2\" file.txt\n</code></pre> <p>This command displays lines in <code>file.txt</code> that contain either \"pattern1\" or \"pattern2\".</p> Additional <code>grep</code> options: <ul> <li> <p><code>-H</code>: Print the filename for each match when searching in multiple files.   <pre><code>grep -H \"pattern\" file1.txt file2.txt\n</code></pre></p> </li> <li> <p><code>-l</code>: Display only the names of files that contain at least one match, instead of showing the matching lines.   <pre><code>grep -l \"pattern\" file1.txt file2.txt\n</code></pre></p> </li> <li> <p><code>-n</code>: Display the line numbers along with the matching lines.   <pre><code>grep -n \"pattern\" file.txt\n</code></pre></p> </li> <li> <p><code>-w</code>: Match the whole word, ensuring that only entire words are considered.   <pre><code>grep -w \"word\" file.txt\n</code></pre></p> </li> <li> <p><code>-i</code>: Perform case-insensitive matching, ignoring differences in case when searching for the pattern.   <pre><code>grep -i \"pattern\" file.txt\n</code></pre></p> </li> <li> <p><code>-B N</code>: Display N lines before the matching line.   <pre><code>grep -B 2 \"pattern\" file.txt\n</code></pre></p> </li> <li> <p><code>-A N</code>: Display N lines after the matching line.   <pre><code>grep -A 2 \"pattern\" file.txt\n</code></pre></p> </li> </ul> <p>These options enhance the functionality of <code>grep</code> by providing more context, line numbers, and filename information when searching for patterns in files.</p>"},{"location":"blog/2023/12/30/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#awk-aho-weinberger-and-kernighan","title":"awk (Aho, Weinberger, and Kernighan)","text":"<ul> <li>Basic Syntax:</li> </ul> <pre><code>awk 'pattern { action }' file.txt\n</code></pre> <ul> <li><code>pattern</code>: The condition that a line must meet to trigger the action.</li> <li> <p><code>action</code>: The set of commands to be executed when the pattern is matched.</p> <p>Example</p> <p><pre><code>awk '/pattern/ { print $1 }' file.txt\n</code></pre> This command prints the first field of each line in <code>file.txt</code> where the pattern is found.</p> Common Use Cases: <ol> <li> <p>Printing Specific Columns: <pre><code>awk '{ print $2, $4 }' file.txt\n</code></pre>   This prints the second and fourth columns of each line in <code>file.txt</code>.</p> </li> <li> <p>Pattern Matching: <pre><code>awk '/error/ { print $0 }' log.txt\n</code></pre>   Prints lines containing the word \"error\" from the <code>log.txt</code> file.</p> </li> <li> <p>Calculations: <pre><code>awk '{ total += $1 } END { print total }' numbers.txt\n</code></pre>   Calculates and prints the sum of the values in the first column of <code>numbers.txt</code>.</p> </li> <li> <p>Custom Field and Record Separators: <pre><code>awk -F',' '{ print $1 }' data.csv\n</code></pre>   Specifies ',' as the field separator in a CSV file.</p> </li> </ol> Advanced Features: <ul> <li> <p>Variables: <pre><code>awk '{ total += $1 } END { print \"Sum:\", total }' numbers.txt\n</code></pre>   Uses the variable <code>total</code> to accumulate values.</p> </li> <li> <p>Built-in Functions: <pre><code>awk '{ print length($0) }' text.txt\n</code></pre>   Prints the length of each line in <code>text.txt</code>.</p> </li> <li> <p>Conditional Statements: <pre><code>awk '{ if ($1 &gt; 10) print $0 }' values.txt\n</code></pre>   Prints lines where the value in the first column is greater than 10.</p> </li> </ul> </li> </ul> <p><code>awk</code> is versatile and can be highly customized for various text processing tasks. It's especially useful for working with structured data in files.</p>"},{"location":"blog/2023/12/30/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#head-and-tail","title":"<code>head</code> and <code>tail</code>","text":"<p>The <code>head</code> command displays the beginning of a file, while <code>tail</code> shows the end.</p> <ul> <li>Viewing the first few lines of a file with <code>head</code>:</li> </ul> <pre><code>head file.txt\n</code></pre> <p>This command displays the first few lines of <code>file.txt</code>.</p> <ul> <li>Displaying a specific number of lines at the beginning of a file with <code>head -n</code>:</li> </ul> <pre><code>head -n 10 file.txt\n</code></pre> <p>This command displays the first 10 lines of <code>file.txt</code>. You can replace <code>10</code> with any number to view a different quantity of lines.</p> <ul> <li>Viewing the last few lines of a file with <code>tail</code>:</li> </ul> <pre><code>tail file.txt\n</code></pre> <p>This command shows the last few lines of <code>file.txt</code>.</p> <ul> <li>Displaying a specific number of lines at the end of a file with <code>tail -n</code>:</li> </ul> <pre><code>tail -n 15 file.txt\n</code></pre> <p>This command shows the last 15 lines of <code>file.txt</code>. Similarly, you can adjust <code>15</code> to any desired number to see a different quantity of lines.</p> <p>Using <code>-n</code> with <code>head</code> or <code>tail</code> allows you to precisely control the number of lines displayed from the beginning or end of a file.</p>"},{"location":"blog/2023/12/30/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#less-and-more","title":"<code>less</code> and <code>more</code>","text":"<p>Both <code>less</code> and <code>more</code> are used to view text files in a paginated manner.</p> <ul> <li>Viewing a file with <code>less</code>:</li> </ul> <pre><code>less file.txt\n</code></pre> <p><code>less</code> allows you to navigate through the file interactively.</p> <ul> <li>Viewing a file with <code>more</code>:</li> </ul> <pre><code>more file.txt\n</code></pre> <p><code>more</code> displays the file content page by page, but it has more limited navigation options compared to <code>less</code>.</p> <p>These commands provide different ways to view file contents, either scrolling through the entire file or just a section at a time.</p>"},{"location":"blog/2023/12/30/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#2-find-command","title":"2. <code>find</code> command","text":""},{"location":"blog/2023/12/30/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#basic-search","title":"basic search","text":"<p>The <code>find</code> command searches for files and directories based on various criteria.</p> <ul> <li>Finding files by name:</li> </ul> <pre><code>find /path/to/search -name \"filename.txt\"\n</code></pre> <p>Searches for <code>filename.txt</code> in <code>/path/to/search</code> and its subdirectories.</p> <pre><code>!!! info \"for case insensitive search, use `-iname` instead of `-name`\n</code></pre> <ul> <li>Finding files matching pattern:</li> </ul> <pre><code>find /path/to/search -name \"req*.txt\"\n</code></pre> <p>Searches ... in <code>/path/to/search</code> and its subdirectories.</p> <ul> <li>Finding files by type:</li> </ul> <pre><code>find /path/to/search -type f\n</code></pre> <p>Finds all files in <code>/path/to/search</code> and its subdirectories.</p> <pre><code>!!! info \"`-type f` search files when `-type d` is for directories\"\n</code></pre> <ul> <li>Find and delete files:</li> </ul> <pre><code>find /path/to/search -name \"file_to_delete.txt\" -delete\n</code></pre> <p>This command finds a file named <code>file_to_delete.txt</code> and deletes it.</p>"},{"location":"blog/2023/12/30/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#find-enhanced-searching-with-more-options","title":"find: Enhanced Searching with more options","text":"<ul> <li>Find files by size:</li> </ul> <pre><code>find /path/to/search -size +10M\n</code></pre> <p>This command finds files larger than 10 megabytes in the specified directory.</p> <ul> <li>Find files modified within a time range:</li> </ul> <pre><code>find /path/to/search -mtime -7\n</code></pre> <p>This command finds files modified within the last 7 days in the specified directory.</p>"},{"location":"blog/2023/12/30/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#conclusion","title":"Conclusion","text":"<p>These commands offer different functionalities:</p> <ul> <li><code>wc</code> counts lines, words, or characters in a file.</li> <li><code>du</code> estimates disk usage for files and directories.</li> <li><code>grep</code> searches for patterns in files and prints lines containing the specified pattern.</li> <li><code>awk</code> is a powerful text processing tool for pattern scanning and processing.</li> <li>The <code>find</code> command in Linux is a powerful tool used for searching files and directories based on various criteria.</li> </ul> <p>You can use these commands to perform various operations related to file content, size estimation, and pattern matching within files.</p>"},{"location":"blog/2023/12/30/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#3-combining-find-with-other-commands","title":"3. Combining find with Other Commands","text":"<p>In this section, We explore how to combine the previous commands using pipes (<code>|</code>), <code>-exec {} \\;</code>, or <code>-exec {} +</code>:</p>"},{"location":"blog/2023/12/30/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#pipes","title":"Pipes (<code>|</code>)","text":"<p>Using pipes to pass the output of one command as input to another.</p> <ul> <li> <p>Finding specific files and counting them:</p> <pre><code>find /path/to/search -name \"*.txt\" | wc -l\n</code></pre> <p>Finds <code>.txt</code> files and counts them using <code>wc -l</code>.</p> </li> </ul>"},{"location":"blog/2023/12/30/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#-exec-find-and-perform-an-action-on-each-file","title":"<code>-exec {} \\;</code>: Find and perform an action on each file","text":"<p>Executing a command for each matched file or directory.</p> <ul> <li> <p>Finding files and displaying their sizes:</p> <pre><code>find /path/to/search -type f -exec du -h {} \\;\n</code></pre> <p>Displays sizes of files (each file in a different command) found by <code>find</code> using <code>du -h</code>.</p> </li> <li> <p>Finding files and performing deletion:</p> <pre><code>find /path/to/search -name \"file_to_delete.txt\" -exec rm {} \\;\n</code></pre> <p>Deletes files (each file in a different command) matching the name <code>file_to_delete.txt</code>.</p> </li> <li> <p>Finding and searching patterns:</p> <pre><code>find /path/to/search -name \"*.txt\" -exec grep \"pattern\" {} \\;\n</code></pre> <p>This command finds all <code>.txt</code> files in the specified directory and runs <code>grep</code> to search for a specific pattern within each of those files.</p> </li> </ul>"},{"location":"blog/2023/12/30/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#-exec-find-and-perform-an-action-on-all-files-at-once","title":"<code>-exec {} +</code>: Find and perform an action on all files at once","text":"<p>Optimizing efficiency by passing multiple arguments to a command.</p> <ul> <li> <p>Finding files and performing deletion:</p> <pre><code>find /path/to/search -name \"file_to_delete.txt\" -exec rm {} +\n</code></pre> <p>Deletes files (all in one command) matching the name <code>file_to_delete.txt</code></p> </li> </ul>"},{"location":"blog/2023/12/30/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#grouping-expressions","title":"<code>\\(</code> ... <code>\\)</code>: Grouping Expressions","text":"<p>When using <code>find</code> to search for files based on multiple criteria, such as file name patterns, types, or sizes, you may need to combine these criteria using logical operators like <code>-and</code>, <code>-or</code>, or <code>-not</code>. The <code>\\( ... \\)</code> construct allows you to group these expressions together to ensure they are evaluated as a single logical unit.</p> <p>Grouping multiple expressions together for logical operations.</p> <ul> <li> <p>Grouping Expressions in <code>find</code>:</p> <pre><code>find /path/to/search \\( -name \"*.txt\" -o -name \"*.pdf\" \\) -size +1M\n</code></pre> <p>Groups the conditions for finding files with either <code>.txt</code> or <code>.pdf</code> extensions and with a size greater than 1MB.</p> <p>Using <code>\\( ... \\)</code> allows for the proper grouping of expressions within a <code>find</code> command, ensuring that logical operations are applied correctly.</p> </li> </ul> <p>Overall, <code>\\( ... \\)</code> is a crucial construct in <code>find</code> commands for combining multiple search criteria and ensuring their proper evaluation. It helps create more complex search patterns while maintaining clarity and precision in the command syntax.</p>"},{"location":"blog/2023/12/30/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#fast-conclusion","title":"fast conclusion","text":"<p><code>find</code> is an incredibly versatile command that can be combined with various flags and options to perform advanced searches based on filenames, types, sizes, modification times, and more. It's a great tool for locating specific files or performing actions on groups of files based on specific criteria.</p>"},{"location":"blog/2023/12/30/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#4-application-showcases","title":"4. Application showcases","text":""},{"location":"blog/2023/12/30/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#counting-files-in-a-folder","title":"Counting Files in a Folder","text":"<p>To count the number of files in a folder, you can use the following commands:</p> <ol> <li> <p>Using <code>find</code>:</p> <pre><code>find /path/to/folder -maxdepth 1 -type f | wc -l\n</code></pre> <p>More</p> <ul> <li>This command uses <code>find</code> to search for files (<code>-type f</code>) in the specified folder without going into subdirectories (<code>-maxdepth 1</code>).</li> <li>The output is then piped to <code>wc -l</code>, which counts the number of lines, effectively giving you the count of files.</li> </ul> </li> <li> <p>Using <code>ls</code>:</p> <pre><code>ls -l /path/to/folder | grep \"^-\" | wc -l\n</code></pre> <p>More</p> <p>Here, - <code>ls -l</code> lists the contents of the folder with detailed information - <code>grep \"^-\"</code> filters out only the lines that represent files (as opposed to directories or other types of items) - <code>wc -l</code> counts the number of lines, providing the count of files in the folder.</p> </li> </ol>"},{"location":"blog/2023/12/30/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#counting-filesfolders-in-a-folder","title":"Counting Files/Folders in a Folder","text":"<pre><code>find /path/to/folder -maxdepth 1 | wc -l\n</code></pre> <p>or</p> <pre><code>ls -l /path/to/folder | wc -l\n</code></pre>"},{"location":"blog/2023/12/30/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#determining-the-number-of-columns-in-a-csv-file","title":"Determining the Number of Columns in a CSV File","text":"<pre><code>head -n 1 input/google-form-data.csv | grep -o \",\" | wc -l\n</code></pre> <p>This command reads the first line, apply the <code>,</code> separator then count</p>"},{"location":"blog/2023/12/30/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#finding-requirementstxt-files-containing-openpyxl","title":"Finding <code>requirements.txt</code> Files Containing \"openpyxl\"","text":"<ul> <li>find all requirements.txt files</li> </ul> <pre><code>find . -name requirements.txt\n</code></pre> <ul> <li>find all requirements.txt files who contain \"openpyxl\"</li> </ul> <pre><code>find . -name requirements.txt -exec grep -l \"openpyxl\" {} \\;\n</code></pre>"},{"location":"blog/2023/12/30/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#utilizing-maxdepth-for-search","title":"Utilizing <code>maxdepth</code> for Search","text":"<ul> <li>find all <code>.txt</code> files non recursively</li> </ul> <pre><code>find . -maxdepth 1 -type f -name \"*.txt\"\n</code></pre> <ul> <li>find ...</li> </ul> <pre><code>find . -maxdepth 3\n</code></pre> <ul> <li>save the result</li> </ul> <pre><code>find . -maxdepth 3 &gt; output.txt\n</code></pre>"},{"location":"blog/2023/12/30/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#skipping-certain-paths-in-a-search","title":"Skipping Certain Paths in a Search","text":"<ul> <li>find all py files but skip venv folders (paths containing venv)</li> </ul> <pre><code>find . -name \"*.py\" ! -path \"*venv*\"\n</code></pre> <ul> <li>find all py files but skip venv folders and apply yapf on each file</li> </ul> <pre><code>find . -name \"*.py\" ! -path \"*venv*\" -exec yapf --in-place {} \\;\n</code></pre> <ul> <li>find all py files but skip folders(likely env) and apply yapf on each file</li> </ul> <pre><code>find . -name \"*.py\" ! -path \"*env/Scripts*\" -exec yapf --in-place {} \\;\n</code></pre> <ul> <li>search files where the word wrappers is mentionned and avoid some folders</li> </ul> <pre><code>find . -type f -not -path '*/node_modules/*' -not -path '*env*' -not -name '*_*' -name '*.py' -exec grep -l 'wrappers' {} +\n</code></pre>"},{"location":"blog/2023/12/30/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#git-grep-for-version-controlled-files","title":"git grep for version controlled files","text":"<ul> <li>search files where the word wrappers is mentionned withing the version controlled files</li> </ul> <pre><code>git grep -l \"wrapper\" -- \"*.py\"\n</code></pre>"},{"location":"blog/2023/12/30/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#using-head-and-tail-commands","title":"Using <code>head</code> and <code>tail</code> Commands","text":"<ul> <li>display the last 50 lines of a file</li> </ul> <pre><code>tail -50 cli.log\n</code></pre> <ul> <li>filter the output of another command</li> </ul> <pre><code>tail -50 cli.log | grep \"/api/\"\n</code></pre> <p>This command will display the last 50 lines of the <code>cli.log</code> file and filter out only the lines that contain \"/api/\". This combination of <code>tail</code> and <code>grep</code> will help you isolate and display the relevant lines.</p> <ul> <li>install the first lines of <code>requirements.txt</code> using <code>head</code> and <code>xargs</code></li> </ul> <pre><code>head -n 18 requirements.txt | xargs -n 1 pip3 install\n</code></pre> <p>This command will read the first 18 lines of <code>requirements.txt</code>, then install each package listed there using <code>pip3</code>.</p> <p>An improvement of this command has been proposed here using <code>sed</code> to remove from the requirement file, spaces, comment, empty lines, ...</p>"},{"location":"blog/2023/12/30/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#search-for-lines-containing-the-word-black-within-sh-files","title":"search for lines containing the word \"black\" within <code>.sh</code> files","text":"<pre><code>find /path/to/search -type f -name \"*.sh\" -exec grep -l \"black\" {} +\n</code></pre> More on find and grep options <p>This command will search for lines containing the word \"black\" within <code>.sh</code> files. The command (<code>grep</code>) displays the actual lines containing \"black\" within the files</p> <ul> <li> <p>grep options: <code>grep</code> vs <code>grep -l</code>     To only show the filenames without the matches, use the command (<code>grep</code>) instead of (<code>grep -l</code>)</p> </li> <li> <p><code>-exec</code> option in the <code>find</code> command     This syntax uses <code>+</code> at the end of the <code>-exec</code> option. It gathers the file names that match the criteria (<code>*.sh</code>) and passes them to <code>grep</code> in batches, rather than invoking <code>grep</code> once per file. This is generally more efficient, especially when dealing with a large number of files.</p> <p>To invoke <code>grep</code> individually for each file that matches the criteria (<code>*.sh</code>), use instead  <code>find /path/to/search -type f -name \"*.sh\" -exec grep -l \"black\" {} \\;</code>: This syntax uses <code>\\;</code> at the end of the <code>-exec</code> option. </p> <p>This method might be less efficient, especially for a large number of files, as it starts a new <code>grep</code> process for each file separately.</p> </li> </ul>"},{"location":"blog/2023/12/30/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#search-for-folders","title":"search for folders","text":"<ul> <li>search folder by name (ex:all name containing eigen3)</li> </ul> <pre><code>find -type d -name \"*eigen3*\"\n</code></pre> <ul> <li>search for a specific folder like \"LAStools/bin\" starting from the root directory <code>/home</code>.</li> </ul> <pre><code>sudo find /home -type d -name \"bin\" -path \"*LAStools*\"\n</code></pre> <p>This command searches the entire root directory <code>/</code> for directories (<code>-type d</code>) named \"bin\" (<code>-name \"bin\"</code>) that are part of a path containing \"LAStools\" (<code>-path \"*LAStools*\"</code>)</p> <p>Using <code>sudo</code> might be necessary to have permission to search directories that your user account doesn't have access to by default.</p>"},{"location":"blog/2023/12/30/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#searching-for-aaa-in-files","title":"Searching for \"AAA\" in Files","text":"<ul> <li>search for \"AAA\" in all files</li> </ul> <pre><code>find . -type f -exec grep \"AAA\" {} \\;\n</code></pre> <ul> <li>search for \"AAA\" in all files with file name and line number display</li> </ul> <pre><code>find . -type f -exec grep -Hn \"AAA\" {} \\;\n</code></pre> <p>Certainly! Let's integrate the additional commands into the existing explanation:</p> <pre><code>### Searching for a keyword in Files from the parent directory\n\n- search for \"L337\" in all files\n```bash\nfind .. -type f -exec grep -Hn \"L337\" {} \\;\n</code></pre> <ul> <li>search for \"L337\" in all files with 5 lines after each match</li> </ul> <pre><code>find .. -type f -exec grep -Hn \"L337\" {} \\; -A 5\n</code></pre> <p>This command will find all occurrences of \"L337\" in files within the parent directory and its subdirectories and display the filename, line number, and the line containing \"L337\", along with the five lines that follow it.</p>"},{"location":"blog/2023/12/30/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#get-unique-lines-across-files","title":"Get unique lines across files","text":"<ul> <li>see the unique lines common to all three files without repetitions</li> </ul> <pre><code>sort file1.txt file2.txt file3.txt | uniq -c | awk '$1 == 3 {print $2}'\n</code></pre> <p>Alternative</p> <pre><code>sort file1.txt file2.txt file3.txt | uniq -c | awk '$1 &gt;= 3 {print $2}'\n</code></pre> <ul> <li>use the precedent list to filter lines from another file (<code>people</code>)</li> </ul> <pre><code>grep -f &lt;(sort file1.txt file2.txt file3.txt | uniq -c | awk '$1 &gt;= 3 {print $2}') ../people\n</code></pre> <p>This command will first find the unique lines common to all three files, then filter those lines using <code>grep -f</code> based on the patterns present in the specified file (in this case, the file containing sorted and unique lines from <code>file1.txt</code>, <code>file2.txt</code>, and <code>file3.txt</code>). Finally, it will display the lines that match in both sets and also contain the term <code>people</code>.</p>"},{"location":"blog/2023/12/30/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#additional-commands","title":"Additional Commands","text":"<ul> <li>extract names of females with the name 'Annabel' from the 'people' file</li> </ul> <pre><code>cat people | awk '$3 == \"F\"' | grep 'Annabel' | awk '{print $1, $2}'\n</code></pre> <p>In the project, this command filter the lines of the file <code>people</code> containing the word <code>Annabel</code> and where the person is female (3<sup>rd</sup> fiel == <code>F</code>) then use <code>awk</code> to print from the filtered file, only the first and second fields. The fields in each line are separated by a space</p> <ul> <li>search for 'Annabel' in all files and extract the names of females with the name 'Annabel'</li> </ul> <pre><code>find . -type f -exec grep -w 'Annabel' {} \\; -exec awk '$3 == \"F\" {print $1, $2}' {} \\;\n</code></pre> <p>This command will apply the precedent operation on each file returned by the <code>find</code> command</p>"},{"location":"blog/2023/12/30/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#grouping-examples","title":"Grouping Examples","text":"Finding files with either \".txt\" or \".pdf\" extensions and with a size greater than 1MB <p>Suppose you want to find files with either \".txt\" or \".pdf\" extensions and with a size greater than 1MB. You can use <code>\\( ... \\)</code> to group the size condition with the extension conditions:</p> <pre><code>find /path/to/search \\( -name \"*.txt\" -o -name \"*.pdf\" \\) -size +1M\n</code></pre> <p>In this command:</p> <ul> <li><code>\\( -name \"*.txt\" -o -name \"*.pdf\" \\)</code> groups the conditions for finding files with \".txt\" or \".pdf\" extensions.</li> <li><code>-size +1M</code> specifies the condition for files with a size greater than 1MB.</li> </ul> <p>By grouping the extension conditions together, you ensure that the size condition is applied to both \".txt\" and \".pdf\" files.</p> Finding specific files with specific extensions <p>Suppose you want to find specific files with extensions such as \".sh\", \".md\", or \"Dockerfile\" and then search for a particular pattern within them. You can use the following command:</p> <pre><code>find /home/ubuntu/Documents/GitHub/ \\( -name \"*.sh\" -o -name \"*.md\" -o -name \"Dockerfile\" \\) -exec grep -Hn \"apt install ./mongodb-database-tools-*.deb &amp;\" {} \\;\n</code></pre> <p>In this command:</p> <ul> <li><code>\\( -name \"*.sh\" -o -name \"*.md\" -o -name \"Dockerfile\" \\)</code> groups the conditions for finding files with the specified extensions.</li> <li><code>-exec grep -Hn \"apt install ./mongodb-database-tools-*.deb &amp;\" {} \\;</code> executes the <code>grep</code> command to search for the specified pattern within each matched file.</li> </ul> <p>The <code>\\( ... \\)</code> construct is used to group the <code>-name</code> expressions together. This grouping is necessary because the <code>-o</code> operator (logical OR) has lower precedence than the implicit logical AND applied to separate <code>find</code> expressions. By using <code>\\( ... \\)</code>, you ensure that the logical OR operation is applied correctly within the grouped expressions.</p> <p>Without the grouping, the command would not function as intended because each <code>-name</code> expression would be evaluated separately, potentially leading to unexpected results.</p> running a linter script md files from a repo subfolder and the readme file in the main directory <p>To find both <code>.md</code> files in the <code>./docs</code> directory and <code>README.md</code> files in the current directory, you can use the <code>-o</code> (OR) operator along with the <code>-exec</code> option. Here's how you can do it:</p> <pre><code>find . \\( -path \"./docs\" -name \"*.md\" -o -path \"./README.md\" \\) -exec markdownlint-cli2 --fix {} +\n</code></pre> <p>In this command:</p> <ul> <li><code>.</code>: Specifies the current directory as the starting point for the <code>find</code> command.</li> <li><code>\\( ... \\)</code>: Groups conditions together.</li> <li><code>-path \"./docs\" -name \"*.md\"</code>: Specifies files with the <code>.md</code> extension in the <code>./docs</code> directory.</li> <li><code>-o</code>: Acts as the logical OR operator.</li> <li><code>-path \"./README.md\"</code>: Specifies the <code>README.md</code> file in the current directory.</li> <li><code>-exec markdownlint-cli2 --fix {} +</code>: Executes the <code>markdownlint-cli2 --fix</code> command on the found files. The <code>{}</code> is replaced by the found file names.</li> </ul> <p>This command will execute <code>markdownlint-cli2 --fix</code> on all <code>.md</code> files in the <code>./docs</code> directory and <code>README.md</code> in the current directory.</p>"},{"location":"blog/2023/12/30/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#conclusion_1","title":"Conclusion","text":"<p>These commands, when mastered and strategically combined, offer a robust toolkit for proficiently managing and manipulating files and directories in a Linux environment. By leveraging these commands in tandem, users can perform intricate searches, conduct comprehensive analyses, and execute operations swiftly, significantly enhancing productivity and workflow efficiency.</p>"},{"location":"blog/2023/11/10/run-an-application-forever-on-linux-made-easy-case-of-a-java-script/","title":"Run an application forever on linux made easy: Case of a java script","text":""},{"location":"blog/2023/11/10/run-an-application-forever-on-linux-made-easy-case-of-a-java-script/#introduction","title":"Introduction","text":"<p>If you're looking to turn your application into a background process, you have come to the right tutorial, always using the fastest way.</p> <p>Instead of just writing theory, we we use a real world example i've worked on.</p> <p>To run a Java application as a background process and keep it running forever, you can use a process manager like <code>systemd</code> on Linux. Here's how you can set up a <code>systemd</code> service to run your Java application:</p> <p>Certainly! Here are the steps named as per their actions:</p>"},{"location":"blog/2023/11/10/run-an-application-forever-on-linux-made-easy-case-of-a-java-script/#step-1-create-service-file","title":"Step 1: Create Service File","text":"<p>Create a new systemd service file for your Java application using a text editor:</p> <pre><code>sudo nano /etc/systemd/system/myapp.service\n</code></pre>"},{"location":"blog/2023/11/10/run-an-application-forever-on-linux-made-easy-case-of-a-java-script/#step-2-configure-service","title":"Step 2: Configure Service","text":"<p>Paste the following configuration into the file, replacing <code>&lt;jar-file-name&gt;</code> with the name of your JAR file:</p> <pre><code>[Unit]\nDescription=My App\n\n[Service]\nUser=myuser\nExecStart=/usr/bin/java -jar /path/to/myapp/&lt;jar-file-name&gt;.jar\nSuccessExitStatus=143\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>"},{"location":"blog/2023/11/10/run-an-application-forever-on-linux-made-easy-case-of-a-java-script/#step-3-save-and-exit","title":"Step 3: Save and Exit","text":"<p>Save the file and exit the text editor.</p>"},{"location":"blog/2023/11/10/run-an-application-forever-on-linux-made-easy-case-of-a-java-script/#step-4-reload-daemon","title":"Step 4: Reload Daemon","text":"<p>Reload the systemd daemon to pick up the new service file:</p> <pre><code>sudo systemctl daemon-reload\n</code></pre>"},{"location":"blog/2023/11/10/run-an-application-forever-on-linux-made-easy-case-of-a-java-script/#step-5-start-service","title":"Step 5: Start Service","text":"<p>Start the new <code>myapp</code> service:</p> <pre><code>sudo systemctl start myapp\n</code></pre>"},{"location":"blog/2023/11/10/run-an-application-forever-on-linux-made-easy-case-of-a-java-script/#step-6-check-service-status","title":"Step 6: Check Service Status","text":"<p>Check the status of the service to make sure it's running:</p> <pre><code>sudo systemctl status myapp\n</code></pre> <p>If everything is set up correctly, you should see output indicating that the service is running. To stop the service, you can use the <code>sudo systemctl stop myapp</code> command.</p> <p>With this setup, your Java application will run as a background process and automatically start when the server boots up. If the application crashes or stops for any reason, <code>systemd</code> will automatically restart it.</p>"},{"location":"blog/2023/10/09/grip-a-github-like-markdown-viewer-in-your-computer/","title":"grip: A github-like markdown viewer in your computer","text":""},{"location":"blog/2023/10/09/grip-a-github-like-markdown-viewer-in-your-computer/#introduction","title":"Introduction","text":"<p>grip is a cli tool that you run in a directory from the terminal. He can parse you mardown files like github do.</p> But How that works and how to use it ?  <p>He access (endpoint based) all files from the repo and parse markdown files after sending them to github unless you use the offline renderer.</p>"},{"location":"blog/2023/10/09/grip-a-github-like-markdown-viewer-in-your-computer/#alternatives","title":"Alternatives","text":"<ul> <li>VScode Extension markdown preview enhanced: not working on ubuntu (see ref to solution), i use grip</li> </ul>"},{"location":"blog/2023/10/09/grip-a-github-like-markdown-viewer-in-your-computer/#prereqsites","title":"Prereqsites","text":"<ul> <li>python: you can install the latest version</li> </ul>"},{"location":"blog/2023/10/09/grip-a-github-like-markdown-viewer-in-your-computer/#installation","title":"Installation","text":"<p>Install it with <code>pip install grip</code></p> As simple as that !"},{"location":"blog/2023/10/09/grip-a-github-like-markdown-viewer-in-your-computer/#usage","title":"Usage","text":"<pre><code>cd path/to/project\n</code></pre> <ul> <li>run readme.md as web app</li> </ul> <pre><code>grip \n</code></pre> <ul> <li>or run another file</li> </ul> <pre><code>grip my-file-name.md\n</code></pre>"},{"location":"blog/2023/10/09/grip-a-github-like-markdown-viewer-in-your-computer/#exports","title":"Exports","text":""},{"location":"blog/2023/10/09/grip-a-github-like-markdown-viewer-in-your-computer/#export-to-pdf-or-html","title":"export to pdf or html","text":"<p>You can export you file to pdf or html. To use this feature, add <code>--export</code> option followed by the filename (with html or pdf extension)</p> <p>troubleshots</p> <ul> <li>he can export with <code>grip my-file-name.md --export inut.pdf</code> but there is a bug</li> <li>so i installed with <code>sudo apt install grip</code> but it takes forever</li> <li>anyway, the web view is cool. And if i neeeed pdf, from this discussion, i can use:</li> <li>windows: the extension <code>markdown preview enhanced</code> works there fine</li> <li>mardown-pdf from npm</li> <li>print the page from the webview and select non-empty pages. The pb is the ref links: the ref inside text to biblio will try to lead the grip webview that will not be open if not run on cli</li> </ul>"},{"location":"blog/2023/10/09/grip-a-github-like-markdown-viewer-in-your-computer/#rate-limiting","title":"rate limiting","text":"<p><code>grip</code> have a rate limit of 60 requests/hour. So each time you save your work and the grip server is running, you lose one request. And it you use VSCode auto-save, you're kind of screwed. Prefer either</p> <ul> <li>not to save, unless you finish modifications</li> <li>not to run grip while editing</li> <li>or add a <code>--norefresh</code> option</li> </ul> <p>You can have 5000 requests/hour if you add option --user with your credentials like this</p> <pre><code>grip my_file.md --user hermann-web:&lt;my-token&gt;\n</code></pre>"},{"location":"blog/2023/10/09/grip-a-github-like-markdown-viewer-in-your-computer/#offline-renderer","title":"Offline renderer","text":"<p>Note</p> <p>There is an offline renderer but it didn't make 2.0 release.</p> <p>When it will be available, it quite is useful if</p> <ul> <li>you don't have internet connection</li> <li>or have have issue about sending your sensitive files to github-microsoft You can use it like this</li> </ul> <pre><code>grip my_file.md --ofline-renderer\n</code></pre>"},{"location":"blog/2023/10/09/grip-a-github-like-markdown-viewer-in-your-computer/#cool-features","title":"Cool features","text":"<ul> <li>add <code>-b</code> if you want it to open a browser tab for you</li> <li><code>--quiet</code> to avoid printing in the terminal Use <code>--help</code> to see more of this</li> </ul>"},{"location":"blog/2023/10/10/pandoc-convert-most-files-without-online-services/","title":"pandoc: convert most files without online services","text":""},{"location":"blog/2023/10/10/pandoc-convert-most-files-without-online-services/#introduction","title":"Introduction","text":"<p>Pandoc is a versatile document conversion tool that can convert Markdown documents to PDF, HTML, Word DOCX, and many other formats. Pandoc provides a wide range of options to customize the output of the converted document. Here is a list of some of the most commonly used options:</p> <ul> <li><code>-s</code>: Create a standalone document with a header and footer.</li> <li><code>-o</code>: Specify the output file name.</li> <li><code>--from</code>: Specify the input format explicitly.</li> <li><code>--to</code>: Specify the output format explicitly.</li> </ul> <ul> <li><code>-V/--variable</code>: Set a template variable when rendering the document in standalone mode.</li> <li><code>--defaults</code>: Specify a package of options in the form of a YAML file.</li> <li><code>--list-input-formats</code>: Print a list of supported input formats.</li> <li><code>--list-output-formats</code>: Print a list of supported output formats.</li> <li><code>--list-highlight-styles</code>: Print a list of supported syntax highlighting styles.</li> <li><code>-f</code>: Specify the input format.</li> <li><code>-t</code>: Specify the output format.</li> </ul> <p>For a full list of options, see the Pandoc User's Guide<sup>1</sup> or the Pandoc manual <sup>2</sup>.</p>"},{"location":"blog/2023/10/10/pandoc-convert-most-files-without-online-services/#installing-pandoc","title":"Installing pandoc","text":"<ul> <li>Installing Pandoc on Ubuntu remove any previous versions of Pandoc.</li> </ul> <pre><code>sudo apt-get purge --auto-remove pandoc\n</code></pre> <ul> <li>Download the latest version of Pandoc from the Pandoc GitHub releases page for example,</li> </ul> <pre><code>wget https://github.com/jgm/pandoc/releases/download/3.1.8/pandoc-3.1.8-1-arm64.deb\n</code></pre> <ul> <li>Install the downloaded package by running the command sudo dpkg -i pandoc--1-amd64.deb, replacing  with the version number of the package you downloaded. for example, <pre><code>dpkg -i pandoc-3.1.8-1-arm64.deb\n</code></pre> <ul> <li>Run the following command</li> </ul> <pre><code>pandoc -f markdown -t latex input.md -o output.tex\n</code></pre> <p>where <code>input.md</code> is the name of your markdown file and <code>output.tex</code> is the name you want to give to the resulting LaTeX file.</p> <p>[For a hard markdown user like me, pandoc is a big time relief as i can note in markdown, store mostly markdown files and still, receiving and sharing files in proprietary like pdf, docx, ...]</p>"},{"location":"blog/2023/10/10/pandoc-convert-most-files-without-online-services/#get-your-options","title":"get your options","text":"<p>You can convert from any to any. To see available input (-f) format, use <code>pandoc --list-input-formats</code> To see available output (-t) format, use <code>pandoc --list-output-formats</code></p> <pre><code>pandoc -f markdown -t latex input.md -o output.tex\n</code></pre> <p>where <code>input.md</code> is the name of your markdown file and <code>output.tex</code> is the name you want to give to the resulting LaTeX file.</p>"},{"location":"blog/2023/10/10/pandoc-convert-most-files-without-online-services/#quick-examples","title":"Quick examples","text":"<ul> <li>md to docx</li> </ul> <pre><code>pandoc my_file.md -s -t docx -o my_file.docx\n</code></pre> <p><code>-s</code> is for standalone, <code>-o</code> to specify output file path, <code>-t</code> to specify output format (but no need as he guess from output file format given)</p> <ul> <li>md to tex</li> </ul> <pre><code>pandoc my_file.md -s -t latex -o my_file.tex\n</code></pre> <ul> <li>one md to pdf</li> </ul> <pre><code>pandoc my_file.md -o my_file.pdf\n</code></pre> <ul> <li>if there is an encoding problem</li> </ul> <pre><code>pandoc my_file.md -o my_file.pdf --pdf-engine=lualatex\n</code></pre> <p>found in this stackexchange discussion</p> <ul> <li>many md to pdf</li> </ul> <pre><code>pandoc *.md -o markdown_book.pdf \n</code></pre> <p>found in this stackoverflow discussion</p> <ul> <li>many md to pdf accross folders</li> </ul> <pre><code>pandoc *.md */*.md -o markdown_book.pdf --pdf-engine=lualatex\n</code></pre> <p>note that images not accessible from the current directory will not be parsed</p> <ul> <li>add automatic section numbering like in latex</li> </ul> <pre><code>pandoc my_file.md -s -o my_file.pdf --number-sections\n</code></pre>"},{"location":"blog/2023/10/10/pandoc-convert-most-files-without-online-services/#cool-features-in-md-to-pdf","title":"Cool features in md to pdf","text":"<p>from is a great blog in this <sup>3</sup> There are several cool options available when converting Markdown to PDF using Pandoc. Here are some of them:</p> <ol> <li><code>--toc</code>: Adds a table of contents to the beginning of the PDF that links to the various sections of the document.</li> <li><code>--template</code>: Allows you to use a custom LaTeX template to modify the appearance of the PDF.</li> <li><code>--variable</code>: Allows you to set variables that can be used in the LaTeX template. For example, you can set the font size or color of the text.</li> <li><code>--highlight-style</code>: Allows you to set the syntax highlighting style for code blocks.</li> <li><code>--number-sections</code>: Numbers the sections of the document.</li> <li><code>--metadata</code>: Allows you to set metadata for the PDF, such as the title, author, and date.</li> <li><code>-f markdown-implicit_figures</code>: ...</li> </ol> <p>For example, here is a command that uses some of these options:</p> <pre><code>pandoc input.md -o output.pdf --toc --template=mytemplate.tex --variable=fontsize:12pt --highlight-style=pygments --number-sections --metadata=title:\"Document\" --metadata=author:\"Hermann Agossou\"\n</code></pre> <p>This command adds a table of contents, uses a custom LaTeX template called <code>mytemplate.tex</code>, sets the font size to 12pt, uses the Pygments syntax highlighting style, numbers the sections, and sets the title and author metadata for the PDF.</p>"},{"location":"blog/2023/10/10/pandoc-convert-most-files-without-online-services/#adding-footnote-citations-in-markdown-files-for-pandoc-pdf-conversion","title":"Adding Footnote Citations in Markdown Files for Pandoc PDF Conversion","text":"<p>This section is about adding footnote citations using Pandoc and referencing a BibTeX file</p>"},{"location":"blog/2023/10/10/pandoc-convert-most-files-without-online-services/#1-creating-a-bibtex-file","title":"1. Creating a BibTeX File","text":"<p>In a separate BibTeX file (e.g., <code>references.bib</code>), store your references in the BibTeX format. Here's an example:</p> <pre><code>@online{las-1,\n  author       = \"{ArcGIS}\",\n  title        = \"{Storing lidar data}\",\n  howpublished = \"\\url{https://desktop.arcgis.com/fr/arcmap/latest/manage-data/las-dataset/storing-lidar-data.htm}\",\n}\n\n@online{las-2,\n  author       = \"{American Society for Photogrammetry and Remote Sensing}\",\n  title        = \"{LAS specification, version 1.4 \u2013 R13}\",\n  date         = \"2013-07-15\",\n  url          = \"https://www.asprs.org/wp-content/uploads/2019/07/LAS_1_4_r15.pdf\",\n}\n</code></pre>"},{"location":"blog/2023/10/10/pandoc-convert-most-files-without-online-services/#1-formatting-citations-in-markdown","title":"1. Formatting Citations in Markdown","text":"<p>To include citations in your Markdown file for conversion to PDF using Pandoc, use the <code>[@citation]</code> format within the text where you want the citation to appear. Here is an example:</p> <pre><code>Here is a statement requiring citation [@las-2].\n</code></pre> <p>Or you can also list the references. they will be parsed as regular mardown content</p> <pre><code>Here is a statement requiring citation [@las-2].\n\n...\n\n# References\n\n[@las-1]: ArcGIS. \"Storing lidar data.\" [Link](https://desktop.arcgis.com/fr/arcmap/latest/manage-data/las-dataset/storing-lidar-data.htm)\n\n[@las-2]: American Society for Photogrammetry and Remote Sensing. \"LAS specification, version 1.4 \u2013 R13.\" [Link](https://www.asprs.org/wp-content/uploads/2019/07/LAS_1_4_r15.pdf)\n</code></pre>"},{"location":"blog/2023/10/10/pandoc-convert-most-files-without-online-services/#3-using-pandoc-for-conversion-to-pdf","title":"3. Using Pandoc for Conversion to PDF","text":"<p>When converting the Markdown file to PDF using Pandoc, include the following options:</p> <ul> <li><code>--citeproc</code>: Enables citation processing.</li> <li><code>--bibliography</code>: Specifies the path to your bibliography file.</li> </ul> <p>Use the following command:</p> <pre><code>pandoc myfile.md -s --citeproc --bibliography=references.bib -o output.pdf\n</code></pre> <p>Replace <code>myfile.md</code> with the name of your Markdown file and <code>references.bib</code> with the actual name of your bibliography file.</p> <p>Pandoc will process the citations marked in <code>[@citation]</code> format within your Markdown file and generate the corresponding footnotes or bibliography entries in the resulting PDF.</p> <p>Remember to adjust the citation style and bibliography file as per your requirements. https://pandoc.org/chunkedhtml-demo/8.20-citation-syntax.html</p>"},{"location":"blog/2023/10/10/pandoc-convert-most-files-without-online-services/#footnote-citations","title":"footnote citations","text":"<p>You still can use <code>[^]</code> based citations. There will appear at the end of each page, not at the end of the file.</p> <p>Examples</p> <pre><code>[^citation-1]: Full citation details here.\n[^citation-2]: https://pandoc.org/chunkedhtml-demo/8.20-citation-syntax.html\n[^citation-3]: [Full citation details here.](https://pandoc.org/chunkedhtml-demo/8.20-citation-syntax.html)\n</code></pre> <p>You can see more on citation styles with <sup>4</sup></p>"},{"location":"blog/2023/10/10/pandoc-convert-most-files-without-online-services/#resize-image-in-markdown","title":"Resize image in markdown","text":"<p>For example, if you want the image to take 50% of the page wifth, use</p> <pre><code>![Caption text](/path/to/image){ width=50% }\n</code></pre>"},{"location":"blog/2023/10/10/pandoc-convert-most-files-without-online-services/#related-links","title":"Related links","text":"<ul> <li>Introducing Two New Packages for Streamlining File Conversions in Python</li> </ul> <ol> <li> <p>Pandoc Manual - pandoc.org \u21a9</p> </li> <li> <p>Pandoc General Writer Options - pandoc.org \u21a9</p> </li> <li> <p>Converting Markdown to Beautiful PDF with Pandoc - jdhao.github.io \u21a9</p> </li> <li> <p>more on citation styles \u21a9</p> </li> </ol>"},{"location":"blog/2023/12/31/a-beginners-guide-to-grpc-with-python/","title":"A Beginner's Guide to gRPC with Python","text":""},{"location":"blog/2023/12/31/a-beginners-guide-to-grpc-with-python/#introduction-to-grpc","title":"Introduction to gRPC","text":"<p>Have you heard of gRPC, high-performance, open-source framework that allows developers to build distributed systems and microservices ?</p> <p>gPRC uses protocol buffers as its interface definition language and provides features such as bi-directional streaming and flow control.</p> <p>In this blog post, we will explore how to get started with gRPC in Python using the official gRPC Python library. We will walk through a simple working example that demonstrates how to:</p> <ol> <li>Define a service in a <code>.proto</code> file</li> <li>Generate server and client code using the protocol buffer compiler</li> <li>Use the Python gRPC API to write a simple client and server for your service</li> </ol>"},{"location":"blog/2023/12/31/a-beginners-guide-to-grpc-with-python/#advantages-of-grpc","title":"Advantages of gRPC","text":"<p>gRPC offers several advantages, making it a versatile and efficient choice for building distributed systems and microservices:</p> <ul> <li>Language Independence: gRPC supports multiple languages seamlessly, allowing developers to build distributed systems using their preferred programming language.</li> <li>Open Source &amp; Multilingual Support: Being open source, gRPC enjoys support across various programming languages, making it a widely adopted solution for building distributed systems.</li> <li>Boilerplate Elimination: gRPC generates code, reducing the need for boilerplate code and simplifying the development process.</li> <li>Efficient Data Encoding: gRPC utilizes buffers instead of JSON for data encoding, resulting in lighter data transmission.</li> </ul>"},{"location":"blog/2023/12/31/a-beginners-guide-to-grpc-with-python/#getting-started-with-grpc-in-python","title":"Getting Started with gRPC in Python","text":""},{"location":"blog/2023/12/31/a-beginners-guide-to-grpc-with-python/#quick-setup","title":"Quick Setup","text":"<p>Follow the steps below to set up a Python environment for gRPC <sup>1</sup>:</p> <ol> <li>Quick Setup:</li> </ol> <pre><code>cd path/to/my/folder\npython -m venv .venv\nsource .venv/bin/activate\npython -m pip install --upgrade pip\n</code></pre> <ol> <li>Install gRPC and gRPC Tools:</li> </ol> <pre><code>python -m pip install grpcio\npython -m pip install grpcio-tools\n</code></pre> <ul> <li>gRPC Tools Include:<ul> <li><code>protoc</code>, the buffer compiler</li> <li>A plugin to generate client and server-side code from <code>.proto</code> files.</li> </ul> </li> </ul>"},{"location":"blog/2023/12/31/a-beginners-guide-to-grpc-with-python/#testing-an-example","title":"Testing an Example","text":"<p>Clone the gRPC repository to access a sample project:</p> <pre><code>git clone -b v1.60.0 --depth 1 --shallow-submodules https://github.com/grpc/grpc\n</code></pre> <p>Run the server in one terminal:</p> <pre><code>cd grpc/examples/python/helloworld\npython greeter_server.py\n</code></pre> <p>The output</p> Output <pre><code>Server started, listening on 50051\n</code></pre> <p>Run the client in another:</p> <pre><code>cd grpc/examples/python/helloworld\npython greeter_client.py\n</code></pre> Output <pre><code>Will try to greet world ...\nGreeter client received: Hello, you!\n</code></pre> <p>Congratulations! You've run your first gRPC application!</p>"},{"location":"blog/2023/12/31/a-beginners-guide-to-grpc-with-python/#what-the-code-does","title":"What the code does","text":"<p>The provided code includes a Protocol Buffers (protobuf) definition, a server-side implementation in Python, and a client-side implementation in Python. The protobuf definition defines a <code>Greeter</code> service with three RPC methods: <code>SayHello</code>, and <code>SayHelloStreamReply</code>. The server-side implementation defines the behavior of the <code>SayHello</code> method, while the client-side implementation makes use of these methods to communicate with the server.</p> <p>The <code>helloworld.proto</code> file defines the <code>Greeter</code> service with three RPC methods. The <code>greeter_server.py</code> file implements the server for the <code>Greeter</code> service, and the <code>greeter_client.py</code> file implements the client to communicate with the server. The <code>python -m grpc_tools.protoc</code> command is used to compile the <code>.proto</code> file and generate the necessary Python code for the server and client.</p>"},{"location":"blog/2023/12/31/a-beginners-guide-to-grpc-with-python/#adding-an-extra-method-on-the-server","title":"Adding an Extra Method on the Server","text":"<ul> <li>Modify <code>../../protos/helloworld.proto</code> and the files <code>greeter_server.py</code> and <code>greeter_client.py</code> in the <code>examples/python/helloworld</code> folder.</li> </ul> <code>examples/protos/helloworld.proto</code> <code>greeter_server.py</code> <code>greeter_client.py</code> <pre><code>...\nservice Greeter {\n  // Sends a greeting\n  rpc SayHello (HelloRequest) returns (HelloReply) {}\n\n  // Sends another greeting\n  rpc SayHelloAgain (HelloRequest) returns (HelloReply) {}\n  rpc SayHelloStreamReply (HelloRequest) returns (stream HelloReply) {}\n  ...\n}\n...\n</code></pre> <pre><code>...\nclass Greeter(helloworld_pb2_grpc.GreeterServicer):\n\n    def SayHello(self, request, context):\n        return helloworld_pb2.HelloReply(message=f\"Hello, {request.name}!\")\n\n    def SayHelloAgain(self, request, context):\n        return helloworld_pb2.HelloReply(message=f\"Hello again, {request.name}!\")\n...\n</code></pre> <pre><code>...\ndef run():\n    with grpc.insecure_channel('localhost:50051') as channel:\n        stub = helloworld_pb2_grpc.GreeterStub(channel)\n        response = stub.SayHello(helloworld_pb2.HelloRequest(name='you'))\n        print(\"Greeter client received: \" + response.message)\n        response = stub.SayHelloAgain(helloworld_pb2.HelloRequest(name='you'))\n        print(\"Greeter client received: \" + response.message)\n...\n</code></pre> <ul> <li>Compile the <code>.proto</code> file and generate the necessary Python code for the server and client</li> </ul> <pre><code>python -m grpc_tools.protoc -I../../protos --python_out=. --pyi_out=. --grpc_python_out=. ../../protos/helloworld.proto\n</code></pre> <ul> <li>Run the client and server again:</li> </ul> <pre><code>python greeter_server.py\npython greeter_client.py\n</code></pre>"},{"location":"blog/2023/12/31/a-beginners-guide-to-grpc-with-python/#what-just-happened","title":"What just happened","text":"<p>Well, we have added another RPC method, called here <code>SayHelloAgain</code>. The implementation includes:</p> <ul> <li>The protobuf definition in the <code>Greeter</code> service in the <code>greeter_server.py</code></li> <li>The server-side implementation in <code>greeter_server.py</code></li> </ul> <p>So, when running the server then the client, we should receive two responses</p> <p>The server output should remain the same</p> <pre><code>Server started, listening on 50051\n</code></pre> <p>But the client will receive two responses from the server.</p> <pre><code>Will try to greet world ...\nGreeter client received: Hello, you!\nGreeter client received: Hello again, you!\n</code></pre> <p>The <code>python -m grpc_tools.protoc</code> command is used to compile the <code>.proto</code> file and generate the necessary Python code for the server and client. This command takes the following arguments:</p> <ul> <li><code>-I../../protos</code>: Specifies the directory containing the <code>.proto</code> file.</li> <li><code>--python_out=.</code>: Specifies the output directory for the generated Python code.</li> <li><code>--grpc_python_out=.</code>: Specifies the output directory for the generated gRPC Python code.</li> </ul> <p>This command generates the <code>helloworld_pb2.py</code> file, which contains the generated request and response classes, and the <code>helloworld_pb2_grpc.py</code> file, which contains the generated server and client stubs.</p> <p>The <code>python -m grpc_tools.protoc</code> command is the recommended way to generate Python code from a <code>.proto</code> file for use with gRPC.</p> <p>For more information, you can refer to the gRPC Python documentation and the Protocol Buffer Basics: Python tutorial.</p> <p>If you need to compile <code>.proto</code> files for other programming languages, the process may differ, and you can refer to the respective language's gRPC documentation for guidance.</p>"},{"location":"blog/2023/12/31/a-beginners-guide-to-grpc-with-python/#further-reading","title":"Further Reading","text":"<ul> <li>Introduction to gRPC</li> <li>gRPC Core Concepts</li> <li>Explore the Python API Reference to discover functions and classes.</li> </ul> <p>For more detailed instructions, refer to the gRPC Python Quickstart <sup>1</sup>.</p> <ol> <li> <p>https://grpc.io/docs/languages/python/quickstart/ \u21a9\u21a9</p> </li> </ol>"},{"location":"blog/2023/11/11/deploying-any-web-application-with-nginx-example-of-flask/","title":"Deploying any Web application with Nginx: Example of Flask","text":""},{"location":"blog/2023/11/11/deploying-any-web-application-with-nginx-example-of-flask/#introduction","title":"Introduction","text":"<p>You have created your flask application. How nice ! Now, you want to go a step further and deploy it. For most hosting services, you have nice interfaces to deploy your python applications with support for flask. But sometimes, you only have access via ssh to the server.</p> <p>This is a very straigthforward tutorial on how to do it.</p> <p>This tutorial also applies to any web server you can run on local but want to deploy</p>"},{"location":"blog/2023/11/11/deploying-any-web-application-with-nginx-example-of-flask/#step-1-install-nginx-and-flask","title":"Step 1: Install Nginx and Flask","text":"<p>Make sure you have Nginx and Flask installed on your server. If not, install them using the appropriate package manager for your operating system.</p>"},{"location":"blog/2023/11/11/deploying-any-web-application-with-nginx-example-of-flask/#step-2-configure-nginx","title":"Step 2: Configure Nginx","text":"<p>Create a new Nginx configuration file for your Flask app in the <code>/etc/nginx/sites-available/</code> directory. For example, you could name it <code>myapp.conf</code>. Edit the file and add the following configuration:</p> <pre><code>server {\n    listen 80;\n    server_name yourdomain.com;\n    location / {\n        proxy_pass http://localhost:5000; # assuming Flask is running on port 5000\n        include /etc/nginx/proxy_params;\n        proxy_redirect off;\n    }\n}\n</code></pre> <p>This tells Nginx to listen on port 80 (HTTP), forward all requests to your Flask app running on <code>localhost:5000</code>, and include some proxy parameters.</p>"},{"location":"blog/2023/11/11/deploying-any-web-application-with-nginx-example-of-flask/#step-3-create-a-symbolic-link","title":"Step 3: Create a symbolic link","text":"<p>Create a symbolic link from the <code>sites-available</code> directory to the <code>sites-enabled</code> directory by running the following command:</p> <pre><code>sudo ln -s /etc/nginx/sites-available/myapp.conf /etc/nginx/sites-enabled/\n</code></pre> <p>This will enable your Nginx configuration.</p>"},{"location":"blog/2023/11/11/deploying-any-web-application-with-nginx-example-of-flask/#step-4-test-the-configuration","title":"Step 4: Test the configuration","text":"<p>Test your Nginx configuration by running the following command:</p> <pre><code>sudo nginx -t\n</code></pre> <p>If there are no errors, reload Nginx by running:</p> <pre><code>sudo service nginx reload\n</code></pre>"},{"location":"blog/2023/11/11/deploying-any-web-application-with-nginx-example-of-flask/#step-5-start-the-flask-app","title":"Step 5: Start the Flask app","text":"<p>Start your Flask app by running the following command:</p> <pre><code>python app.py\n</code></pre> <p>Your Flask app should now be running and accessible through Nginx at <code>http://yourdomain.com</code>.</p>"},{"location":"blog/2023/11/11/deploying-any-web-application-with-nginx-example-of-flask/#exemple-of-flask-app","title":"exemple of flask app","text":"<pre><code>from flask import Flask\n\napp = Flask(__name__)\n\n@app.route('/')\ndef index():\n    return 'Hello, World!'\n\nif __name__ == '__main__':\n    app.run()\n</code></pre>"},{"location":"blog/2024/01/11/navigating-redirect-challenges-with-github-pages-a-creative-approach-to-domain-migration/","title":"Navigating Redirect Challenges With GitHub Pages: A Creative Approach to Domain Migration","text":""},{"location":"blog/2024/01/11/navigating-redirect-challenges-with-github-pages-a-creative-approach-to-domain-migration/#introduction","title":"Introduction","text":"<p>Imagine having a GitHub Pages website. Now, you've migrated to project on another GitHub Pages website. As reports surfaced about users being unable to access the site, the need for a swift redirection from old URLs to the current ones became paramount.</p> <p>The catch? The solution had to operate within the constraints of a static web page, using only HTML, CSS, and JavaScript.</p> <p>While conventional methods like Flask and Frozen-Flask failed, the journey led to a creative solution using HTML and JavaScript. In this blog post, I'll share the step-by-step process of how I navigated through the obstacles and achieved seamless redirection.</p>"},{"location":"blog/2024/01/11/navigating-redirect-challenges-with-github-pages-a-creative-approach-to-domain-migration/#the-initial-attempts-static-html","title":"The Initial Attempts: Static HTML","text":""},{"location":"blog/2024/01/11/navigating-redirect-challenges-with-github-pages-a-creative-approach-to-domain-migration/#using-html-for-single-page-redirect","title":"Using HTML for Single Page Redirect","text":"<p>My first inclination was to use HTML for redirection. To redirect only one page, a simple HTML script could be used. For instance:</p> <p>Using HTML for Single Page Redirect</p> <pre><code>&lt;!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\"&gt;\n&lt;html lang=\"fr\"&gt;\n    &lt;head&gt;\n        &lt;title&gt;Accueil&lt;/title&gt;\n        &lt;script&gt;window.location.replace(\"https://hermann-web.github.io/blog/\")&lt;/script&gt;\n    &lt;/head&gt;\n&lt;body&gt;&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"blog/2024/01/11/navigating-redirect-challenges-with-github-pages-a-creative-approach-to-domain-migration/#static-html-challenges","title":"Static HTML Challenges","text":"<p>However, it quickly became apparent that defining redirects for every page statically was impractical, as I would need to create an HTML page for each endpoint. Then, I thought of using React or Flask, but they require explicitly defined routes too.</p> <p>Regardless, there is a module that uses Flask to implement the first option. So, I can combine the list of URLs with the Frozen-Flask module.</p> <p>The existence of this URL list proved crucial, as without it, I would have encountered additional difficulties.</p>"},{"location":"blog/2024/01/11/navigating-redirect-challenges-with-github-pages-a-creative-approach-to-domain-migration/#another-way-flask-and-frozen-flask","title":"Another Way: Flask and Frozen-Flask","text":"<p>Next, I explored Flask and Frozen-Flask, but challenges arose when dealing with dynamic endpoints in a static context. The attempt to freeze the Flask app yielded an error, highlighting the limitations of this approach.</p>"},{"location":"blog/2024/01/11/navigating-redirect-challenges-with-github-pages-a-creative-approach-to-domain-migration/#first-attempts-with-flask","title":"First Attempts with Flask","text":"Prerequisites <p>Before diving into the implementation, I set up the development environment with the following commands:</p> <pre><code># Install Flask and Frozen-Flask\npython -m venv .venv\nsource .venv/bin/activate\npip install Flask Frozen-Flask\n\n# Download the list of endpoints\nwget https://raw.githubusercontent.com/Hermann-web/blog/gh-pages/sitemap.xml\n</code></pre> <p>After environment setup, I've created three python files</p> <ul> <li><code>utils.py</code> to parse the sitemap.xml file and extract the necessary endpoints</li> <li><code>app.py</code> responsible for handling the redirection logic</li> <li><code>main.py</code> convert the Flask app into a static website using <code>frozen-flask</code> module</li> </ul> First Attemps with Flask <code>utils.py</code> <code>app.py</code> <code>main.py</code> <pre><code>import xml.etree.ElementTree as ET\n\n# Load the XML file\nxml_file_path = \"sitemap.xml\"\ntree = ET.parse(xml_file_path)\nroot = tree.getroot()\n\n# Define the namespace\nnamespace = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n\nWEBSITE_URL = \"https://hermann-web.github.io/blog\"\n\ndef endpoint_parser(endpoint:str):\n    if endpoint.startswith(\"/\"):\n        endpoint = endpoint[1:]\n    if endpoint.endswith(\"/\"):\n        endpoint = endpoint[:-1]\n    return endpoint\n\n# Extract endpoint URLs\ndef get_endpoints():\n    endpoints = [url_element.text for url_element in root.findall('.//ns:loc', namespace)]\n    endpoints = [endpoint_parser(url.replace(WEBSITE_URL, \"\")) for url in endpoints]\n    return endpoints\n</code></pre> <pre><code>from flask import Flask, redirect\nfrom utils import WEBSITE_URL, endpoint_parser\n\napp = Flask(__name__)\ntarget_domain = WEBSITE_URL\n\nassert target_domain\n\n\n@app.route('/')\ndef index():\n    return redirect(target_domain)\n\n# Create a route for redirection\n@app.route('/&lt;path:endpoint&gt;')\ndef redirect_to_another_server(endpoint):\n    endpoint = endpoint_parser(endpoint)\n    print(\"endpoint:\",endpoint)\n    if endpoint in endpoints:\n        target_url = f\"{target_domain}/{endpoint}\"\n        return redirect(target_url)\n    else:\n        return redirect(target_domain)\n\nif __name__ == \"__main__\":\n    app.run(debug=True)\n</code></pre> <pre><code>from flask_frozen import Freezer\nfrom app import app\n\nfreezer = Freezer(app)\n\nif __name__ == '__main__':\n    freezer.freeze()\n</code></pre> <p>Running the app went fine:</p> <pre><code>python app.py\n</code></pre> <pre><code> * Serving Flask app 'app'\n * Debug mode: on\nWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n * Running on http://127.0.0.1:5000\nPress CTRL+C to quit\n * Restarting with stat\n * Debugger is active!\n</code></pre> <p>However, running <code>python main.py</code> to freeze the Flask app using Frozen-Flask failed due to the inability to follow external redirects.</p> <pre><code>RuntimeError: Following external redirects is not supported.\n</code></pre> <p>This makes sense. A static webpage cannot accept dynamic endpoints. So, I should create the endpoints manually using a for-loop, hoping it will work with Flask.</p> <p>So, i've tried some workarounds</p>"},{"location":"blog/2024/01/11/navigating-redirect-challenges-with-github-pages-a-creative-approach-to-domain-migration/#workarounds-with-flask","title":"Workarounds with Flask","text":""},{"location":"blog/2024/01/11/navigating-redirect-challenges-with-github-pages-a-creative-approach-to-domain-migration/#workaround-1-manually-creating-routes","title":"Workaround 1: Manually Creating Routes","text":"<p>However, failed. The issue was that I should not have duplicated function names for the routes:</p> Manually Creating Routes <pre><code>from flask import Flask, redirect\nfrom get_endpoints import get_endpoints, WEBSITE_URL, endpoint_parser\n\napp = Flask(__name__)\ntarget_domain = WEBSITE_URL\n\nassert target_domain\n\nendpoints = get_endpoints()\n\n# Create route functions for each endpoint\nfor endpoint in endpoints:\n    @app.route(f'/{endpoint}')\n    def redirect_to_another_server():\n        target_url = f\"{target_domain}/{endpoint}\"\n        return redirect(target_url)\n\n\nif __name__ == \"__main__\":\n    app.run(debug=True)\n</code></pre> <p>A first attempt to automatically generate route functions in Flask failed, as <code>redirect_to_another_server</code> was duplicated:</p> <pre><code>AssertionError: View function mapping is overwriting an existing endpoint function: redirect_to_another_server\n</code></pre> <p>So, that approach faced an issue with function duplication, prompting a need for a workaround.</p>"},{"location":"blog/2024/01/11/navigating-redirect-challenges-with-github-pages-a-creative-approach-to-domain-migration/#workaround-2-dynamic-function-generation","title":"Workaround 2: Dynamic Function Generation","text":"<p>There is another solution that involves encapsulating the route functions within another function, ensuring a unique context for each endpoint.</p> <p>Another solution can be to change the function name with a decorator, but it is not possible. So, I figured I can define the functions inside another function, hoping it will work.</p> Dynamic Function Generation <pre><code>def generate_endpoint(endpoint):\n    @app.route(f'/{endpoint_parser(endpoint)}')\n    def dynamic_function():\n        target_url = f\"{target_domain}/{endpoint}\"\n        return redirect(target_url)\n\n# Create route functions for each endpoint\nfor endpoint in endpoints:\n    generate_endpoint(endpoint)\n</code></pre> <p>The attempt to generate dynamic functions also faced an issue with function duplication.</p> <pre><code>AssertionError: View function mapping is overwriting an existing endpoint function: dynamic_function\n</code></pre>"},{"location":"blog/2024/01/11/navigating-redirect-challenges-with-github-pages-a-creative-approach-to-domain-migration/#final-solution-overcoming-a-static-constraint","title":"Final Solution: Overcoming a Static Constraint","text":"<p>When attempting to freeze the Flask app using Frozen-Flask, a runtime error occurs due to the inability to follow external redirects. This limitation is inherent in static web pages, preventing the use of dynamic endpoints.</p> <p>To work around this constraint, a custom <code>404.html</code> page is created, embedding JavaScript to correct the URL before redirecting. This clever solution ensures that even erroneous URLs lead users to the correct destination.</p> <pre><code>&lt;!-- 404.html --&gt;\n&lt;!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\"&gt;\n&lt;html lang=\"en\"&gt;\n\n&lt;head&gt;\n    &lt;title&gt;Page Not Found&lt;/title&gt;\n    &lt;script&gt;\n        // Redirect logic to correct the URL\n        let new_url = \"/\";\n        if (window.location.href.startsWith(\"https://hermann-web.github.io/web\")) {\n            new_url = window.location.href.replace(\"https://hermann-web.github.io/web\", \"/blog\");\n        }\n        window.location.replace(new_url);\n    &lt;/script&gt;\n&lt;/head&gt;\n\n&lt;body&gt;&lt;/body&gt;\n\n&lt;/html&gt;\n</code></pre> <p>I noticed that all erroneous URLs redirect the user to the <code>404.html</code> page. For GitHub Pages, I made the remark that even on the <code>404.html</code> page, the erroneous URLs are conserved in the browser. So, I can just correct the last endpoint <code>/web/*</code> to the correct one <code>/blog/*</code> using JavaScript.</p>"},{"location":"blog/2024/01/11/navigating-redirect-challenges-with-github-pages-a-creative-approach-to-domain-migration/#conclusion","title":"Conclusion","text":"<p>In this guide, we explored a step-by-step approach to redirecting all pages from one domain to another using Flask and Frozen-Flask. From parsing the <code>sitemap.xml</code> file to handling dynamic endpoints and overcoming static constraints, each aspect was covered in detail. The use of a custom <code>404.html</code> page with JavaScript ensures a smooth redirection experience for users, making this solution both effective and elegant.</p>"},{"location":"blog/2023/12/17/getting-started-with-mkdocs-for-documentation/","title":"MkDocs: Your Straightforward Documentation Companion","text":""},{"location":"blog/2023/12/17/getting-started-with-mkdocs-for-documentation/#introduction","title":"Introduction","text":"<p>Welcome to MkDocs: the hassle-free documentation solution!</p> <p>In search of a tool that makes documentation creation a breeze? MkDocs is your answer!</p> <p>This straightforward platform simplifies the process of generating professional project documentation.</p> <p>This guide is your gateway to exploring MkDocs' user-friendly approach. You'll uncover how this tool streamlines the creation of polished and organized documentation for all your projects. Let's dive in and harness MkDocs' straightforwardness for your documentation needs.</p>"},{"location":"blog/2023/12/17/getting-started-with-mkdocs-for-documentation/#comparing-documentation-tools","title":"Comparing Documentation Tools","text":"<p>When it comes to documenting projects, various tools offer unique features and complexities. Let's explore a few:</p> <ul> <li> <p>Sphinx: Known for its robustness and flexibility, Sphinx is powerful but can be intricate for beginners due to its configuration requirements.</p> </li> <li> <p>Docusaurus: Ideal for creating user-centric documentation with React, but might feel overwhelming for those unfamiliar with JavaScript frameworks.</p> </li> <li> <p>GitBook: Offers a user-friendly interface, yet its extensive feature set might be more than needed for straightforward documentation needs.</p> </li> <li> <p>MkDocs: Unlike some other tools, MkDocs stands out for its simplicity. It's based on Markdown, a plain text format, making it incredibly easy to use. With MkDocs, creating professional documentation feels straightforward and hassle-free.</p> </li> </ul>"},{"location":"blog/2023/12/17/getting-started-with-mkdocs-for-documentation/#mkdocs-embracing-simplicity","title":"MkDocs: Embracing Simplicity","text":"<p>MkDocs adopts Markdown, a plain text format widely accessible and intuitive for beginners. Its minimalistic approach enables users to focus on content creation without getting lost in complex configurations.</p>"},{"location":"blog/2023/12/17/getting-started-with-mkdocs-for-documentation/#features-of-mkdocs","title":"Features of MkDocs","text":"<ul> <li>Simple Configuration: MkDocs requires minimal setup, with a straightforward configuration file.</li> <li>User-Friendly: Its Markdown-based structure simplifies content creation for all levels of users.</li> <li>Live Preview: Offers a live preview of documentation, ensuring instant visual feedback.</li> <li>Extensibility: While basic, MkDocs supports various themes and plugins for enhanced functionality.</li> </ul> <p>MkDocs excels in providing a straightforward and efficient way to create professional documentation without overwhelming users with unnecessary complexities. It's the perfect choice for those seeking a quick and easy documentation solution.</p>"},{"location":"blog/2023/12/17/getting-started-with-mkdocs-for-documentation/#tutorial-getting-started-with-mkdocs","title":"Tutorial: Getting Started with MkDocs","text":"<p>MkDocs simplifies the process of creating documentation for your Python projects. Follow these steps to create a documentation site using MkDocs:</p>"},{"location":"blog/2023/12/17/getting-started-with-mkdocs-for-documentation/#1-install-mkdocs","title":"1. Install MkDocs","text":"<p>Install MkDocs by running the following command in your terminal:</p> <pre><code>pip install mkdocs\n</code></pre>"},{"location":"blog/2023/12/17/getting-started-with-mkdocs-for-documentation/#2-set-up-your-project","title":"2. Set Up Your Project","text":"<p>Create a new directory for your project and initialize an MkDocs project:</p> <pre><code>mkdir my-project\ncd my-project\nmkdocs new .\n</code></pre> <p>This creates a new <code>mkdocs.yml</code> configuration file and a <code>docs</code> directory with a sample Markdown file.</p>"},{"location":"blog/2023/12/17/getting-started-with-mkdocs-for-documentation/#3-install-a-theme","title":"3. Install a Theme","text":"<p>Enhance your documentation's appearance by installing a theme like <code>mkdocs-material</code>:</p> <pre><code>pip install mkdocs-material\n</code></pre>"},{"location":"blog/2023/12/17/getting-started-with-mkdocs-for-documentation/#4-configure-your-site","title":"4. Configure Your Site","text":"<p>Edit the <code>mkdocs.yml</code> file to configure your documentation site. Define the title, theme, and pages to include. Check examples.</p> <ul> <li>Configure <code>docs_dir</code> to specify the folder where MkDocs will find <code>.md</code> files.</li> <li>Use the <code>nav</code> section to structure your files into tabs.</li> </ul>"},{"location":"blog/2023/12/17/getting-started-with-mkdocs-for-documentation/#5-write-documentation","title":"5. Write Documentation","text":"<p>Create your documentation in Markdown format and save the files in the <code>docs</code> directory.</p>"},{"location":"blog/2023/12/17/getting-started-with-mkdocs-for-documentation/#6-preview-your-site","title":"6. Preview Your Site","text":"<p>To preview your site locally, run:</p> <pre><code>mkdocs serve\n</code></pre> <p>This will start a local web server and open your documentation site in your default web browser. You can make changes to your documentation and the site will automatically update.</p>"},{"location":"blog/2023/12/17/getting-started-with-mkdocs-for-documentation/#7-optional-more-options","title":"7. (optional) More options","text":"<p>You can add more options. For example,</p> <pre><code>mkdocs serve --dirty -a localhost:8001\n</code></pre> <p>Note</p> <ul> <li><code>--dirty</code>: Only re-build files that have changed.</li> <li><code>-a, --dev-addr &lt;IP:PORT&gt;</code>: IP address and port to serve documentation locally (default: localhost:8000)</li> <li>use <code>mkdocs serve -h</code> for more options</li> </ul> <p>warning</p> <p><code>A 'dirty' build [...] will likely lead to inaccurate navigation and other links within your site. This option is designed for site development purposes only.</code>, mkdocs</p>"},{"location":"blog/2023/12/17/getting-started-with-mkdocs-for-documentation/#8-build-your-site","title":"8. Build Your Site","text":"<p>Generate a static HTML site by running:</p> <pre><code>mkdocs build\n</code></pre> <p>This creates a <code>site</code> directory containing the built site. You can deploy this to a web server.</p> <p>Remember, MkDocs supports numerous plugins, such as <code>mkdocs-run-shell-cmd-plugin</code>, enabling extended functionalities.</p>"},{"location":"blog/2023/12/17/getting-started-with-mkdocs-for-documentation/#conclusion","title":"Conclusion","text":"<p>MkDocs provides a straightforward way to create and manage documentation for Python projects. With its simple setup, configuration, and live preview features, it streamlines the documentation process, making it an excellent choice for developers.</p>"},{"location":"blog/2023/12/17/getting-started-with-mkdocs-for-documentation/#more-ressources","title":"More ressources","text":"<p>Explore the resources below to dive deeper into MkDocs:</p> <ul> <li>Real Python - Python Project Documentation with MkDocs</li> <li>MkDocs - Getting Started</li> <li>MkDocs - User Guide CLI</li> <li>MkDocs - Issues and Discussions</li> <li>YouTube - Getting Started with MkDocs</li> <li>MkDocs Run Shell Cmd Plugin</li> </ul>"},{"location":"blog/2023/12/17/getting-started-with-mkdocs-for-documentation/#related-pages","title":"Related Pages","text":"<ul> <li>MkDocs: Your Straightforward Documentation Companion</li> </ul>"},{"location":"blog/2023/12/25/using-mkdocs-with-docker-streamlining-documentation-workflow/","title":"Using MkDocs with Docker: Streamlining Documentation Workflow","text":""},{"location":"blog/2023/12/25/using-mkdocs-with-docker-streamlining-documentation-workflow/#introduction","title":"Introduction","text":"<p>Looking to streamline your documentation workflow using MkDocs and Docker?</p> <p>Documentation lies at the heart of every successful project. MkDocs offers a straightforward way to create elegant documentation sites, while Docker ensures a consistent and isolated environment for various applications. Combining these tools optimizes the documentation process and enhances collaboration within development teams.</p> <p>This tutorial serves as your guide, illustrating how to set up MkDocs within a Docker container effectively. By following these steps, you'll establish a robust documentation framework, facilitating seamless documentation creation and deployment.</p> <p>Let's dive into the process of integrating MkDocs with Docker to revolutionize your documentation workflow.</p>"},{"location":"blog/2023/12/25/using-mkdocs-with-docker-streamlining-documentation-workflow/#prerequisites","title":"Prerequisites","text":"<p>Before starting, make sure you have the following files in the same directory:</p> <ol> <li><code>docker-compose.yml</code></li> <li><code>requirements.txt</code> (or your specific requirements file)</li> </ol> <p>For example,</p> <pre><code># python 3.9.18\nmkdocs==1.5.3\nmkdocs-material==9.4.7\nmkdocs-material-extensions==1.3\nmkdocs-minify-plugin==0.7.1\nmkdocs-roamlinks-plugin==0.3.2\n</code></pre> <ol> <li><code>mkdocs.yml</code> (the MkDocs configuration file)</li> <li>A folder named <code>docs</code> containing your documentation files</li> <li>Optionally, a <code>.dockerignore</code> file to exclude unnecessary files from the Docker image</li> </ol> <p>For example,</p> <pre><code>venv/\n</code></pre>"},{"location":"blog/2023/12/25/using-mkdocs-with-docker-streamlining-documentation-workflow/#setting-up-mkdocs-with-docker","title":"Setting Up MkDocs with Docker","text":"<p>Let's create a <code>docker-compose.yml</code> file:</p> <pre><code>version: '3'\n\nservices:\n  mkdocs:\n    image: python:3.9.18\n    volumes:\n      - ./:/app/\n    working_dir: /app\n    ports:\n      - \"49162:8000\"\n    command: &gt;\n      bash -c \"\n        pip install -r requirements.txt &amp;&amp;\n        mkdocs serve -a 0.0.0.0:8000\"\n</code></pre> <p>Ensure your <code>requirements.txt</code> contains the necessary dependencies as outlined in the example. Customize it based on your project's requirements.</p> <p>After placing all the required files in the same directory, open a terminal or command prompt and navigate to this directory.</p> <p>Execute the following command:</p> <pre><code>docker-compose up\n</code></pre> <p>This command builds the Docker image and starts the MkDocs server. Access the MkDocs site by visiting <code>http://localhost:49162</code> in your web browser.</p> <p>hot reload</p> <p>As mkdocs rebuild all the files when changes are made, you may want to add the <code>--dirty</code> option to <code>mkdocs serve</code> to rebuild only the modified files. Read more about it in the mkdocs tutorial</p>"},{"location":"blog/2023/12/25/using-mkdocs-with-docker-streamlining-documentation-workflow/#conclusion","title":"Conclusion","text":"<p>Integrating MkDocs with Docker simplifies the setup process, ensuring consistency across different environments. It provides an isolated space for documentation management, enhancing collaboration and deployment.</p> <p>Remember to replace placeholder file names (<code>requirements.txt</code>, <code>docs</code>, etc.) with your actual file names if they differ.</p> <p>By following this guide, you've streamlined your documentation workflow using MkDocs within a Docker container, fostering efficient documentation management for your projects.</p>"},{"location":"blog/2023/12/25/using-mkdocs-with-docker-streamlining-documentation-workflow/#related-pages","title":"Related Pages","text":"<ul> <li>MkDocs: Your Straightforward Documentation Companion</li> <li>Mastering Docker: A Comprehensive Guide to Efficient Container Management</li> <li>Simple guide to using Docker on Windows 10 and access from WSL 2</li> </ul>"},{"location":"blog/2023/12/14/mastering-ssh-and-file-transfers-to-remote-servers-a-beginners-handbook/","title":"Mastering SSH and File Transfers to Remote servers: A Beginner's Handbook","text":""},{"location":"blog/2023/12/14/mastering-ssh-and-file-transfers-to-remote-servers-a-beginners-handbook/#introduction","title":"Introduction","text":"<p>Do you find yourself baffled by the intricacies of SSH connections and file transfers to remote servers ?</p> <p>Navigating the landscape of SSH connections, troubleshooting connection issues, and securely transferring files across servers can be a daunting task, especially for newcomers.</p> <p>This guide is your compass in the world of SSH, unraveling the complexities and providing step-by-step instructions for establishing secure connections and transferring files seamlessly using Git Bash or WSL2 for Windows users and straightforward methods for Linux enthusiasts.</p> <p>Whether you're a developer, sysadmin, or tech enthusiast stepping into the realm of remote server access or seeking efficient file transfer solutions, this guide is tailored to demystify SSH, troubleshoot common pitfalls, and equip you with the skills to maneuver through file transfers effortlessly.</p> <p>This document break down the process of connecting via SSH and file transfer and should help someone new to SSH understand the process, troubleshoot common issues, and handle file transfers easily</p>"},{"location":"blog/2023/12/14/mastering-ssh-and-file-transfers-to-remote-servers-a-beginners-handbook/#requirements","title":"Requirements","text":"<ul> <li>For windows users, use Git Bash installed on your computer or use wsl2 (for windows &gt;=10)</li> <li>For linux users, this should be straighforward</li> </ul>"},{"location":"blog/2023/12/14/mastering-ssh-and-file-transfers-to-remote-servers-a-beginners-handbook/#connecting-via-ssh-from-cli","title":"Connecting via SSH from cli","text":""},{"location":"blog/2023/12/14/mastering-ssh-and-file-transfers-to-remote-servers-a-beginners-handbook/#steps-to-connect","title":"Steps to Connect","text":"<ol> <li> <p>Open Terminal: Or search for Git Bash in your applications and open it.</p> </li> <li> <p>Accessing the Server:</p> <ul> <li> <p>Use the command</p> <pre><code>ssh {username}@{domain}\n</code></pre> <p>or</p> <pre><code>ssh {username}@{server_ip}\n</code></pre> </li> <li> <p>Replace <code>{username}</code> with your remote server username.</p> </li> <li>Replace <code>{domain}</code> with the domain name or <code>{server_ip}</code> with the server's IP address.</li> </ul> </li> <li> <p>Adding a Specific Port:</p> <ul> <li>If the server uses a different port (usually 22), use <code>ssh {username}@{domain} -p {port}</code>. Replace <code>{port}</code> with the correct port number.</li> </ul> </li> <li> <p>Entering Credentials:</p> <ul> <li>After executing the command, you'll be prompted to enter your remote server account password. Type it in and press Enter.</li> </ul> </li> </ol>"},{"location":"blog/2023/12/14/mastering-ssh-and-file-transfers-to-remote-servers-a-beginners-handbook/#troubleshooting-ssh-connection-issues","title":"Troubleshooting SSH Connection Issues","text":""},{"location":"blog/2023/12/14/mastering-ssh-and-file-transfers-to-remote-servers-a-beginners-handbook/#error-unable-to-negotiate-with-port","title":"Error: \"Unable to negotiate with... port...\"","text":"<p>If you encounter the error <code>Unable to negotiate with &lt;IP&gt; port &lt;Port&gt;: no matching host key type found. Their offer: ssh-rsa,ssh-dss</code></p> <p>The Solution</p> <ul> <li>Configure the client to accept the host key sent by the server.</li> <li>Edit the <code>~/.ssh/config</code> file:</li> </ul> <pre><code>Host {domain}\n    HostKeyAlgorithms +ssh-rsa,ssh-dss\n</code></pre> <ul> <li>Use a text editor like Nano, Vim, or Notepad to modify the file.</li> </ul> Other Common SSH errors: <ul> <li>Permissions: Ensure correct file permissions for <code>~/.ssh</code> and authorized_keys.</li> <li>Network problems: Check firewall settings and network connectivity.</li> </ul>"},{"location":"blog/2023/12/14/mastering-ssh-and-file-transfers-to-remote-servers-a-beginners-handbook/#alternative-to-ssh-access-though-a-cli-putty-and-openssh","title":"Alternative to ssh access though a cli: Putty and OpenSSH","text":"<ul> <li>Putty (Windows):</li> </ul> <p>Known for its user-friendly GUI, Putty offers a straightforward interface for establishing SSH connections on Windows systems. It's particularly popular among users who prefer a graphical interface for SSH connections.</p> <p>That's why Putty is a popular SSH client primarily used on Windows systems. However, it's worth noting that while Putty is predominantly associated with Windows, it can also be utilized on other operating systems through compatibility layers or third-party tools like Wine on Linux or macOS.</p> <ul> <li>OpenSSH:</li> </ul> <p>OpenSSH, on the other hand, is an open-source implementation of the SSH protocol. It's available not just for Windows but also for Linux, macOS, and various Unix-like operating systems. OpenSSH provides both the client (ssh) and server (sshd) components, making it a versatile and widely adopted solution for secure remote access, file transfer, and tunneling across different platforms. It offers a robust set of features, including secure remote access, file transfer (using tools like <code>scp</code> and <code>sftp</code>), and tunneling capabilities.</p> <p>Thats's why OpenSSH is often preferred by users who work in mixed environments or want a consistent SSH experience across different operating systems. It's commonly used in command-line environments and scripts due to its versatile nature.</p>"},{"location":"blog/2023/12/14/mastering-ssh-and-file-transfers-to-remote-servers-a-beginners-handbook/#file-transfer-using-ssh","title":"File Transfer Using SSH","text":"<p>You can use <code>scp</code> command to transfer files directly between two servers (local to remote or one remote to another) by specifying their addresses and file paths</p>"},{"location":"blog/2023/12/14/mastering-ssh-and-file-transfers-to-remote-servers-a-beginners-handbook/#sending-files-to-remote-server","title":"Sending Files to Remote Server","text":"<ul> <li>Use the <code>scp</code> command:</li> </ul> <pre><code>local_file=\"/path/to/local/file\"\nremote_file=\"$remote_user@$remote_host:/path/to/remote/file/or/folder\"\nscp \"$local_file\" \"$remote_file\"\n</code></pre> <ul> <li>You'll be prompted for the password before the file is sent.</li> </ul>"},{"location":"blog/2023/12/14/mastering-ssh-and-file-transfers-to-remote-servers-a-beginners-handbook/#transferring-between-servers","title":"Transferring Between Servers","text":"<p>Similar to local to server transfer, use <code>scp</code> between two remote servers by specifying their addresses.</p> <p>Use the <code>scp</code> command to transfer files directly between two remote servers:</p> <pre><code>```bash\nremote_file_source=\"$remote_user1@$remote_host1:/path/to/source/file\"\nremote_file_destination=\"$remote_user2@$remote_host2:/path/to/destination/folder\"\nscp \"$remote_file_source\" \"$remote_file_destination\"\n```\n</code></pre> <p>Replace:     - <code>$remote_user1</code> with the username for the first remote server.     - <code>$remote_host1</code> with the address or IP of the first remote server.     - <code>$remote_user2</code> with the username for the second remote server.     - <code>$remote_host2</code> with the address or IP of the second remote server.</p> <p>This will transfer the specified file from the first remote server to the specified folder on the second remote server.</p>"},{"location":"blog/2023/12/14/mastering-ssh-and-file-transfers-to-remote-servers-a-beginners-handbook/#scp-options","title":"SCP Options","text":"<p>Using <code>-r</code> for recursive copying of directories:</p> <pre><code>scp -r local_directory username@remote_host:/remote_directory\n</code></pre>"},{"location":"blog/2023/12/14/mastering-ssh-and-file-transfers-to-remote-servers-a-beginners-handbook/#alternative-file-transfer-methods","title":"Alternative File Transfer Methods","text":"<p>Using <code>rsync</code> for efficient synchronization:</p> <pre><code>rsync -avz --progress /path/to/source username@remote_host:/path/to/destination\n</code></pre>"},{"location":"blog/2023/12/14/mastering-ssh-and-file-transfers-to-remote-servers-a-beginners-handbook/#running-commands-on-a-remote-server-without-accessing-its-cli","title":"Running commands on a remote server without accessing its cli","text":"<p>For example, using ssh to Fetch latest changes from the remote repository</p> <pre><code>ssh \"$remote_user@$remote_host\" \"cd $remote_path &amp;&amp; git fetch\"\n</code></pre>"},{"location":"blog/2023/12/14/mastering-ssh-and-file-transfers-to-remote-servers-a-beginners-handbook/#simplifying-ssh-access-with-sshpass","title":"Simplifying SSH Access with sshpass","text":"<p>To avoid being prompted to write the password, <code>sshpass</code> is a tool you want</p> <ol> <li> <p>Install sshpass:     If needed, install sshpass using <code>sudo apt install sshpass</code>.</p> </li> <li> <p>Accessing SSH without Password Prompt:     Instead of</p> <pre><code>ssh \"$remote_user@$remote_host\"\n</code></pre> <p>use</p> <pre><code>sshpass -p \"$password\" ssh \"$remote_user@$remote_host\"\n</code></pre> </li> </ol> <p>to tetch latest changes from the remote repository, run</p> <pre><code>sshpass -p \"$password\" ssh \"$remote_user@$remote_host\" \"cd $remote_path &amp;&amp; git fetch\"\n</code></pre>"},{"location":"blog/2023/12/14/mastering-ssh-and-file-transfers-to-remote-servers-a-beginners-handbook/#conclusion","title":"Conclusion","text":"<p>Congratulations! You've now mastered the fundamentals of SSH connections and file transfers using Git Bash.</p> <p>In this guide, we've covered the essential steps to initiate SSH connections, troubleshoot common errors, and conduct seamless file transfers between local and remote servers. You've learned to troubleshoot connection issues, enhance security configurations, and optimize file transfers using <code>scp</code> and <code>rsync</code>.</p> <p>Remember, SSH is a powerful tool for secure communication and file transfer, and understanding its nuances empowers you to work efficiently across different servers and systems.</p> <p>As you continue your journey, keep exploring the capabilities of SSH, experiment with different options and configurations, and don't hesitate to delve deeper into security best practices.</p> <p>Whether you're a developer collaborating on remote repositories or a system administrator managing servers, the knowledge gained here will serve as a solid foundation for your endeavors.</p> <p>Embrace the power of SSH, continue to explore, and may your future endeavors in remote access and file transfer be secure, efficient, and hassle-free!</p>"},{"location":"blog/2024/03/11/setting-up-remote-desktop-access-with-remmina-on-ubuntu/","title":"How to Set Up Remote Desktop Access from Linux to Windows 10 Using Remmina","text":"<p>Remote desktop access has become an essential feature in today's digital landscape, allowing users to connect to their computers from anywhere. While Windows users have built-in options for remote desktop access, Linux users often need to rely on third-party applications.</p> <p>In this guide, we'll explore how to set up remote desktop access from a Linux system to a Windows 10 machine using Remmina.</p>"},{"location":"blog/2024/03/11/setting-up-remote-desktop-access-with-remmina-on-ubuntu/#microsofts-remote-desktop-services","title":"Microsoft's Remote Desktop Services","text":"<p>Microsoft offers extensive documentation on remote desktop services, providing official clients for various platforms such as Windows 10, macOS, and others. However, there isn't an official client for Linux systems. This gap has led Linux users to explore alternative solutions, with Remmina being a popular choice.</p>"},{"location":"blog/2024/03/11/setting-up-remote-desktop-access-with-remmina-on-ubuntu/#step-1-preparing-your-windows-10-machine","title":"Step 1: Preparing Your Windows 10 Machine","text":"<p>Before connecting remotely to your Windows 10 machine, you'll need to enable remote connections.</p> <ol> <li> <p>Enable Remote Connections: Navigate to the \"Remote Desktop settings\" on your Windows 10 machine and ensure that remote connections are allowed.</p> </li> <li> <p>Find the IP Address: While the computer name is usually used for remote connections, you can also use the private IP address of the Windows machine. You can find this IP address by running <code>ipconfig</code> in the Command Prompt and copying the IPv4 address listed under \"Carte r\u00e9seau sans fil Wi-Fi &gt; Adresse IPv4\".</p> </li> <li> <p>Add Remote Desktop Account: Add the account (Windows session) you plan to use for remote access. While administrator accounts should work, it's recommended to use a specific user account for a smoother experience.</p> </li> </ol>"},{"location":"blog/2024/03/11/setting-up-remote-desktop-access-with-remmina-on-ubuntu/#step-2-installing-and-configuring-remmina-on-linux","title":"Step 2: Installing and Configuring Remmina on Linux","text":"<p>Remmina is an open-source remote desktop client for Linux systems, offering an intuitive interface and robust features.</p> <p>1. Install Remmina: Open a terminal on your Linux system and install Remmina using your package manager:</p> <pre><code>sudo apt install remmina\n</code></pre> <p>2. Create a New Connection: Launch Remmina and create a new connection profile. Enter the private IP address of your Windows 10 machine as the server address, and provide the username and password of the session you added in the previous step.</p> <p>3. Establish the Connection: Once you've entered the necessary information, click \"Connect\" to establish the remote desktop connection to your Windows 10 machine.</p>"},{"location":"blog/2024/03/11/setting-up-remote-desktop-access-with-remmina-on-ubuntu/#step-3-testing-remote-desktop-access","title":"Step 3: Testing Remote Desktop Access","text":"<p>Now that you've set up the connection, it's time to test remote desktop access.</p> <ol> <li> <p>Within the Same Network: Connect to your Windows 10 machine from your Linux system while both devices are on the same network. This allows you to ensure that everything is set up correctly.</p> </li> <li> <p>Outside the Network (Optional): If you encounter issues connecting from outside the network, it may be due to network restrictions or firewall settings. In such cases, you may need to contact your network administrator to allow remote desktop connections from external locations. Alternatively, consider using a virtual private network (VPN) to establish a secure connection to your network and access the Windows 10 machine remotely.</p> </li> </ol> <p>By testing remote desktop access within and potentially outside the network, you can verify the functionality of your setup and troubleshoot any connectivity issues effectively.</p> <p>By following these steps, you can enjoy seamless remote desktop access from your Linux system to a Windows 10 machine, enhancing your productivity and flexibility.</p>"},{"location":"blog/2024/03/11/setting-up-remote-desktop-access-with-remmina-on-ubuntu/#conclusion","title":"Conclusion","text":"<p>Setting up remote desktop access from a Linux system to a Windows 10 machine using Remmina offers convenience and flexibility, allowing users to access their computers remotely from anywhere. While the process involves a few initial setup steps and potential network considerations, the ability to connect seamlessly enhances productivity and enables efficient remote work. By following the steps outlined in this guide and troubleshooting any connectivity issues, users can enjoy the benefits of remote desktop access with ease.</p>"},{"location":"blog/2023/12/04/understanding-git-pull-vs-merge-in-git-workflow/","title":"Understanding Git Pull vs Merge in Git Workflow","text":""},{"location":"blog/2023/12/04/understanding-git-pull-vs-merge-in-git-workflow/#introduction","title":"Introduction","text":"<p>Did you know <code>git pull</code> and <code>git merge</code> are quite similar commands ?</p> <p>When it comes to managing branches in Git, understanding the nuances between <code>git pull</code> and <code>git merge</code> can significantly impact your workflow's efficiency.</p> <p>Both commands, <code>git pull</code> and <code>git merge</code>, serve the purpose of integrating changes from a remote branch (<code>dev</code>) into your local branch. However, they employ different strategies to achieve this.</p> <p>In this exploration, we'll delve into the differences between <code>git pull origin dev</code> and <code>git merge origin/dev</code>, unraveling their distinct approaches and highlighting the practical implications of their usage. Understanding these differences will empower you to make informed decisions while managing your Git branches effectively.</p> <p>Let's dive into the nuances of <code>git pull</code> and <code>git merge</code> to optimize your Git workflow and ensure seamless collaboration across teams.</p>"},{"location":"blog/2023/12/04/understanding-git-pull-vs-merge-in-git-workflow/#git-pull-vs-merge","title":"Git Pull vs Merge","text":""},{"location":"blog/2023/12/04/understanding-git-pull-vs-merge-in-git-workflow/#git-checkout-master-git-merge-dev","title":"<code>git checkout master &amp;&amp; git merge dev</code>","text":"<ol> <li><code>git checkout master</code>: Switches to the local <code>master</code> branch.</li> <li><code>git merge dev</code>: Attempts to merge the local <code>dev</code> into the local <code>master</code>.</li> </ol>"},{"location":"blog/2023/12/04/understanding-git-pull-vs-merge-in-git-workflow/#git-checkout-master-git-merge-origindev","title":"<code>git checkout master &amp;&amp; git merge origin/dev</code>","text":"<ol> <li><code>git checkout master</code>: Switches to the local <code>master</code> branch.</li> <li><code>git merge origin/dev</code>: Attempts to merge the remote <code>dev</code> into the local <code>master</code>.</li> </ol>"},{"location":"blog/2023/12/04/understanding-git-pull-vs-merge-in-git-workflow/#git-checkout-master-git-pull-origin-dev","title":"<code>git checkout master &amp;&amp; git pull origin dev</code>","text":"<ol> <li><code>git checkout master</code>: Switches to the local <code>master</code> branch.</li> <li><code>git pull origin dev</code>:<ul> <li>Fetches changes from the remote <code>dev</code> to the local <code>dev</code> like <code>git fetch origin dev</code>.</li> <li>Attempts to merge the remote <code>dev</code> into the local <code>master</code> like <code>git merge origin/dev</code>.</li> </ul> </li> </ol>"},{"location":"blog/2023/12/04/understanding-git-pull-vs-merge-in-git-workflow/#understanding-the-differences","title":"Understanding the Differences","text":"<p>Technically, <code>git pull origin dev</code> and <code>git merge origin/dev</code> both aim to integrate changes from a remote branch (<code>dev</code>) into your current local branch.</p> <p>However, they differ in approach:</p> <code>git pull origin dev</code> <code>git merge origin/dev</code> <ul> <li>Combines <code>git fetch</code> (retrieve changes from the remote repository) and <code>git merge</code> (integrate changes into your local branch) in one step.</li> <li>Fetches changes from the remote <code>dev</code> branch and immediately merges them into your current local branch.</li> </ul> <ul> <li>Directly attempts to merge changes from the remote <code>dev</code> branch into your current local branch without explicitly fetching changes separately.</li> <li>Assumes you already have the remote branch's changes available in your local repository.</li> </ul>"},{"location":"blog/2023/12/04/understanding-git-pull-vs-merge-in-git-workflow/#practical-considerations","title":"Practical Considerations","text":"<ul> <li><code>git pull</code> is often preferred for its convenience and safety in ensuring your local branch is up-to-date with the remote before merging.</li> <li><code>git merge</code> requires manually fetching changes beforehand.</li> <li>If unsure about the status of your local branch compared to the remote or if there might be new changes on the remote branch, <code>git pull origin dev</code> is a safer option.</li> <li>It fetches and merges changes in a single step, reducing chances of conflicts due to outdated local information.</li> </ul>"},{"location":"blog/2023/12/04/understanding-git-pull-vs-merge-in-git-workflow/#conclusion","title":"Conclusion","text":"<ul> <li><code>git pull</code> is essentially a <code>git fetch</code> followed by a <code>git merge</code> in one step, useful for updating your local branch with changes from a remote branch.</li> <li><code>git pull origin dev</code> is equivalent to <code>git fetch origin dev</code> + <code>git merge origin/dev</code>.</li> <li>Using <code>git pull</code> can be more concise and convenient, but separating actions (fetch and merge) provides explicit control over each step, allowing review of changes fetched from the remote branch before merging into the local branch.</li> </ul>"},{"location":"blog/2023/12/04/understanding-git-pull-vs-merge-in-git-workflow/#related-pages","title":"Related pages","text":"<ul> <li>Managing Local Modifications and Remote Changes in Git</li> <li>Mastering Git Merge Strategies: A Developer's Guide</li> <li>Nesting Repositories with Git Submodules: A Newbie's Guide</li> <li>Mastering Git Branch Handling: Strategies for Deletion and Recovery</li> </ul>"},{"location":"blog/2023/11/14/nesting-repositories-with-git-submodules-a-newbies-guide/","title":"Nesting Repositories with Git Submodules: A Newbie's Guide","text":""},{"location":"blog/2023/11/14/nesting-repositories-with-git-submodules-a-newbies-guide/#introduction","title":"Introduction","text":"<p>Are you facing the challenge of handling multiple code pieces scattered across different repositories in your project, unsure how to seamlessly integrate them?</p> <p>For developers new to the concept, managing disparate repositories within a single project can be overwhelming. Git submodules offer a guiding light, acting as a map through the maze of organizing and linking these separate codebases or libraries within your projects.</p>"},{"location":"blog/2023/11/14/nesting-repositories-with-git-submodules-a-newbies-guide/#real-life-scenario-aligning-frontend-and-backend-strategies","title":"Real-Life Scenario: Aligning Frontend and Backend Strategies","text":"<p>Back in 2022, I found myself as the lead developer overseeing the backend team, while collaborating closely with a talented frontend developer responsible for crafting engaging user interfaces.</p> <p>Our teams operated independently, each excelling in our specialized domains. However, this independence led to distinct branch strategies. The backend team adopted a unique approach, separate from the frontend's strategy.</p> <p>Over time, this divergence in branch strategies caused disparities between our repositories' states. Aligning frontend changes with the evolving backend structures became a complex task. Ensuring seamless integration between our frontend branches and specific backend versions posed challenges.</p> <p>Recognizing these challenges, I engaged in a discussion with the frontend developer. We brainstormed solutions to synchronize versions and segregate our branch strategies effectively.</p> <p>During our deliberation, we explored the idea of utilizing Git submodules. It wasn't merely about syncing versions but aligning our separate branch strategies while maintaining distinct team autonomy.</p> <p>The proposal envisioned Git submodules as the bridge between our frontend and backend repositories, facilitating version synchronization and accommodating separate yet aligned branch strategies. This approach aimed to streamline collaboration and ensure smoother integration between our teams' work.</p> <p>Motivated by the vision of enhanced collaboration and harmonized branch strategies, we collectively agreed to integrate Git submodules. This decision promised a more cohesive development environment, allowing both teams to synchronize versions and align branch strategies seamlessly.</p>"},{"location":"blog/2023/11/14/nesting-repositories-with-git-submodules-a-newbies-guide/#extending-to-a-computer-vision-project","title":"Extending to a Computer Vision Project","text":"<p>Additionally, in a computer vision project, I encountered a similar challenge. Testing code from various repositories required frequent modifications, causing inefficiencies. Managing these disparate codebases led me to prefer a unified repository managed with Git submodules. This approach enabled me to centralize and manage all required codebases efficiently, adapting them as needed.</p> <p>Think of Git submodules as containers that neatly organize and link external repositories to your main project\u2014providing a solution to the discomfort of handling disjointed pieces of code. Join us as we embark on a journey to explore how to clone, set up, and effortlessly synchronize these submodules within your projects.</p> <p></p> <p>This document simplifies Git submodules in a beginner-friendly way, offering developers new to the concept a clear path to effectively manage multiple repositories as cohesive parts of their projects.</p>"},{"location":"blog/2023/11/14/nesting-repositories-with-git-submodules-a-newbies-guide/#cloning-a-repository-with-submodules-and-cloning-a-specific-submodule","title":"Cloning a Repository with Submodules and Cloning a Specific Submodule","text":""},{"location":"blog/2023/11/14/nesting-repositories-with-git-submodules-a-newbies-guide/#clone-the-main-repository","title":"Clone the Main Repository","text":"<ol> <li> <p>Open your terminal and navigate to the desired directory for cloning:</p> <pre><code>cd /desired/directory/path\n</code></pre> </li> <li> <p>Clone the main repository:</p> <pre><code>git clone &lt;repository_url&gt;\n</code></pre> <p>Replace <code>&lt;repository_url&gt;</code> with the URL of the main repository.</p> </li> <li> <p>Change your working directory to the repository:</p> <pre><code>cd &lt;repository_directory&gt;\n</code></pre> <p>Replace <code>&lt;repository_directory&gt;</code> with the name of the cloned directory.</p> </li> </ol>"},{"location":"blog/2023/11/14/nesting-repositories-with-git-submodules-a-newbies-guide/#initialize-and-update-submodules","title":"Initialize and Update Submodules","text":"<ol> <li> <p>Initialize the submodules:</p> <pre><code>git submodule init\n</code></pre> <p>This sets up necessary Git configurations for submodules.</p> </li> <li> <p>Update the submodules:</p> <pre><code>git submodule update\n</code></pre> <p>This fetches submodule contents based on references in the main repository.</p> </li> </ol>"},{"location":"blog/2023/11/14/nesting-repositories-with-git-submodules-a-newbies-guide/#clone-a-specific-submodule","title":"Clone a Specific Submodule","text":"<ol> <li> <p>Clone a specific submodule:</p> <pre><code>git submodule update --recursive -- &lt;submodule_path&gt;\n</code></pre> <p>Replace <code>&lt;submodule_path&gt;</code> with the specific submodule path. This command updates only the specified submodule and its dependencies, leaving others unchanged. - The <code>--recursive</code> flag initializes nested submodules within the specified submodule.</p> </li> </ol> <p>Now that you've successfully cloned the main repository along with its submodules, let's explore how to create and manage submodules within an existing repository.</p> <p>If the update fails, you may want to read this stackoverflow thread</p>"},{"location":"blog/2023/11/14/nesting-repositories-with-git-submodules-a-newbies-guide/#creating-a-git-submodule","title":"Creating a Git Submodule","text":"<ol> <li> <p>Move to the parent repository's root directory:</p> <pre><code>cd /path/to/parent/repository\n</code></pre> </li> <li> <p>Add the Submodule:</p> <pre><code>git submodule add &lt;submodule_repository_url&gt; &lt;submodule_path&gt;\n</code></pre> <ul> <li><code>&lt;submodule_repository_url&gt;</code>: URL of the submodule repository.</li> <li><code>&lt;submodule_path&gt;</code>: Path within the parent repository to place the submodule.</li> </ul> <p>Example:</p> <pre><code>git submodule add https://github.com/example/submodule-repo.git path/to/submodule\n</code></pre> </li> <li> <p>Commit the Changes:</p> <pre><code>git commit -m \"Add submodule: &lt;submodule_path&gt;\"\n</code></pre> <p>Replace <code>&lt;submodule_path&gt;</code> with the actual path used when adding the submodule.</p> </li> <li> <p>Push Changes (Optional):</p> <pre><code>git push\n</code></pre> </li> </ol> <p>Now, let's delve into pulling changes from both the main repository and its submodules to keep your local copy up to date.</p>"},{"location":"blog/2023/11/14/nesting-repositories-with-git-submodules-a-newbies-guide/#pulling-changes-from-the-main-repository-and-submodules","title":"Pulling Changes from the Main Repository and Submodules","text":""},{"location":"blog/2023/11/14/nesting-repositories-with-git-submodules-a-newbies-guide/#update-the-main-repository","title":"Update the Main Repository","text":"<ol> <li> <p>Navigate to the main repository's directory:</p> <pre><code>cd /path/to/main/repository\n</code></pre> </li> <li> <p>Fetch the latest changes:</p> <pre><code>git pull origin main\n</code></pre> <p>This command fetches and merges the latest changes from the remote repository into your local <code>main</code> branch.</p> </li> </ol>"},{"location":"blog/2023/11/14/nesting-repositories-with-git-submodules-a-newbies-guide/#update-submodules","title":"Update Submodules","text":"<ol> <li> <p>Update submodules to the latest commits:</p> <pre><code>git submodule update --remote\n</code></pre> <p>This updates each submodule to the commit specified by the main repository.</p> </li> <li> <p>Update a specific submodule:</p> <ul> <li> <p>Using <code>git submodule update --remote &lt;submodule_path&gt;</code>:</p> <pre><code>git submodule update --remote path/to/submodule\n</code></pre> </li> <li> <p>Or manually in the submodule directory:</p> <pre><code>cd path/to/submodule\ngit pull origin master\n</code></pre> </li> </ul> </li> </ol>"},{"location":"blog/2023/11/14/nesting-repositories-with-git-submodules-a-newbies-guide/#pushing-updated-submodule-references-bonus","title":"Pushing Updated Submodule References (Bonus)","text":"<ol> <li> <p>Inside the main repository, after updating submodule references:</p> <pre><code>git commit -am \"Update submodule references\"\ngit push origin main\n</code></pre> </li> <li> <p>If there are changes in the submodules themselves:</p> <pre><code>cd path/to/submodule\ngit commit -am \"Update submodule\"\ngit push origin master\n</code></pre> </li> </ol>"},{"location":"blog/2023/11/14/nesting-repositories-with-git-submodules-a-newbies-guide/#removing-a-git-submodule","title":"Removing a Git Submodule","text":"<p>To remove a submodule from your repository, follow these steps. The following instructions are based on a detailed explanation found on Stack Overflow (source).</p> <ol> <li> <p>Move to the parent repository's root directory:</p> <p>Before removing the submodule, move it temporarily to a different location within your working directory.</p> <pre><code>mv &lt;submodule_path&gt; &lt;submodule_path&gt;_tmp\n</code></pre> </li> <li> <p>Deinitialize the Submodule:     Use the following command to deinitialize the submodule:</p> <pre><code>git submodule deinit -f -- &lt;submodule_path&gt;\n</code></pre> </li> <li> <p>Remove Submodule Configuration:     Delete the submodule's configuration from the <code>.git/modules</code> directory:</p> <pre><code>rm -rf .git/modules/&lt;submodule_path&gt;\n</code></pre> </li> <li> <p>Remove Submodule from Repository:     There are two options to remove the submodule from the repository:</p> <p>a. If you want to completely remove it from the repository and your working tree:</p> <pre><code>git rm -f &lt;submodule_path&gt;\n</code></pre> <p>Note: Replace <code>&lt;submodule_path&gt;</code> with the actual submodule path.</p> <p>b. If you want to keep it in your working tree but remove it from the repository:</p> <pre><code>git rm --cached &lt;submodule_path&gt;\n</code></pre> </li> <li> <p>Restore Submodule (Optional):     If you moved the submodule in step 0, restore it to its original location:</p> <pre><code>mv &lt;submodule_path&gt;_tmp &lt;submodule_path&gt;\n</code></pre> </li> </ol> <p>By following these steps, you'll effortlessly manage main repositories and their submodules, ensuring your projects are up to date.</p> <p>Stay tuned for more Git tips and tricks on our blog for seamless collaboration and version control!</p>"},{"location":"blog/2023/11/14/nesting-repositories-with-git-submodules-a-newbies-guide/#related-pages","title":"Related pages","text":"<ul> <li>Managing Local Modifications and Remote Changes in Git</li> <li>Mastering Git Merge Strategies: A Developer's Guide</li> <li>Understanding Git Pull vs Merge in Git Workflow</li> <li>Mastering Git Branch Handling: Strategies for Deletion and Recovery</li> </ul>"},{"location":"blog/2023/11/18/mastering-git-branch-handling-strategies-for-deletion-and-recovery/","title":"Mastering Git Branch Handling: Strategies for Deletion and Recovery","text":""},{"location":"blog/2023/11/18/mastering-git-branch-handling-strategies-for-deletion-and-recovery/#introduction","title":"Introduction","text":"<p>Are you looking to master the art of handling Git branches with finesse?</p> <p>Git branches are pivotal to managing project versions effectively. Understanding how to delete branches locally and remotely, as well as recovering deleted branches, is essential for maintaining a clean and organized repository. This guide serves as your compass, navigating you through the realm of Git branch management and ensuring a smooth and efficient version control process.</p>"},{"location":"blog/2023/11/18/mastering-git-branch-handling-strategies-for-deletion-and-recovery/#deleting-a-branch-locally-cli","title":"Deleting a Branch Locally (CLI)","text":"<p>Deleting a branch in Git locally can be done using the <code>git branch -d</code> command:</p> <pre><code>git branch -d &lt;branch-name&gt;\n</code></pre>"},{"location":"blog/2023/11/18/mastering-git-branch-handling-strategies-for-deletion-and-recovery/#deleting-a-branch-remotely-cli","title":"Deleting a Branch Remotely (CLI)","text":"<p>To delete a remote branch from your local repository and push that deletion to the remote repository (e.g., GitHub), use:</p> <pre><code>git push origin --delete &lt;branch-name&gt;\n</code></pre>"},{"location":"blog/2023/11/18/mastering-git-branch-handling-strategies-for-deletion-and-recovery/#deleting-a-branch-online-github-interface","title":"Deleting a Branch Online (GitHub Interface)","text":"<ul> <li>Go to the repository on GitHub.</li> <li>Click on the \"Branches\" tab.</li> <li>Locate the branch you want to delete.</li> <li>Click on the trash can icon or \"Delete\" button next to the branch name.</li> <li>Confirm the deletion if prompted.</li> </ul>"},{"location":"blog/2023/11/18/mastering-git-branch-handling-strategies-for-deletion-and-recovery/#fetching-and-cleaning-up-deletions","title":"Fetching and Cleaning Up Deletions","text":"<p>After deleting branches remotely, update your local repository to reflect these deletions:</p> <pre><code>git fetch --prune\n</code></pre> <p>This command fetches changes from the remote and prunes (removes) any remote-tracking references that no longer exist on the remote repository.</p>"},{"location":"blog/2023/11/18/mastering-git-branch-handling-strategies-for-deletion-and-recovery/#recovering-deleted-branches","title":"Recovering Deleted Branches","text":"<p>If a branch was mistakenly deleted and not yet pruned, it might be recoverable.</p> <ol> <li> <p>Check the Reflog:    Use <code>git reflog show</code> to view recently deleted branches and find the one you want to restore.</p> </li> <li> <p>Recover the Branch:    Identify the commit hash associated with the deleted branch in the reflog and create a new branch at that commit:</p> </li> </ol> <pre><code>git checkout -b &lt;branch-name&gt; &lt;commit-hash&gt;\n</code></pre> <p>Replace <code>&lt;branch-name&gt;</code> with the branch name and <code>&lt;commit-hash&gt;</code> with the commit hash from the reflog.</p> <p>Note: The ability to recover a deleted branch depends on recent activity and whether Git has pruned references.</p>"},{"location":"blog/2023/11/18/mastering-git-branch-handling-strategies-for-deletion-and-recovery/#related-pages","title":"Related pages","text":"<ul> <li>Managing Local Modifications and Remote Changes in Git</li> <li>Mastering Git Merge Strategies: A Developer's Guide</li> <li>Understanding Git Pull vs Merge in Git Workflow</li> <li>Nesting Repositories with Git Submodules: A Newbie's Guide</li> </ul>"},{"location":"blog/2024/01/04/simplifying-large-file-management-in-git-with-git-lfs/","title":"Simplifying Large File Management in Git with Git LFS","text":""},{"location":"blog/2024/01/04/simplifying-large-file-management-in-git-with-git-lfs/#introduction","title":"Introduction","text":""},{"location":"blog/2024/01/04/simplifying-large-file-management-in-git-with-git-lfs/#introduction_1","title":"Introduction","text":"<p>Have you ever faced the challenge of managing large files within a Git repository?</p> <p>Whether you're an experienced developer or just beginning your coding journey, dealing with large files in version control can be perplexing. Often, developers resort to <code>.gitignore</code> to exclude files, but what if there are essential large files crucial for your project's integrity?</p> <p>Enter Git LFS (Large File Storage), a solution designed to revolutionize how Git repositories handle large files. While some files are pivotal to track, keeping repositories lean and efficient remains a priority.</p> <p>This guide unlocks the potential of Git LFS, providing a step-by-step approach to seamlessly incorporate it into your version control workflow. Discover how Git LFS streamlines large file management, ensuring your repository stays clean and optimized.</p> <p>Let's navigate the realm of large file management in Git, ensuring your projects stay organized and efficient.</p>"},{"location":"blog/2024/01/04/simplifying-large-file-management-in-git-with-git-lfs/#steps-to-implement-git-lfs","title":"Steps to Implement Git LFS","text":""},{"location":"blog/2024/01/04/simplifying-large-file-management-in-git-with-git-lfs/#1-installing-git-lfs","title":"1. Installing Git LFS","text":"<p>Begin by installing Git LFS. Visit the Git LFS website for installation instructions tailored to your operating system.</p>"},{"location":"blog/2024/01/04/simplifying-large-file-management-in-git-with-git-lfs/#2-initializing-git-lfs","title":"2. Initializing Git LFS","text":"<p>In your repository, run the command:</p> <pre><code>git lfs install\n</code></pre> <p>This command sets up Git LFS for your project, preparing it to manage large files.</p>"},{"location":"blog/2024/01/04/simplifying-large-file-management-in-git-with-git-lfs/#3-tracking-large-files","title":"3. Tracking Large Files","text":"<p>Identify the large files you want to store using Git LFS and begin tracking them. You can manually specify these files using:</p> <pre><code>git lfs track \"path/to/your/large/file\"\n</code></pre> Example: Track files <code>.avi</code> and <code>.gif</code> files larger than 19MB <pre><code>For an efficient approach to track multiple files of specific extensions and sizes (such as `.avi` and `.gif` files larger than 19MB), a script can simplify the process. For exa:\n\n```bash\n#!/bin/bash\n\n# Find .avi files larger than 19MB and track them with Git LFS\nfind . -type f -name \"*.avi\" -size +19M | while read -r file; do\n    git lfs track \"$file\"\ndone\n\n# Find .gif files larger than 19MB and track them with Git LFS\nfind . -type f -name \"*.gif\" -size +19M | while read -r file; do\n    git lfs track \"$file\"\ndone\n```\n\nEnsure to execute this script within your Git repository directory.\n</code></pre> <p>Always review in <code>.gitattributes</code> the file selections to confirm they match your requirements before committing changes to Git LFS.</p>"},{"location":"blog/2024/01/04/simplifying-large-file-management-in-git-with-git-lfs/#4-updating-gitattributes","title":"4. Updating <code>.gitattributes</code>","text":"<p>After tracking the files, update your <code>.gitattributes</code> file with the tracking information:</p> <pre><code>git add .gitattributes\ngit commit -m \"Track large .avi and .gif files with Git LFS\"\n</code></pre>"},{"location":"blog/2024/01/04/simplifying-large-file-management-in-git-with-git-lfs/#5-commit-and-push-changes","title":"5. Commit and Push Changes","text":"<p>Following the usual Git workflow, add and commit your changes:</p> <pre><code>git add .\ngit commit -m \"Message\"\n</code></pre> <p>Finally, push the changes to your remote repository:</p> <pre><code>git push origin master\n</code></pre> <p>Assuming you're on the master branch, this step uploads the large files to the Git LFS server.</p> <p>By successfully implementing Git LFS, your large files are now efficiently managed within the repository, enhancing version control capabilities.</p> <p>For detailed installation instructions and additional information about Git LFS, refer to the Git LFS installation guide.</p>"},{"location":"blog/2023/11/04/managing-local-modifications-and-remote-changes-in-git/","title":"Managing Local Modifications and Remote Changes in Git","text":""},{"location":"blog/2023/11/04/managing-local-modifications-and-remote-changes-in-git/#introduction","title":"Introduction","text":"<p>Git is, without discussion, a powerful version control system that enables collaborative development.</p> <p>Ever found yourself in a twist trying to mix changes you made with updates from others in Git?</p> <p>It's like trying to blend your cooking style with someone else's recipe without making a mess. Git's awesome for team coding, but when your tweaks clash with online updates, how do you sort it out ?</p> <p>Indeed, when local modifications clash with remote changes, navigating these conflicts efficiently becomes crucial. Let's explore different strategies to handle this situation effectively.</p> <p></p> <p>by Johnson Huang &lt;https://github.com/jshuang0520/git&gt;</p>"},{"location":"blog/2023/11/04/managing-local-modifications-and-remote-changes-in-git/#options-for-handling-local-and-remote-changes-in-git","title":"Options for Handling Local and Remote Changes in Git","text":"<p>When you encounter local modifications and remote changes in your Git workflow, several options are available:</p>"},{"location":"blog/2023/11/04/managing-local-modifications-and-remote-changes-in-git/#1-incorporating-local-changes-with-remote-changes","title":"1. Incorporating Local Changes with Remote Changes","text":"<p>If no conflicts exist between your local changes and the remote changes, use:</p> <pre><code>git pull --rebase\n</code></pre> <ul> <li>This command integrates your commits after the remote commits, making it seem as if your changes were made after the remote changes.</li> <li>Manual resolution is required if conflicts arise during the rebase process.</li> </ul> <p>To apply rebase whenever you do a <code>git pull</code>, run this command to modify git default behavior</p> <pre><code>git config pull.rebase true\n</code></pre> <p>Example Scenario: Consider a scenario where ...</p> <p>Pros:</p> <ul> <li>Keeps a linear, clean commit history.</li> <li>Integrates local changes after remote ones, maintaining chronological order.</li> </ul> <p>Cons:</p> <ul> <li>Requires manual resolution of conflicts that may arise during rebase.</li> </ul>"},{"location":"blog/2023/11/04/managing-local-modifications-and-remote-changes-in-git/#2-preserving-local-changes-without-rebasing","title":"2. Preserving Local Changes without Rebasing","text":"<p>In this scenario:</p> <ul> <li>Git will consider the point where you started modifying without pulling as the base.</li> <li>There's no predetermined order or priority between your local commits and the remote commits.</li> <li>Git combines changes from your local repository and the remote commits into a single commit.</li> </ul> <p>You set the default pulling as no-rebase with</p> <pre><code>git config pull.rebase false\n</code></pre> <p>You pull with the no-rebase option with</p> <pre><code>git pull --no-rebase\n</code></pre> <p>Example Scenario: Suppose you've made substantial local changes and need to pull remote updates without modifying your commit history:</p> <pre><code>git pull --no-rebase origin main\n</code></pre> <p>Pros:</p> <ul> <li>Creates a single combined commit representing both local and remote changes.</li> <li>Retains the original commit structure without altering history.</li> </ul> <p>Cons:</p> <ul> <li>Might lose individual context from multiple local commits.</li> </ul>"},{"location":"blog/2023/11/04/managing-local-modifications-and-remote-changes-in-git/#3-pulling-with-strict-fast-forward-mode-git-config-pullff-only","title":"3. Pulling with Strict Fast-Forward Mode (<code>git config pull.ff only</code>)","text":"<p>In this scenario:</p> <ul> <li>Git checks for any absence of remote commits.</li> <li>If no remote commits exist, it adds your commits seamlessly.</li> <li>However, conflicts prompt resolution when remote commits are present.</li> </ul> <p>You set the default pulling as no-rebase with</p> <pre><code>git config pull.ff only\n</code></pre> <p>You pull with the fast-forward option with</p> <pre><code>git pull --ff-only\n</code></pre> <p>Example Scenario: When ensuring a linear history is a priority and avoiding merge commits:</p> <pre><code>git pull --ff-only origin main\n</code></pre> <p>Pros:</p> <ul> <li>Enforces a strictly linear history if possible, avoiding merge commits.</li> <li>Facilitates a cleaner commit timeline for easier tracking.</li> </ul> <p>Cons:</p> <ul> <li>Requires conflict resolution if the remote branch diverges.</li> </ul>"},{"location":"blog/2023/11/04/managing-local-modifications-and-remote-changes-in-git/#4-default-behavior-fast-forward-merge-git-config-pullff-merge","title":"4. Default Behavior: Fast-Forward Merge (<code>git config pull.ff merge</code>)","text":"<p>In this scenario (default behavior):</p> <ul> <li>Proceeds as usual when no remote commits are present.</li> <li>Performs a rebase based on the <code>git config pull.rebase</code> setting if remote commits exist.</li> </ul> <p>Example Scenario: Pulling changes with flexibility based on configured rebase settings:</p> <pre><code>git pull --ff-merge origin main\n</code></pre> <p>Pros:</p> <ul> <li>Offers flexibility based on configured rebase settings (<code>pull.rebase true/false</code>).</li> <li>Can accommodate both linear and non-linear commit histories.</li> </ul> <p>Cons:</p> <ul> <li>May result in a non-linear history with merge commits in certain scenarios.</li> </ul>"},{"location":"blog/2023/11/04/managing-local-modifications-and-remote-changes-in-git/#5-git-push-force","title":"5. <code>git push --force</code>","text":"<ul> <li>Enables forceful pushing, overriding any changes made by others.</li> <li>Caution is advised as it can lead to the loss of other developers' work.</li> <li>Coordination with the team is crucial for a smooth collaboration process.</li> </ul> <p>Example Scenario: When pushing changes forcefully becomes necessary:</p> <pre><code>git push --force origin feature-branch\n</code></pre> <p>Pros:</p> <ul> <li>Allows correcting mistakes or overriding changes when needed.</li> <li>Provides a quick resolution to divergent branch issues.</li> </ul> <p>Cons:</p> <ul> <li>Risks losing or overwriting others' work, disrupting collaboration.</li> <li>Requires careful coordination within the team.</li> </ul>"},{"location":"blog/2023/11/04/managing-local-modifications-and-remote-changes-in-git/#conclusion","title":"Conclusion","text":"<p>Choosing the right Git approach involves understanding the implications and trade-offs associated with each method. Experimenting with these options within the context of your team's workflow helps determine the most suitable approach for a smoother collaborative Git environment.</p> <p>Play it safe !</p> <p>In this guide, remember to play it safe: make backups as you work on different branches, decide if you're merging or rebasing changes like picking different tools for different jobs, and keep feature branches separate from the main code like organizing toys into different boxes. These simple tips will keep your code kitchen running smoothly!</p>"},{"location":"blog/2023/11/04/managing-local-modifications-and-remote-changes-in-git/#related-pages","title":"Related pages","text":"<ul> <li>Mastering Git Merge Strategies: A Developer's Guide</li> <li>Understanding Git Pull vs Merge in Git Workflow</li> <li>Nesting Repositories with Git Submodules: A Newbie's Guide</li> <li>Mastering Git Branch Handling: Strategies for Deletion and Recovery</li> </ul>"},{"location":"blog/2023/10/11/mastering-git-merge-strategies-a-developers-guide/","title":"Mastering Git Merge Strategies: A Developer's Guide","text":""},{"location":"blog/2023/10/11/mastering-git-merge-strategies-a-developers-guide/#introduction","title":"Introduction","text":"<p>Have you ever found yourself tangled in a web of Git branches, unsure of the best path to weave your changes together?</p> <p>The world of version control can be a maze, especially when deciding between Git's merge strategies. Fear not! This guide is your compass through the wilderness of rebases and merges, shedding light on the best routes to keep your repository history tidy and your sanity intact.</p> <p>Git offers two primary trails: the rebase, known for its clean and linear history, and the merge, preserving the unique storylines of each branch. Join us on this journey as we navigate the pros, cons, and conflict resolution techniques, empowering you to choose the right path for your project's narrative.</p> <p>So, this document provides guidance on using Git merge strategies, specifically focusing on the rebase and merge options.</p>"},{"location":"blog/2023/10/11/mastering-git-merge-strategies-a-developers-guide/#rebase-creating-a-linear-history","title":"Rebase: Creating a Linear History","text":"<pre><code>git checkout feature-branch\ngit pull --rebase origin main\n</code></pre> <p>or</p> <pre><code>git fetch origin main \ngit checkout feature-branch\ngit rebase origin main \n</code></pre> <code>Pros</code> <code>Cons</code> <ul> <li>Keeps a linear, clean commit history.</li> <li>Integrates local changes after remote ones, maintaining chronological order.</li> </ul> <ul> <li>Requires manual resolution of conflicts that may arise during rebase.</li> </ul>"},{"location":"blog/2023/10/11/mastering-git-merge-strategies-a-developers-guide/#handling-conflicts-during-rebase","title":"Handling Conflicts during Rebase","text":"<p>During the process of rebasing branches, conflicts might arise when applying commits from one branch onto another. Git requires manual resolution of conflicts that occur during a rebase operation.</p>"},{"location":"blog/2023/10/11/mastering-git-merge-strategies-a-developers-guide/#resolving-conflicts-manually","title":"Resolving Conflicts Manually","text":"<p>When conflicts occur during a rebase, Git halts the process and prompts you to resolve conflicts in the files where they arise. After resolving conflicts, you can continue the rebase using:</p> <pre><code>git rebase --continue\n</code></pre>"},{"location":"blog/2023/10/11/mastering-git-merge-strategies-a-developers-guide/#vscode-for-conflict-handling","title":"VSCode for Conflict Handling","text":"<p>Visual Studio Code (VSCode) offers a user-friendly interface to resolve conflicts during a rebase operation. Follow these steps within the VSCode environment:</p> <ol> <li> <p>Start the Rebase: Execute the rebase command in your terminal:</p> <pre><code>git rebase &lt;branch_name&gt;\n</code></pre> <p>This command initiates the rebase process.</p> </li> <li> <p>Conflict Indication: When conflicts occur, VSCode visually highlights them within the editor. You'll notice markers indicating the conflicted sections.</p> </li> <li> <p>Resolve Conflicts: Navigate to the conflicted file(s) in VSCode. Locate the sections marked as conflicted, displaying both versions of the conflicting changes.</p> </li> <li> <p>Choose Resolution: Review the changes and decide which version to keep or edit the content to create a resolution. Remove conflict markers (&lt;&lt;&lt;&lt;&lt;&lt;&lt;, =======, &gt;&gt;&gt;&gt;&gt;&gt;&gt;) once the conflict is resolved.</p> </li> <li> <p>Stage Changes: After resolving conflicts in each file, stage the changes using the Source Control panel in VSCode.</p> </li> <li> <p>Continue Rebase: Once conflicts are resolved and staged, return to your terminal and continue the rebase:</p> <pre><code>git rebase --continue\n</code></pre> <p>This command proceeds with the rebase process using the resolved changes.</p> </li> </ol> <p>VSCode streamlines the conflict resolution process by providing a visual and intuitive interface, making it easier to handle conflicts during a rebase operation.</p>"},{"location":"blog/2023/10/11/mastering-git-merge-strategies-a-developers-guide/#merge-preserving-branch-narratives","title":"Merge: Preserving Branch Narratives","text":"<p>Using <code>merge</code> in Git combines changes from different branches, preserving their individual commit histories. This method creates a new commit to capture the integration of changes from one branch into another.</p> <code>Pros</code> <code>Cons</code> <ul> <li>Preserves the complete history of changes made in each branch.</li> <li>Maintains a clear track record of when and where changes were integrated.</li> </ul> <ul> <li>May result in a non-linear history with multiple merge commit points.</li> <li>Can potentially clutter the commit history with merge commits.</li> </ul>"},{"location":"blog/2023/10/11/mastering-git-merge-strategies-a-developers-guide/#handling-conflicts","title":"Handling Conflicts","text":"<p>Similar to the rebase operation, merging branches in Git can lead to conflicts, especially when changes made in the same file or code sections conflict with each other. Git provides options to manage these conflicts during a merge operation.</p>"},{"location":"blog/2023/10/11/mastering-git-merge-strategies-a-developers-guide/#resolving-conflicts-by-favoring-a-specific-branch","title":"Resolving Conflicts by Favoring a Specific Branch","text":"<p>Suppose you're merging <code>branchA</code> into <code>branchB</code> and wish to favor the changes from <code>branchB</code> in case of conflicts:</p> <pre><code>git checkout branchB  # Switch to the target branch (branchB)\ngit merge -X ours branchA  # Merge branchA into branchB, favoring branchB changes in conflicts\n</code></pre> <p>Explanation:</p> <ol> <li> <p><code>git checkout branchB</code>: Switches to the target branch where changes will be merged (in this case, <code>branchB</code>).</p> </li> <li> <p><code>git merge -X ours branchA</code>: Merges <code>branchA</code> into <code>branchB</code>, and the <code>-X ours</code> option ensures conflicts are resolved by favoring changes from the current branch (<code>branchB</code>).</p> </li> </ol> <p>Upon executing this command, Git will merge the changes from <code>branchA</code> into <code>branchB</code>, automatically resolving conflicts by favoring the modifications present in <code>branchB</code>.</p>"},{"location":"blog/2023/10/11/mastering-git-merge-strategies-a-developers-guide/#other-merge-strategies","title":"Other Merge Strategies","text":"<p>Git provides various merge strategies such as <code>recursive</code>, <code>octopus</code>, and <code>resolve</code>, each with its own approach to handling merges. Choosing the right strategy depends on the project's requirements and the nature of changes between branches.</p>"},{"location":"blog/2023/10/11/mastering-git-merge-strategies-a-developers-guide/#conclusion","title":"Conclusion","text":"<p>As we conclude this journey through Git's merge strategies, remember the beauty lies in choice. Rebase crafts a linear tale, while merge celebrates branch narratives. The decision depends on your project's needs and the story you wish to tell.</p> <p>Experiment, explore, and harness the power of Git's merging artistry to sculpt your repository's history. Beyond rebase and merge, Git unveils a treasure trove of strategies, offering endless possibilities for your collaborative coding adventure.</p> <p>So, venture forth armed with this knowledge, shaping your repository's saga amidst the ever-evolving landscape of team collaboration.</p>"},{"location":"blog/2023/10/11/mastering-git-merge-strategies-a-developers-guide/#related-pages","title":"Related pages","text":"<ul> <li>Managing Local Modifications and Remote Changes in Git</li> <li>Understanding Git Pull vs Merge in Git Workflow</li> <li>Nesting Repositories with Git Submodules: A Newbie's Guide</li> <li>Mastering Git Branch Handling: Strategies for Deletion and Recovery</li> </ul>"},{"location":"blog/2023/11/13/guide-to-applying-query-on-you-mongodb-atlas-hosted-database-from-command-line/","title":"Guide to Applying query on you mongodb atlas hosted database from command line","text":""},{"location":"blog/2023/11/13/guide-to-applying-query-on-you-mongodb-atlas-hosted-database-from-command-line/#introduction","title":"Introduction","text":"<p>Often when you're using a database in a dev project, you want to access it quickly to check for modifications. When you're working with mysql database, you have a client that can help you with that. But what to do when you're using mongo db ? In this tutorial, i present how to access, from command line, your databased hosted with mongo db atlas. Then i showcase basic still important query examples. It should work also for those hosted locally.</p>"},{"location":"blog/2023/11/13/guide-to-applying-query-on-you-mongodb-atlas-hosted-database-from-command-line/#prerequistes","title":"Prerequistes","text":"<ul> <li>Node js</li> <li>a mongo db database, offline or locally served</li> </ul>"},{"location":"blog/2023/11/13/guide-to-applying-query-on-you-mongodb-atlas-hosted-database-from-command-line/#download-and-install-mongosh","title":"Download and Install Mongosh","text":"<p>You can download and install <code>mongosh</code> from the MongoDB website or using package managers like npm or yarn. Make sure you have Node.js installed on your system before proceeding.</p> <p>Personally, using windows, i've downloaded it (and installed the setup) from the website, put the bin file (containing mongosh.exe) into environment variables and read a bit of the docu. However, using ubuntu, i've tested both approch, installing using <code>apt-get</code> and using <code>nodejs</code>.</p> <p>Let's use the Package Manage option, as it is a more straightforward approach</p> <code>Using npm (Node Package Manager)</code> <code>Using yarn (Package Manager)</code> <pre><code>npx mongosh --version\n</code></pre> <pre><code>yarn dlx mongosh --version\n</code></pre> <p>This command will install mongosh it you doesn't have it.</p> <p>For more details on installing <code>mongosh</code>, refer to the MongoDB documentation <sup>1</sup>.</p> <p>I figured later on i can install mongodb database tools in a Dockerfile like proposed here</p>"},{"location":"blog/2023/11/13/guide-to-applying-query-on-you-mongodb-atlas-hosted-database-from-command-line/#access-a-dababase-from-cli","title":"Access a dababase from cli","text":"<p>I assume you have a MongoDB deployment to connect to. You can use a free cloud-hosted deployment like MongoDB Atlas or run a local MongoDB deployment.</p> <p>Connect to your MongoDB deployment using mongosh by running the command <sup>2</sup></p> <pre><code>mongosh \"mongodb+srv://&lt;username&gt;:&lt;password&gt;@&lt;cluster-address&gt;/&lt;database-name&gt;\"\n</code></pre> <p>Replace <code>&lt;username&gt;</code>, <code>&lt;password&gt;</code>, <code>&lt;cluster-address&gt;</code>, and <code>&lt;database-name&gt;</code> with your own values.</p> <p>You can find more information on how to install and use mongosh in the official MongoDB documentation <sup>1</sup> <sup>2</sup>.</p>"},{"location":"blog/2023/11/13/guide-to-applying-query-on-you-mongodb-atlas-hosted-database-from-command-line/#get-one-record-by-searching-an-attribute","title":"Get one record by searching an attribute","text":"<p>Get user by id</p> <pre><code>db.users.find({\"_id\":ObjectId(\"&lt;value&gt;\")})\n</code></pre> <p>Get user by telephone</p> <pre><code>db.users.find({telephone:\"&lt;telephone&gt;\"})\n</code></pre>"},{"location":"blog/2023/11/13/guide-to-applying-query-on-you-mongodb-atlas-hosted-database-from-command-line/#get-all-record-on-constraint","title":"Get all record on constraint","text":"<p>Get all restaurants</p> <pre><code>db.restaurants.find()\n</code></pre> <p>Get all users whose telephone contains 210</p> <pre><code>db.users.find({ telephone: { $regex: '210' }})\n</code></pre> <p>Get the n latest</p> <pre><code>db.purchases.find({}).sort({_id:-1}).limit(1)\n</code></pre> <p>Get the list of _id for all articles</p> <pre><code>db.articles.find({}, { _id: 1 }).toArray().map((doc) =&gt; doc._id)\n</code></pre> <p>Get users whose prenoms contain <code>abc</code>, case insensitive</p> <pre><code>db.users.find({ prenoms: { $regex: \"abc\", $options: 'i' } });\n</code></pre>"},{"location":"blog/2023/11/13/guide-to-applying-query-on-you-mongodb-atlas-hosted-database-from-command-line/#find-with-cross-tables-constraints","title":"Find with cross tables constraints","text":"<p>Get all restaurants without an owner</p> <pre><code>db.restaurants.aggregate([\n  { $lookup: { from: \"users\", localField: \"own_by\", foreignField:\"_id\", as: \"owner\" } },\n  { $match: { owner: { $size: 0 } /* Filter where \"owner\" array is empty, meaning no matching user found*/ } }\n]);\n</code></pre> <p>Get all users whose _id is present in the restaurants table and are of type \"RESTAU\"</p> <pre><code>db.users.aggregate([\n  { $lookup: { from: \"restaurants\", localField: \"_id\", foreignField: \"own_by\", as: \"restau\" }},\n  { $match: { type: \"RESTAU\" }}\n]);\n</code></pre> <p>Get users with at least one restaurant and apply a modification</p> <pre><code>db.users.aggregate([\n  { $lookup: { from: \"restaurants\", localField: \"_id\", foreignField: \"own_by\", as: \"restaurants\" } },\n  { $match: { restaurants: { $exists: true, $not: { $size: 0 } } /* Users with at least one restaurant*/ } },\n  { $set: { key: \"value\" } }\n]);\n</code></pre>"},{"location":"blog/2023/11/13/guide-to-applying-query-on-you-mongodb-atlas-hosted-database-from-command-line/#update-rows","title":"Update rows","text":"<p>Update the user with a specific _id</p> <pre><code>db.users.updateOne({\"_id\":ObjectId(\"&lt;value&gt;\")}, {$set : {\"nom\":\"new name\"}})\n</code></pre> <p>Update the user with a specific telephone</p> <pre><code>db.users.updateOne({telephone:\"&lt;value&gt;\"}, {$set: {\"notif_data.notif_token\":\"&lt;value&gt;\"}})\n</code></pre> <p>Update the password of all users with type=\"RESTAU</p> <pre><code>db.users.updateMany({\"type\":\"RESTAU\"}, {\"$set\":{\"password\":\"&lt;value&gt;\"}})\n</code></pre>"},{"location":"blog/2023/11/13/guide-to-applying-query-on-you-mongodb-atlas-hosted-database-from-command-line/#other-commands","title":"Other Commands","text":"<p>See all collections</p> <pre><code>show tables\n</code></pre> <p>See all databases</p> <pre><code>show dbs\n</code></pre> <p>Switch to the database <code>BiWag</code></p> <pre><code>use BiWag\n</code></pre> <p>See help</p> <pre><code>help\n</code></pre>"},{"location":"blog/2023/11/13/guide-to-applying-query-on-you-mongodb-atlas-hosted-database-from-command-line/#conclusion","title":"Conclusion","text":"<p>Note: This document contains example commands and use cases for the <code>mongosh</code> shell in MongoDB. Always be cautious when performing any updates or deletions on your database and ensure you have proper backups and permissions.</p> <ol> <li> <p>https://www.mongodb.com/docs/mongodb-shell/install/ \u21a9\u21a9</p> </li> <li> <p>https://www.mongodb.com/docs/mongodb-shell/connect/ \u21a9\u21a9</p> </li> </ol>"},{"location":"blog/2023/11/12/guide-to-installing-mysql-and-connecting-to-databases/","title":"Guide to Installing MySQL and Connecting to Databases","text":""},{"location":"blog/2023/11/12/guide-to-installing-mysql-and-connecting-to-databases/#introduction","title":"Introduction","text":"<p>MySQL is a popular relational database management system used for storing and managing data. To get started, you'll need to install MySQL, set it up, and then connect to databases. Here's a comprehensive guide to help you through the process.</p>"},{"location":"blog/2023/11/12/guide-to-installing-mysql-and-connecting-to-databases/#installation-process","title":"Installation Process","text":"<p>To install MySQL, follow these steps:</p> <ul> <li>update the package lists</li> </ul> <pre><code>sudo apt-get update\n</code></pre> <ul> <li>install MySQL</li> </ul> <pre><code>sudo apt-get install mysql-server\n</code></pre> <p>or use yum</p> <pre><code>yum install &lt;package-name&gt;\nor \nsudo yum install mysql-server\n</code></pre> <p>you will be prompted to set a password for the MySQL root user. Make sure to choose a strong password and remember it, as you will need it to access MySQL.</p> <ul> <li>if you haven't been prompted the password,</li> <li> <p>do this</p> <pre><code>sudo mysql_secure_installation\n</code></pre> </li> <li> <p>or connect later on using your ssh details (username, password): <code>mysql -u [username] -p</code></p> </li> <li> <p>start the MySQL service</p> </li> </ul> <pre><code>sudo service mysql start\nor\nsudo systemctl start mysqld\n</code></pre> <ul> <li>check if MySQL is running,</li> </ul> <pre><code>sudo service mysql status\n</code></pre> <p>If MySQL is running, you should see a message that says \"Active: active (running)\".</p>"},{"location":"blog/2023/11/12/guide-to-installing-mysql-and-connecting-to-databases/#testing-the-mysql-connection","title":"Testing the MySQL Connection","text":"<p>Here are a few commands you can use to test your MySQL connection:</p> <pre><code>mysql -u [username] -p -h [hostname] -P [port]\n</code></pre> <ul> <li> <p>Replace [username] with your database username, [hostname] with your database hostname, [port] with your database port number, and leave out the brackets.</p> </li> <li> <p>For example, if your database username is \"myuser\", your database hostname is \"db.example.com\", and your database port number is 3306, the command would look like this:</p> </li> </ul> <pre><code>mysql -u myuser -p -h db.example.com -P 3306\n</code></pre> <ul> <li> <p>Press Enter and then enter your database password when prompted. If the connection is successful, you'll see a prompt that looks like this:</p> </li> <li> <p>if that don't work, test that command with your ssh [username] and [password]</p> </li> </ul> <pre><code>mysql -u [username] -p\n</code></pre> <p>you will see this</p> <pre><code>mysql&gt;\n</code></pre> <p>This means you're now connected to your MySQL server.</p> <ul> <li>To test that you can retrieve data from your database, enter the following command:</li> </ul> <pre><code>use [databasename];\nselect * from [tablename];\n</code></pre> <ul> <li> <p>Replace [databasename] with the name of your database and [tablename] with the name of a table in your database. This will select all rows from the specified table.</p> </li> <li> <p>If the command returns data from your database, then your connection is working properly.</p> </li> </ul>"},{"location":"blog/2023/11/12/guide-to-installing-mysql-and-connecting-to-databases/#essential-actions","title":"Essential Actions","text":"<p>Here are some quick commands and actions you can perform in MySQL:</p> <ul> <li>show all the databases</li> </ul> <pre><code>SHOW DATABASES;\n</code></pre> <ul> <li>create a new database</li> </ul> <pre><code>CREATE DATABASE mydatabase;\n</code></pre> <ul> <li>use this database:</li> </ul> <pre><code>USE mydatabase;\n</code></pre> <ul> <li>create a new table:</li> </ul> <pre><code>CREATE TABLE mytable (id INT, name VARCHAR(20));\n</code></pre> <ul> <li>insert data into this table:</li> </ul> <pre><code>INSERT INTO mytable VALUES (1, 'John'), (2, 'Jane');\n</code></pre> <ul> <li>query the data:</li> </ul> <pre><code>SELECT * FROM mytable;\n</code></pre> <p>This will display the data you inserted into the table.</p> <ul> <li>To exit the MySQL shell:</li> </ul> <pre><code>exit\n</code></pre>"},{"location":"blog/2023/11/12/guide-to-installing-mysql-and-connecting-to-databases/#create-a-new-mysql-user-account","title":"Create a new MySQL user account","text":"<pre><code>sudo mysql -u root -p\n</code></pre> <pre><code>CREATE USER 'yourusername'@'localhost' IDENTIFIED BY 'yourpassword';\n\nGRANT ALL PRIVILEGES ON *.* TO 'yourusername'@'localhost' WITH GRANT OPTION;\n\nFLUSH PRIVILEGES;\n</code></pre> <p>Replace yourusername and yourpassword with the desired username and password for your MySQL user account.</p>"},{"location":"blog/2023/11/12/guide-to-installing-mysql-and-connecting-to-databases/#connecting-to-an-online-mysql-database","title":"Connecting to an Online MySQL Database","text":"<pre><code># Extracting details from the connection string\nusername=\"doadmin\"\npassword=\"AVNS_7wyTjplB7LVpwf3VKKf\"\nhostname=\"db-mysql-metalandapi-do-user-12655475-0.b.db.ondigitalocean.com\"\nport=\"25060\"\ndatabase_name=\"defaultdb\"\n\n# Constructing the MySQL CLI command\nmysql -u $username -p$password -h $hostname -P $port $database_name\n</code></pre>"},{"location":"blog/2023/11/12/guide-to-installing-mysql-and-connecting-to-databases/#deployment-and-integration","title":"Deployment and Integration","text":"<p>Configure your web application to utilize the MySQL database. Ensure your web server knows:</p> <ul> <li><code>Hostname</code> (usually localhost if on the same machine)</li> <li><code>Port</code> (default is 3306 for MySQL)</li> <li><code>Username</code> and <code>password</code> created earlier Database name you established</li> </ul> <p>Employ programming languages like PHP or Python to interact with the MySQL database. Use Nginx as a reverse proxy to direct requests to your application server.</p>"},{"location":"blog/2023/11/12/guide-to-installing-mysql-and-connecting-to-databases/#example-integrating-mysql-with-maven","title":"Example: Integrating MySQL with Maven","text":"<p>An example of integrating MySQL with a Maven project:</p> <ol> <li>Modify your Maven project to connect to the MySQL database. You can add the MySQL JDBC driver as a dependency in your project's <code>pom.xml</code> file, like this:</li> </ol> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;mysql&lt;/groupId&gt;\n    &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;\n    &lt;version&gt;8.0.25&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>Replace the version number with the latest version of the MySQL JDBC driver.</p> <ol> <li>Configure your application to use the MySQL database. You can add the necessary configuration properties to your application.properties file, like this:</li> </ol> <pre><code>spring.datasource.url=jdbc:mysql://localhost:3306/your_database_name\nspring.datasource.username=root\nspring.datasource.password=your_mysql_password\nspring.jpa.hibernate.ddl-auto=update\n</code></pre> <p>Replace your_database_name with the name of the database you created in step 2, and your_mysql_password with the password you set for the MySQL root user.</p> <ol> <li> <p>Build your Maven project and create an executable JAR file using the mvn package command.</p> </li> <li> <p>Start your application using the executable JAR file you created earlier. You can start it using the <code>java -jar &lt;jar-file-name&gt;</code> command.</p> </li> </ol> <p>Now your application should be up and running, connected to the MySQL database and loaded with the data from your SQL file. Nginx can then be used as a reverse proxy to serve your application to users.</p>"},{"location":"projects/","title":"My Projects","text":""},{"location":"projects/#data-science","title":"Data science","text":"<ul> <li>D\u00e9tection de bot dans dans les ench\u00e8res</li> <li>Search Engine for domain specific french users</li> <li>Streamlit App for Formula OCR using pix2tex</li> </ul>"},{"location":"projects/#web-development","title":"Web development","text":"<ul> <li>Flask based File Hosting (web app &amp; api &amp; python module &amp; cli app)</li> <li>Introducing Two New Packages for Streamlining File Conversions in Python</li> </ul>"},{"location":"projects/bot_detection_in_auction/","title":"D\u00e9tection de bot dans dans les ench\u00e8res","text":""},{"location":"projects/bot_detection_in_auction/#objectif-general","title":"Objectif g\u00e9n\u00e9ral","text":"<p>Notre objectif g\u00e9n\u00e9ral est d\\'\u00e9tudier un \u00ab\u00a0dataset\u00a0\u00bb issu d\\'une plateforme d\\'ench\u00e8res publicitaires pour pouvoir pr\u00e9dire si l'agent qui a \u00e9mis des ench\u00e8res est un humain ou un robot. A partir d'une analyse bien approfondie de diverses donn\u00e9es concernant les transactions effectu\u00e9es notamment les outils num\u00e9riques utilis\u00e9s, les temps de ces transactions et bien d'autres \u00ab\u00a0features\u00a0\u00bb, nous allons d\u00e9velopper un mod\u00e8le de classification capable de pr\u00e9dire la variable binaire \u00ab\u00a0outcome\u00a0\u00bb de telle sorte que 0 d\u00e9signe <code>humain</code> et 1, <code>robot</code>. En outre, nous visons \u00e0 travers ce projet \u00e0 minimiser le taux des faux n\u00e9gatifs (pr\u00e9dire que l'agent est un humain, alors qu'il est un robot) et donc augmenter comme m\u00e9trique le \u00ab\u00a0recall\u00a0\u00bb pour \u00e9viter toute sorte de fraude. Enfin, nous allons choisir au maximum 5 variables pour notre mod\u00e9lisation.</p>"},{"location":"projects/bot_detection_in_auction/#comprehension-des-donnees","title":"Compr\u00e9hension des donn\u00e9es","text":"<p>La base de donn\u00e9es fournie contient des informations sur les soumissionnaires de l'ench\u00e8re et sur l'ench\u00e8re. Les features donn\u00e9s sont expliqu\u00e9s ci-dessous\u00a0:</p> <p>Output</p> <pre><code>| **Feature**         | **Explication**                                                |\n|---------------------|---------------------------------------------------------------|\n| Bidder_id           | Un identifiant unique pour le soumissionnaire                  |\n| Bid_id              | Un identifiant unique de l'offre fait par le soumissionnaire   |\n| Auction             | Un identifiant unique de l'ench\u00e8re (l'offre publique)          |\n| Merchandise         | La cat\u00e9gorie du produit/offre                                  |\n| Device              | Mod\u00e8le de t\u00e9l\u00e9phone d'un visiteur                              |\n| Time                | Temps \u00e0 lequel la transaction a \u00e9t\u00e9 faite pour l'ench\u00e8re      |\n| Country             | Le pays auquel appartient l'IP                                 |\n| IP                  | Adresse IP de l'enrichisseur                                   |\n| Url                 | Le site \u00e0 partir duquel l'enrichisseur a \u00e9t\u00e9 r\u00e9f\u00e9r\u00e9            |\n| Payment_account     | Le compte \u00e0 partir duquel l'enrichisseur a pay\u00e9                |\n| Address             | L'adresse de l'enrichisseur                                    |\n| Outcome             | 1 si robot, 0 si homme                                         |\n</code></pre>"},{"location":"projects/bot_detection_in_auction/#analyse-descriptive-et-selection-de-variables","title":"Analyse descriptive et s\u00e9lection de variables","text":""},{"location":"projects/bot_detection_in_auction/#some-constants","title":"some constants","text":"<pre><code>TARGET_COL = \"outcome\"\nREMOVE_EVIDENT_MERCHANDISE = False\nFILE_VERSION = \"v7\"\nPROD_INSTEAD_OF_SUM =  True\nADD_LEN_TO_GROUPBY = True\n#prod+nolen &lt; prod+len\n</code></pre>"},{"location":"projects/bot_detection_in_auction/#load-data","title":"Load data","text":"<pre><code>df = pd.read_csv(\"Projet_ML.csv\")\n</code></pre> <pre><code>df.bidder_id.nunique()\n</code></pre> <pre><code>87\n</code></pre>"},{"location":"projects/bot_detection_in_auction/#preview","title":"Preview","text":"<p>Voici un aper\u00e7u de la dataset\u00a0:</p> <pre><code>df\n</code></pre> Output <pre><code>| bidder_id                              | bid_id  | auction | merchandise       | device    | time          | country | ip              | url            | payment_account                      | address                             | outcome |\n|----------------------------------------|---------|---------|-------------------|-----------|---------------|---------|-----------------|----------------|--------------------------------------|-------------------------------------|---------|\n| 001068c415025a009fee375a12cff4fcnht8y | 7179832 | 4ifac   | jewelry           | phone561  | 5.140996e-308 | bn      | 139.226.147.115 | vasstdc27m7nks3 | a3d2de7675556553a5f08e4c88d2c228iiasc | a3d2de7675556553a5f08e4c88d2c2282aj35 | 0       |\n| 0030a2dd87ad2733e0873062e4f83954mkj86 | 6805028 | obbny   | mobile            | phone313  | 5.139226e-308 | ir      | 21.67.17.162    | vnw40k8zzokijsv | a3d2de7675556553a5f08e4c88d2c228jem8t | f3bc67b04b43c3cebd1db5ed4941874c9br67 | 0       |\n| 00a0517965f18610417ee784a05f494d4dw6e | 2501797 | l3o6q   | books and music   | phone451  | 5.067829e-308 | bh      | 103.165.41.136  | kk7rxe25ehseyci | 52743ba515e9c1279ac76e19f00c0b001p3pm | 7578f951008bd0b64528bf81b8578d5djy0uy | 0       |\n| 00a0517965f18610417ee784a05f494d4dw6e | 2724778 | du967   | books and music   | phone117  | 5.068704e-308 | tr      | 239.250.228.152 | iu2iu3k137vakme | 52743ba515e9c1279ac76e19f00c0b001p3pm | 7578f951008bd0b64528bf81b8578d5djy0uy | 0       |\n| 00a0517965f18610417ee784a05f494d4dw6e | 2742648 | wx3kf   | books and music   | phone16   | 5.068805e-308 | in      | 255.108.248.101 | u85yj2e7owkz6xp | 52743ba515e9c1279ac76e19f00c0b001p3pm | 7578f951008bd0b64528bf81b8578d5djy0uy | 0       |\n| ...                                    | ...     | ...     | ...               | ...       | ...           | ...     | ...             | ...            | ...                                  | ...                                 | ...     |\n| 0ad17aa9111f657d71cd3005599afc24fd44y | 1411172 | toxfq   | mobile            | phone1036 | 5.201503e-308 | in      | 186.94.48.203   | vasstdc27m7nks3 | 22cdb26663f071c00de61cc2dcde7b556rido | db147bf6056d00428b1bbf250c6e97594ewjy | 1       |\n| 0ad17aa9111f657d71cd3005599afc24fd44y | 1411587 | ucb4u   | mobile            | phone127  | 5.201506e-308 | in      | 119.27.26.126   | vasstdc27m7nks3 | 22cdb26663f071c00de61cc2dcde7b556rido | db147bf6056d00428b1bbf250c6e97594ewjy | 1       |\n| 0ad17aa9111f657d71cd3005599afc24fd44y | 1411727 | sg8yd   | mobile            | phone383  | 5.201507e-308 | in      | 243.25.54.63    | yweo7wfejrgbi2d | 22cdb26663f071c00de61cc2dcde7b556rido | db147bf6056d00428b1bbf250c6e97594ewjy | 1       |\n| 0ad17aa9111f657d71cd3005599afc24fd44y | 1411877 | toaj7   | mobile            | phone26   | 5.201508e-308 | in      | 17.66.120.232   | 4dd8ei0o5oqsua3 | 22cdb26663f071c00de61cc2dcde7b556rido | db147bf6056d00428b1bbf250c6e97594ewjy | 1       |\n| 0ad17aa9111f657d71cd3005599afc24fd44y | 1412085 | 07axb   | mobile            | phone25   | 5.201509e-308 | in      | 64.30.57.156    | 8zdkeqk4yby6lz2 | 22cdb26663f071c00de61cc2dcde7b556rido | db147bf6056d00428b1bbf250c6e97594ewjy | 1       |\n</code></pre>"},{"location":"projects/bot_detection_in_auction/#variable-a-predire","title":"variable \u00e0 pr\u00e9dire","text":"<pre><code>df[TARGET_COL].value_counts()\n</code></pre> <p>0    90877 1     9123 Name: outcome, dtype: int64</p> <p>c'est une classification binaire</p>"},{"location":"projects/bot_detection_in_auction/#description-des-champs-numeriques","title":"Description des champs num\u00e9riques","text":"<pre><code>df.info()\n</code></pre> <p>Output</p> <pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 100000 entries, 0 to 99999\nData columns (total 12 columns):\n\n## Column           Non-Null Count   Dtype  \n\n---  ------           --------------   -----  \n0   bidder_id        100000 non-null  object\n1   bid_id           100000 non-null  int64  \n2   auction          100000 non-null  object\n3   merchandise      100000 non-null  object\n4   device           100000 non-null  object\n5   time             100000 non-null  float64\n6   country          99816 non-null   object\n7   ip               100000 non-null  object\n8   url              100000 non-null  object\n9   payment_account  100000 non-null  object\n10  address          100000 non-null  object\n11  outcome          100000 non-null  int64  \ndtypes: float64(1), int64(2), object(9)\nmemory usage: 9.2+ MB\n</code></pre> <p>On a pr\u00e8s de 200 cellules vides dans country. on verra \u00e7a apr\u00e8s</p>"},{"location":"projects/bot_detection_in_auction/#types-et-autres-informations","title":"Types et autres informations","text":"<p>Le dataset contient 12 colonnes et 100.000 lignes.</p>"},{"location":"projects/bot_detection_in_auction/#description","title":"Description","text":"<p>L'image ci-dessous pr\u00e9sente plusieurs caract\u00e9ristiques de chaque champ de la table</p> <ul> <li> <p>dtype\u00a0: type de la variable (int64 pour les entiers, float64 pour les nombres, object pour les champs textes ou non-identifi\u00e9s)</p> </li> <li> <p>nunique\u00a0: nombre de valeurs uniques que prends cette variable</p> </li> <li> <p>nunique(%)\u00a0: proportion des valeurs uniques que prends cette variable par rapport au nombre de lignes dans la table</p> </li> <li> <p>nunique_per_bid&gt;1(%)\u00a0: nombre de \u00ab\u00a0bidder_id\u00a0\u00bb qui pr\u00e9sentes plusieurs valeurs diff\u00e9rentes pour cette variable.</p> </li> <li> <p>is_cat: 1 si la variable peut \u00eatre consid\u00e9r\u00e9e categorielle (ici, moins de 10 valeurs uniques) 0 sinon</p> </li> </ul> get_cols_info <pre><code>from math import ceil\n\ndef get_cols_info(df, index_col=None):\n  print(\"&gt;&gt;&gt; df.shape= \", df.shape)\n  print(\"\\n&gt;&gt;&gt; df.info= \")\n  df.info()\n  dd = {\"col\":[],\"dtype\":[],\"nunique\":[],\"nunique(%)\":[],\"nunique_per_bid&gt;1(%)\":[],\"is_cat\":[]}\n  for elt in df.columns:\n  dd[\"col\"].append(elt)\n  dd[\"nunique\"].append(df[elt].nunique())\n  dd[\"nunique(%)\"].append(0.1*ceil(10*100*df[elt].nunique()/len(df)))\n  dd[\"dtype\"].append(df[elt].dtype)\n  dd[\"is_cat\"].append(int(df[elt].nunique()&lt;10))\n  if index_col: dd[\"nunique_per_bid&gt;1(%)\"].append(0.1*ceil(10*100*(df.groupby(index_col)[elt].nunique()&gt;1).sum()/df[index_col].nunique()))\n  else: dd[\"nunique_per_bid&gt;1(%)\"].append('')\n  list_indx = dd[\"col\"]\n  del dd[\"col\"]\n  print(\"\\n&gt;&gt;&gt; df.more_info= \")\n  print(pd.DataFrame(dd, index=list_indx).sort_values(by=['nunique']))\n  print(\"\\n&gt;&gt;&gt; df.describe= \")\n  print(df.describe())\n\nget_cols_info(df, \"bidder_id\")\n</code></pre> Output <pre><code>  &gt;&gt;&gt; df.shape=  (100000, 12)\n\n  &gt;&gt;&gt; df.info= \n  &lt;class 'pandas.core.frame.DataFrame'&gt;\n  RangeIndex: 100000 entries, 0 to 99999\n  Data columns (total 12 columns):\n  ##   Column           Non-Null Count   Dtype  \n  ---  ------           --------------   -----  \n  0   bidder_id        100000 non-null  object \n  1   bid_id           100000 non-null  int64  \n  2   auction          100000 non-null  object \n  3   merchandise      100000 non-null  object \n  4   device           100000 non-null  object \n  5   time             100000 non-null  float64\n  6   country          100000 non-null  object \n  7   ip               100000 non-null  object \n  8   url              100000 non-null  object \n  9   payment_account  100000 non-null  object \n  10  address          100000 non-null  object \n  11  outcome          100000 non-null  int64  \n  dtypes: float64(1), int64(2), object(9)\n  memory usage: 9.2+ MB\n\n  &gt;&gt;&gt; df.more_info= \n  dtype  nunique  nunique(%)  nunique_per_bid&gt;1(%)  is_cat\n  outcome            int64        2         0.1                   0.0       1\n  merchandise       object        6         0.1                   0.0       1\n  bidder_id         object       87         0.1                   0.0       0\n  payment_account   object       87         0.1                   0.0       0\n  address           object       87         0.1                   0.0       0\n  country           object      175         0.2                  70.2       0\n  device            object     1871         1.9                  80.5       0\n  auction           object     3438         3.5                  79.4       0\n  url               object    21951        22.0                  72.5       0\n  ip                object    35083        35.1                  84.0       0\n  time             float64    92385        92.4                  85.1       0\n  bid_id             int64   100000       100.0                  85.1       0\n\n  &gt;&gt;&gt; df.describe= \n  bid_id           time        outcome\n  count  1.000000e+05  100000.000000  100000.000000\n  mean   3.697622e+06       0.543571       0.091230\n  std    2.380217e+06       0.362483       0.287937\n  min    8.900000e+01       0.000000       0.000000\n  25%    1.463762e+06       0.085364       0.000000\n  50%    3.660968e+06       0.514295       0.000000\n  75%    5.881387e+06       0.935483       0.000000\n  max    7.656326e+06       1.000000       1.000000\n</code></pre> <p>on remarque le bid_id est un identifiant unique pour chaque bidder_id qui repr\u00e9sente l'action de l'offre men\u00e9e par chaque bidder</p> <p>on constate que time est n'est pas \u00e0 100% unique donc on peut avoir deux actions d'ench\u00e8re qui sont r\u00e9alis\u00e9 en m\u00eame instant</p>"},{"location":"projects/bot_detection_in_auction/#etude-de-la-variable-predictive","title":"Etude de la variable pr\u00e9dictive","text":"<pre><code>tg = \"outcome\" #(\"outcome\", \"&lt;lambda&gt;\")\nsss[tg] = sss[tg].values.astype(int)\nax = sns.histplot(x=tg, data=sss)\nadd_labels_to_histplot(ax, title=\"distribution of outcome\")\n</code></pre> <p>On remarque que sur 87 bidder_id, seuls 6 correspondent \u00e0 des robots. Ainsi, il s'agit d'un probl\u00e8me de classification binaire d\u00e9s\u00e9quilibr\u00e9.</p>"},{"location":"projects/bot_detection_in_auction/#selection-des-variables","title":"S\u00e9lection des variables","text":"<p>Les variables qui prennent plusieurs valeurs pour un m\u00eame joueur pourraient \u00eatre utilis\u00e9s afin d'\u00e9tudier la vari\u00e9t\u00e9 des outils utilis\u00e9s par le joueur. C'est le cas des variables (country, device, auction, url, ip). La variable bid_id est un identifiant unique de transaction (nunique=100%) et ne sera pas consid\u00e9r\u00e9.</p> <p>Les variables cat\u00e9gorielles, si pertinentes seront int\u00e9gr\u00e9es. C'est le cas de la variable (merchandise)</p> <p>Les variables avec un \u00ab\u00a0nunique_per_bid\u00a0\u00bb prenant la valeur nulle, n'apportent pas d'informations et seront rejet\u00e9es s'il n'y a pas d'extraction d'informations possibles. C'est le cas pour les variables (payment_account et address)</p> Variable Pertinence Descriptions \u00c0 Rejeter Outcome (status du joueur) - Il s'agit de la variable \u00e0 pr\u00e9dire Variable cat\u00e9gorielle (0 ou 1). Chaque joueur a un seul status. Cette information est v\u00e9rifi\u00e9e sur toute la table. NON Merchandise (type de produit achet\u00e9s) - Les robots ou les humains pourraient avoir des tendances vers des produits particuliers. Variable cat\u00e9gorielle (6 cat\u00e9gories) NON bidder_id (identifiant du joueur) - Il s'agit de l'object de la pr\u00e9diction. Identifiant. Mais il n'apporte pas d'informations suppl\u00e9mentaires. NON PAYMENT_ACCOUNT ET ADRESSE - Ces variables n'apportent pas d'informations suppl\u00e9mentaires (unique_per_bid = 0). Identifiants sans possibilit\u00e9 d'extraction d'informations. OUI COUNTRY - Les joueurs de certains pays pourraient avoir plus tendance \u00e0 utiliser ou non des robots. Variable texte (identifiant du pays). NON device (Appareil utilis\u00e9), AUCTION, URL, IP - Les robots ou humains pourraient avoir tendance \u00e0 utiliser certains appareils (pc vs mobile vs web ...) Variable texte (nom du device). NON BID_ID - Le bid_id est unique sur les lignes et ne contient pas de features \u00e0 extraire. Identifiant ne contenant pas de features \u00e0 extraire. OUI"},{"location":"projects/bot_detection_in_auction/#valeurs-nulles","title":"Valeurs nulles","text":"<p>lignes vides par colonne</p> <pre><code>df.isnull().sum()\n</code></pre> <p>bidder_id            0 bid_id               0 auction              0 merchandise          0 device               0 time                 0 country            184 ip                   0 url                  0 payment_account      0 address              0 outcome              0 dtype: int64</p> <p>Il n'y a de cellules vides que dans country</p> <p>Valeurs dupliqu\u00e9es\u00a0?</p> <p></p> <p>Il n'y a pas de lignes dupliqu\u00e9es dans la table. Mais si on enl\u00e8ve le champs bid_id, il y a deux lignes dupliqu\u00e9es. On supprime ces deux lignes puisque ce sont des transactions qui se r\u00e9p\u00e8tent.</p>"},{"location":"projects/bot_detection_in_auction/#feature-engineering-visualisation-etou-test-des-hypotheses","title":"Feature Engineering, Visualisation et/ou Test des Hypoth\u00e8ses","text":"<p>Afin d'en savoir plus sur le pouvoir de s\u00e9parabilit\u00e9 des variables par rapport \u00e0 la variable \u00ab\u00a0outcome\u00a0\u00bb, nous allons effectuer une s\u00e9rie d'analyses et de visualisations.</p> <p>Par ailleurs, nous ferons une agr\u00e9gation des donn\u00e9es selon le \u00ab\u00a0bidder_id\u00a0\u00bb qui identifie la nature (robot ou humain) des joueurs.</p>"},{"location":"projects/bot_detection_in_auction/#librairies-utiles","title":"Librairies utiles","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport scipy.stats as stats\nfrom statsmodels.api import Logit\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n</code></pre>"},{"location":"projects/bot_detection_in_auction/#null-values-country","title":"Null values (country)","text":"<p>Seule la variable country contient des valeurs nulles. Nous allons remplacer toutes les valeurs nulles par une constante nomm\u00e9e \u00ab\u00a0NO_COUNTRY\u00a0\u00bb</p> <pre><code>## proportion des valeurs nulles\ndf.country.isnull().sum()/len(df)\n</code></pre> <p>0.00184</p>"},{"location":"projects/bot_detection_in_auction/#transformation-ip","title":"Transformation (ip)","text":"<pre><code>df.ip\n</code></pre> <pre><code>0        139.226.147.115\n1           21.67.17.162\n2         103.165.41.136\n3        239.250.228.152\n4        255.108.248.101\n              ...       \n99995      186.94.48.203\n99996      119.27.26.126\n99997       243.25.54.63\n99998      17.66.120.232\n99999       64.30.57.156\nName: ip, Length: 100000, dtype: object\n</code></pre> <pre><code>## identify network instead of device\ndf[\"ip\"] = df.ip.apply(lambda x: '.'.join(x.split('.')[:2]))\n</code></pre> <pre><code>df.ip\n</code></pre> <pre><code>0        139.226\n1          21.67\n2        103.165\n3        239.250\n4        255.108\n          ...   \n99995     186.94\n99996     119.27\n99997     243.25\n99998      17.66\n99999      64.30\nName: ip, Length: 100000, dtype: object\n</code></pre> <pre><code>df.groupby(\"ip\").agg({\"country\": lambda x: x.nunique()}).country.value_counts()\n</code></pre> <pre><code>1     25426\n2      7918\n3      1469\n4       205\n5        44\n6         9\n8         4\n7         3\n10        2\n9         2\n13        1\nName: country, dtype: int64\n</code></pre>"},{"location":"projects/bot_detection_in_auction/#normalisation-time","title":"Normalisation (time)","text":"<p>La variable \u00ab\u00a0time\u00a0\u00bb prends des valeurs tr\u00e8s petites (de l'ordre de 10**(-308)). On appliquera une normalisation lin\u00e9aire et qui donc ne r\u00e9duit pas l'information de cette variable.</p> <pre><code>df.time.describe()\n</code></pre> <pre><code>count     1.000000e+05\nmean     5.143168e-308\nstd       0.000000e+00\nmin      5.067452e-308\n25%      5.079343e-308\n50%      5.139090e-308\n75%      5.197759e-308\nmax      5.206746e-308\nName: time, dtype: float64\n</code></pre> <pre><code>## normalize time\ndf[\"time\"] = (df.time - df.time.min())/(df.time.max() - df.time.min())\n</code></pre> <pre><code>df[\"time\"].describe()\n</code></pre> <pre><code>count    100000.000000\nmean          0.543571\nstd           0.362483\nmin           0.000000\n25%           0.085364\n50%           0.514295\n75%           0.935483\nmax           1.000000\nName: time, dtype: float64\n</code></pre>"},{"location":"projects/bot_detection_in_auction/#visualisation-merchandise","title":"Visualisation (Merchandise)","text":"<pre><code>## histogrammes des produits\nax = sns.histplot(x=\"merchandise\", data=df, color='b')\nadd_labels_to_histplot(ax, title=\"Distribution of merchandises\")\n</code></pre> <pre><code>## histogramme des produits par outcome\nax = sns.histplot(x=\"merchandise\", data=df, hue=\"outcome\", palette=[\"b\",\"orange\"])\nadd_labels_to_histplot(ax, title=\"Distribution of merchandises by outcome\")\n</code></pre> <pre><code>## liste des outcome par produit\ndef human(x): return (x==0).sum()\ndef bot(x): return (x==1).sum()\ndf.groupby(\"merchandise\").agg({\"outcome\": [human, bot] })\n</code></pre> <p>|            |     outcome     |        | |            |  human     | bot| |------------|-----------------|--------| | merchandise|                 |        | | books and music|   227         | 0      | | home goods     |  13002        | 2093   | | jewelry        |  25004        | 0      | | mobile         |  26228        | 6853   | | office equipment| 21          | 0      | | sporting goods  | 26395        | 177    |</p>"},{"location":"projects/bot_detection_in_auction/#encoding-merchandise","title":"Encoding (Merchandise)","text":"<pre><code>df.merchandise.unique()\n</code></pre> <pre><code>array(['jewelry', 'mobile', 'books and music', 'office equipment',\n       'sporting goods', 'home goods'], dtype=object)\n</code></pre> <pre><code>REMOVE_EVIDENT_MERCHANDISE = False\n</code></pre> <pre><code>if REMOVE_EVIDENT_MERCHANDISE : df = df[df.merchandise.apply(lambda x: x in [\"mobile\", \"sporting goods\", \"home goods\"])]\nelse: df[\"non_robot_merchandise\"] = df.merchandise.apply(lambda x: int(x in ['jewelry', 'books and music', 'office equipment']))\n</code></pre> <pre><code>def one_hot_encoder_v2(data, col_name):\n  data = data.copy()\n  new_cols = []\n  for i, elt in enumerate(data[col_name].unique()):\n    new_cols.append(f\"{col_name}_{i+1}\")\n    data[new_cols[-1]] = data[col_name].apply(lambda x: int(x==elt))\n  del data[col_name]\n  return data, new_cols\n\ndf2, new_cols = one_hot_encoder_v2(df, \"merchandise\")\ndf2\n</code></pre> bidder_id auction device time country ip url outcome non_robot_merchandise merchandise_1 merchandise_2 merchandise_3 merchandise_4 merchandise_5 merchandise_6 0 001068c415025a009fee375a12cff4fcnht8y 4ifac phone561 0.527973 bn 139.226 vasstdc27m7nks3 0 1 1 0 0 0 0 0 1 0030a2dd87ad2733e0873062e4f83954mkj86 obbny phone313 0.515267 ir 21.67 vnw40k8zzokijsv 0 0 0 1 0 0 0 0 2 00a0517965f18610417ee784a05f494d4dw6e l3o6q phone451 0.002705 bh 103.165 kk7rxe25ehseyci 0 1 0 0 1 0 0 0 3 00a0517965f18610417ee784a05f494d4dw6e du967 phone117 0.008986 tr 239.250 iu2iu3k137vakme 0 1 0 0 1 0 0 0 4 00a0517965f18610417ee784a05f494d4dw6e wx3kf phone16 0.009710 in 255.108 u85yj2e7owkz6xp 0 1 0 0 1 0 0 0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 99995 0ad17aa9111f657d71cd3005599afc24fd44y toxfq phone1036 0.962363 in 186.94 vasstdc27m7nks3 1 0 1 0 0 0 0 0 99996 0ad17aa9111f657d71cd3005599afc24fd44y ucb4u phone127 0.962380 in 119.27 vasstdc27m7nks3 1 0 1 0 0 0 0 0 99997 0ad17aa9111f657d71cd3005599afc24fd44y sg8yd phone383 0.962386 in 243.25 yweo7wfejrgbi2d 1 0 1 0 0 0 0 0 99998 0ad17aa9111f657d71cd3005599afc24fd44y toaj7 phone26 0.962393 in 17.66 4dd8ei0o5oqsua3 1 0 1 0 0 0 0 0 99999 0ad17aa9111f657d71cd3005599afc24fd44y 07axb phone25 0.962401 in 64.30 8zdkeqk4yby6lz2 1 0 1 0 0 0 0 0"},{"location":"projects/bot_detection_in_auction/#country","title":"country","text":"<pre><code>#sns.barplot(x=\"country\", y=\"outcome\", data=df)\n</code></pre> <p>no unique on merchandise because of unicite/bidder_id</p>"},{"location":"projects/bot_detection_in_auction/#groupage-par-bidder_id","title":"Groupage par bidder_id","text":"<p>Les variables actuelles</p> <pre><code>df.columns\n</code></pre> <pre><code>Index(['bidder_id', 'auction', 'device', 'time', 'country', 'ip', 'url',\n       'outcome', 'non_robot_merchandise', 'merchandise_1', 'merchandise_2',\n       'merchandise_3', 'merchandise_4', 'merchandise_5', 'merchandise_6'],\n      dtype='object')\n</code></pre> <p>Les nouvelles variables\u00a0:</p> Champs Descriptif nb_device Nombre d'appareils uniques utilis\u00e9s par bidder_id nb_auction Nombre d'ench\u00e8res auxquelles bidder_id a particip\u00e9 nb_ip Nombre d'adresses IP (identifiant de r\u00e9seau) uniques utilis\u00e9es par bidder_id nb_url Nombre d'URLs utilis\u00e9es par le bidder_id nb_country Nombre de pays uniques identifi\u00e9s par bidder_id outcome 1 si robot, 0 si homme non_robot_merchandise Cat\u00e9gories non choisies par les robots time Moyenne des \u00e9carts de temps pour chaque bidder_id nb_bid Nombre de bids effectu\u00e9es par bidder_id nb_merchandise_1 Nombre de \"jewelry\" uniques utilis\u00e9s par bidder_id nb_merchandise_2 Nombre de \"mobile\" uniques utilis\u00e9s par bidder_id nb_merchandise_3 Nombre de \"books and music\" uniques utilis\u00e9s par bidder_id nb_merchandise_4 Nombre de \"office equipment\" uniques utilis\u00e9s par bidder_id nb_merchandise_5 Nombre de \"sporting goods\" uniques utilis\u00e9s par bidder_id nb_merchandise_6 Nombre de \"home goods\" uniques utilis\u00e9s par bidder_id"},{"location":"projects/bot_detection_in_auction/#groupage-par-bidder_id-and-time","title":"Groupage par bidder_id and time","text":"<p>Pour chaque bidder_id, \u00e0 chaque instant donn\u00e9, on identifie le nombre de fois que l'utilisateur a utilis\u00e9 simultan\u00e9ment certaines ressources</p> <ul> <li> <p>adresse ip\u00a0: on identifie le nombre d'adresses ip utilis\u00e9es \u00e0 cet instant par cette personne</p> </li> <li> <p>auction\u00a0: le nombre d'ench\u00e8res o\u00f9 il \u00e9tait connect\u00e9 simultan\u00e9ment</p> </li> <li> <p>device\u00a0: le nombre d'appareils diff\u00e9rents qu'il a utilis\u00e9 simultan\u00e9ment</p> </li> <li> <p>url\u00a0: le nombre d'urls diff\u00e9rents qui ont \u00e9t\u00e9 utilis\u00e9s simultan\u00e9ment par l'enrichisseur</p> </li> <li> <p>country\u00a0: le nombre de pays depuis lesquels l'enrichisseur a fait simultan\u00e9ment la transaction .</p> </li> </ul> <p>Ensuite, pour chaque bidder_id et chaque temps, on somme ces 5 quantit\u00e9s.</p> <p>Ainsi, pour chaque bidder_id, on a une s\u00e9rie de valeurs sur laquelle on calcule des statistiques simples telles que la moyenne (my_agg_mean), la somme(my_agg_sum), l'\u00e9cart-type(my_agg_std) et le max(my_agg_max)</p> <pre><code>def compute_groupby(filename):\n  dd = df.groupby([\"bidder_id\", \"time\"]).agg({\n              \"auction\":lambda x: x.nunique() - 1,\n              \"device\":lambda x: x.nunique() - 1,\n              \"country\":lambda x: x.nunique() - 1,\n              \"ip\":lambda x: x.nunique() - 1,\n              \"url\":lambda x: x.nunique() - 1,\n              \"outcome\":lambda x: x.unique()[0]\n              })\n  cls_ =list(set(dd.columns)-{'outcome','bidder_id','time'})\n  dd[\"my_agg\"] = dd[cls_].sum(axis=1)\n  dd2 = dd.reset_index()\n  dd_min_per_bidder_id = {bidder_id: dd2[dd2.bidder_id==bidder_id].time.min() for bidder_id in dd2.bidder_id.unique()}\n\n  def modif_time(x):\n    x[\"time\"] = x[\"time\"] - dd_min_per_bidder_id[x[\"bidder_id\"]]\n    return x\n\n  dd2 = dd2.apply(modif_time, axis=1)\n  dd2.to_csv(filename)\n  return dd2\n</code></pre>"},{"location":"projects/bot_detection_in_auction/#normalisation-tous-les-features","title":"Normalisation (tous les features)","text":"<p>Chaque variable est normalis\u00e9e entre 0 et 1 \u00e0 l'aide d'un min-max-scaler</p> Preview <p></p> <p></p> <p></p> Histogramme <p>Pour chaque variable, nous faisons un histogramme afin de comprendre l'explicabilit\u00e9 de nos variables explicatives par rapport \u00e0 notre variable expliqu\u00e9 \u00ab\u00a0outcome\u00a0\u00bb. Nous ajoutons \u00e0 ces diagrammes une estimation de la densit\u00e9 de chacune de ses variables.</p> <p>Comme nos donn\u00e9es sont d\u00e9s\u00e9quilibr\u00e9es par rapport \u00e0 la variable expliqu\u00e9, nous effectuons une augmentation de donn\u00e9es par duplication pour ces visualisations.</p> <p>Les agr\u00e9gations sur bidder_id\u00a0:</p> <ul> <li>Nb_ip</li> </ul> <p> </p> <ul> <li>Nb_auction</li> </ul> <p> </p> <ul> <li>Nb_device</li> </ul> <p> </p> <ul> <li>Nb_url</li> </ul> <p> </p> <ul> <li>Nb_country</li> </ul> <p> </p> <ul> <li>Nb_bid</li> </ul> <p> </p> <p>On s'attends \u00e0 ce que les robots effectuent plus d'op\u00e9rations que les humains, ce qui se v\u00e9rifie facilement dans ces diagrammes. En effet, on repr\u00e9sente les histogrammes des variables (nb_ip, nb_auction, nb_device, nb_url, nb_country, nb_bid) (bleu pour les humains et rouge pour les robots).</p> <p>D'abord, on remarque qu'il y a beaucoup de valeurs nulles, ce qui signifie que pour chaque ressource (ip, url, ...), l'usage multiple de ces ressources n'est pas courant. En plus, parmi ceux qui font un usage multiple de ces ressources, on note des robots mais aussi des humains pour certaines ressources comme l'url ou l'appareil. Ce qui est contre intuitif, c'est que les humains d\u00e9passent parfois les robots sur ces m\u00e9triques.</p> <p>Avec les variables suivantes, nous introduiront les produits achet\u00e9s ainsi que le temps qui est tr\u00e8s important.</p> <ul> <li>Nb_merchandise_1 et 2 et 4 et 6</li> </ul> <p> </p> <ul> <li>Nb_merchandise_3</li> </ul> <p></p> <ul> <li>Nb_merchandise_5</li> </ul> <p> </p> <p>Ces visualisations montrent bien que les robots se s\u00e9parent des hommes quand l'utilisation des ressources augmente</p> <ul> <li>time</li> </ul> <p></p> <p>Cette variable est pertinante car la visualisation confirme l'\u00e9vidence\u00a0: le temps entre les connections des robots sont beaucoup plus court que ceux des hommes</p> <ul> <li>my_agg (aggr\u00e9gation sur le bidder_id et le temps)</li> </ul> <p></p> <p>Les agr\u00e9gations effectu\u00e9es sur le temps et le bidder_id, sont pertinentes car les robots ont plus tendance \u00e0 se connecter plusieurs fois de fa\u00e7on simultan\u00e9e</p> <p>Nous avons fait des tests statistiques en plus de ces visualisations mais ils ne seront pr\u00e9sent\u00e9s que bri\u00e8vement dans ce rapport.</p>"},{"location":"projects/bot_detection_in_auction/#binarization-on-continuous-features","title":"Binarization (on continuous features)","text":"<p>L'objectif est d'exploiter le pouvoir s\u00e9paratif de nos variables afin de rendre les algorithmes de classification plus efficaces.</p> <p>Chaque feature est projet\u00e9e sur l'espace {0,1} en d\u00e9finissant un seuil (threshold). Les valeurs sup\u00e9rieures au seuil correspondent \u00e0 1, tandis que les valeurs inf\u00e9rieures ou \u00e9gales au seuil correspondent \u00e0 0.</p> <p>Ce seuil est d\u00e9termin\u00e9 pour chaque feature. Pour cela, on choisit un nombre limit\u00e9 de valeurs possibles entre 0 et1 (range (0, 1, 0.05)). Pour chaque valeur,</p> <p>On transforme la s\u00e9rie en variable cat\u00e9gorielle</p> <p></p> <p>On s\u00e9pare la s\u00e9rie en deux \u00e9chantillons (outcome=0 vs outcome=1). Puis, on r\u00e9alise un test de student pour les moyennes de deux \u00e9chantillons pour voir s'ils sont significativement diff\u00e9rents</p> <p></p> <p>On s\u00e9pare la s\u00e9rie en deux \u00e9chantillons selon les deux cat\u00e9gories. Puis, on calcule la proportion de robots dans chaque \u00e9chantillon. Ensuite, on compare ces proportions \u00e0 l'aide d'un z-test pour voir s'ils sont significativement diff\u00e9rents</p> <p></p> <p>Si ces deux tests donnent des r\u00e9sultats conclusifs (p-value\\&lt;0.05), on les retients</p> <p>On choisit le seuil qui passe les deux tests avec des p-valeurs minimales. Ce seuil est utilis\u00e9 pour transformer la variable continue en variable binaire (0 et 1)</p>"},{"location":"projects/bot_detection_in_auction/#resampling-techniques","title":"Resampling Techniques","text":"<p>On remarque que sur 87 bidder_id, seuls 6 correspondent \u00e0 des robots. Nous appliquons alors une m\u00e9thode de oversampling nomm\u00e9e SMOTE. Cette m\u00e9thode, ne duplique pas des lignes mais cr\u00e9\u00e9 de nouveaux points \u00e0 partir des anciens en trouvant des points au milieu des points existants.</p>"},{"location":"projects/bot_detection_in_auction/#strategie-dentrainement","title":"Strat\u00e9gie d'entrainement","text":""},{"location":"projects/bot_detection_in_auction/#split-train-test","title":"Split train test","text":"<p>Nous s\u00e9parons d'abord notre base de donn\u00e9es en deux parties. Une partie pour l'entrainement et une autre pour le test (50% du dataset).</p>"},{"location":"projects/bot_detection_in_auction/#models","title":"Models","text":"<p>Nous utilisons principalement des mod\u00e8les de classification disponibles sous la librairie sklearn. Il s'agit notamment</p> <ul> <li> <p>Mod\u00e8les lin\u00e9aires (R\u00e9gression logistique (sous sklearn ou statsmodels), Support vecteur Machine (SVC), Descente de gradient stochastique (SDG))</p> </li> <li> <p>Tree based (Arbre de d\u00e9cision, Random Forest)</p> </li> <li> <p>D'autres mod\u00e8les (KNN, LDA)</p> </li> <li> <p>Des mod\u00e8les ensemblistes (AdaBoost, Gradient Boosting)</p> </li> </ul> <p>Nous pr\u00e9senterons d'abord les r\u00e9sultats obtenus sur les algorithmes\u00a0: R\u00e9gression logistique, Support vecteur machine et random Forest.</p>"},{"location":"projects/bot_detection_in_auction/#choix-des-hyperparametres","title":"Choix des hyperparam\u00e8tres","text":"<p>Pour certains mod\u00e8les, nous utilisons une m\u00e9thode nomm\u00e9e Grid Search. Nous choisissions certains param\u00e8tres importants ainsi que des valeurs possibles. Cette m\u00e9thode am\u00e9liore optimise le loss du mod\u00e8le sur le training set en fonction des valeurs des hyperparam\u00e8tres donn\u00e9es.</p> Algorithm Parameters Objectives Regression Logistic - max_iter: [5, 100, 1000, 2000]  - penalty: [\"l1\", \"l2\", \"elasticnet\", \"none\"]  - C: [0.01, 0.1, 0.5, 1]  - solver: [\"lbfgs\", \"liblinear\"] - Varying the number of iterations  - Testing different regularizations with different weights  - Testing the \"liblinear\" solver suitable for small datasets SVM - gamma='scale'  - probability=True  - C=1  - kernel=\"linear\"  - class_weight=\"balanced\" - Utilizing a linear model Random Forest - max_depth: [1, 3, 5, 15, 25]  - max_leaf_nodes: [1, 3, 5, 15]  - n_estimators: [10, 50, 100]  - max_features: [\"sqrt\", \"log2\", None]  - criterion: [\"gini\", \"entropy\", \"log_loss\"] - Varying tree sizes but maintaining a maximum limit  - Testing different metrics, particularly the log-loss"},{"location":"projects/bot_detection_in_auction/#selection-de-feature","title":"Selection de feature?","text":"<p>Nous avons fait nos mod\u00e8les sur 19 variables et nous souhaitons tiliser 5 variables. Afin de s\u00e9lectionner les variables les plus pertinentes pour chaque mod\u00e8le, nous utilisons une m\u00e9thode qui \u00e9limine les variables les moins importantes de fa\u00e7on it\u00e9rative. Une impl\u00e9mentation est disponible sous sklearn \u00e0 travers la fonction RFE (recursive feature elimination)</p>"},{"location":"projects/bot_detection_in_auction/#metriques-de-validation","title":"M\u00e9triques de validation","text":"<p>Afin de valider nos mod\u00e8les, nous utilisons des m\u00e9triques adapt\u00e9es \u00e0 la classification. Il s'agit de</p> <ul> <li> <p>Matrice de confusion\u00a0: Elle montre le nombre de pr\u00e9dictions (bien class\u00e9 vs mal class\u00e9) en fonction des cat\u00e9gories</p> </li> <li> <p>Pr\u00e9cision\u00a0: Mesure la capacit\u00e9 du mod\u00e8le \u00e0 identifier tous les humains m\u00eame s'il classe mal des robots. Il r\u00e9duit le nombre de faux positifs</p> </li> <li> <p>recall\u00a0: Mesure la capacit\u00e9 du mod\u00e8le \u00e0 identifier tous les robots m\u00eame si il classe mal des humains. Il r\u00e9duit le nombre de faux n\u00e9gatifs</p> </li> </ul> <p></p> <ul> <li>f1\u00a0: elle combine la precision et le recall</li> </ul> <p></p> <ul> <li>Courbe ROC\u00a0: Calculer l\\'aire sous la courbe ROC \u00e0 partir des scores de pr\u00e9diction.</li> </ul> <p>Toutes ces m\u00e9triques sont disponibles sous sklearn</p>"},{"location":"projects/bot_detection_in_auction/#prediction","title":"Pr\u00e9diction","text":""},{"location":"projects/bot_detection_in_auction/#librairies-utiles-pour-la-prediction","title":"Librairies utiles pour la pr\u00e9diction","text":"<pre><code>from sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix,r2_score,accuracy_score,roc_auc_score,f1_score,precision_score,recall_score\n</code></pre>"},{"location":"projects/bot_detection_in_auction/#demarche","title":"D\u00e9marche","text":"<p>Les mod\u00e8les sont utilis\u00e9s dans une classe qui impl\u00e9mente la m\u00e9thode suivante</p> <pre><code>def compute_all(self, model_name, important_cols:list=None, oversample:bool=None, test_size:float=None, remove_cols:list=None, feature_eng:bool=None):\n      print(f\"model loader: {self.description}\")\n      list_fct = {\n            \"logit\": self.logit_regression,\n            \"logistic\": logistic_regression_sklearn,\n            \"svc\": svc_classifier,\n            \"knn\":knn_classifier,\n            \"sdg\":sdg_classifier,\n            \"tree\":decision_tree_classifier,\n            \"lda\": lda_classifier,\n            \"forest\":random_forest_classifier,\n            \"ada\": ada_boost_classifier,\n            \"xgboost\": gradient_boosting_classifier\n            }\n\n      # transform, split and oversample\n      X_train, X_test, y_train, y_test = self.get_train_test(model_name=model_name, important_cols=important_cols, oversample=oversample, test_size=test_size, remove_cols=remove_cols, feature_eng=feature_eng)\n\n      # get model\n      my_fct = list_fct[model_name]\n      print(f\"{'':~^50}\\n{model_name:~^50}\\n{'':~^50}\")\n\n      # train\n      y_test_proba, y_train_proba, y_train_pred, y_test_pred, nb_params = my_fct(X_train, y_train, X_test, y_test)\n\n      # evaluate\n      train_res, test_res = show_results(y_train, y_train_pred, y_train_proba,y_test, y_test_pred, y_test_proba)\n      g = f\"{model_name}: nb_params = {nb_params}\"\n      print(f\"{g:~^50}\")\n</code></pre> <p>Cette m\u00e9thode utilise des algorithmes qui impl\u00e9mentent chaque une m\u00e9thode de classification avec comme valeur ajout\u00e9e</p> <ul> <li> <p>La s\u00e9lection des hyperparam\u00e8tres \u00e0 l'aide de GridSearchCV</p> </li> <li> <p>Le choix \u00e9ventuel d'un threshold plus optimal pour les m\u00e9thodes qui g\u00e9n\u00e9rent des probabilit\u00e9s</p> </li> <li> <p>La s\u00e9lection des variables avec RFE</p> </li> </ul> <p>Les r\u00e9sultats sont pr\u00e9sent\u00e9s avec cet algorithme</p> <pre><code>def return_dict_scores(y, y_pred, pred_proba):\n  return {\"acc\":round(accuracy_score(y_pred, y),3), \"f1\":round(f1_score(y_pred, y),3), \"pres\":round(precision_score(y_pred, y),3), \"rec\":round(recall_score(y_pred, y),3), \"roc\":round(roc_auc_score(y_pred, pred_proba),3)}\n\ndef bestThressholdForF1(y_true,y_pred_proba):\n    best_thresh = None\n    best_score = 0\n    for thresh in np.arange(0.1, 0.501, 0.01):\n        y_pred = np.array(y_pred_proba)&gt;thresh\n        score = f1_score(y_true, y_pred)\n        if score &gt; best_score:\n            best_thresh = thresh\n            best_score = score\n    return best_score , best_thresh, return_dict_scores(y_true, y_pred, y_pred_proba)\n\ndef print_metrics(y, y_pred,pred_proba):\n  print(\"- confusion_matrix\\n\",confusion_matrix(y, y_pred))\n  print(f\"- accuracy = {100*accuracy_score(y, y_pred):.2f}%\") #better -&gt;1 ##accuracy = nb_sucess/nb_sample\n  print(f\"- f1 = {100*f1_score(y, y_pred):.2f}%\") #better -&gt;1 ##f1 = 2 * (precision * recall) / (precision + recall)\n  print(f\"- roc(area under the curve) = {100*roc_auc_score(y, pred_proba):.2f}%\") #better -&gt;1 ##area under ROC and AUC\n  print(f\"- precision = {100*precision_score(y, y_pred):.2f}%\") #better-&gt;1 ##precision = tp / (tp + fp) where (tp=true_positive; fp:false_positive)\n  print(f\"- recall = {100*recall_score(y, y_pred):.2f}%\") #better-&gt;1 ##precision = tp / (tp + fn) where (tp=true_positive; fn:false_negative)\n  print(f\"- bestThressholdForF1 = {bestThressholdForF1(y,pred_proba)}\")\n  return return_dict_scores(y, y_pred, pred_proba)\n\ndef show_results(y_train, y_train_pred, y_train_proba,y_test, y_test_pred, y_test_proba ):\n  print(\"\\n&gt;&gt;&gt;&gt; metriques sur la base de donn\u00e9es d'entrainement\")\n  train_res = print_metrics(y_train,y_train_pred,y_train_proba)\n  print(\"\\n&gt;&gt;&gt;&gt; metriques sur la base de donn\u00e9es de test\")\n  test_res = print_metrics(y_test, y_test_pred,y_test_proba)\n  return train_res, test_res\n</code></pre>"},{"location":"projects/bot_detection_in_auction/#quelques-exemples","title":"Quelques exemples","text":"<ul> <li>R\u00e9gression logistique</li> </ul> <pre><code>def logistic_regression_sklearn(X_train, y_train, X_test, y_test):\n  parameters = {\"max_iter\":[5,100,1000,2000], \"penalty\":[\"l1\", \"l2\", \"elasticnet\", \"none\"], \"C\":[0.01, 0.1, 0.5, 1], \"solver\":[\"lbfgs\", \"liblinear\"]}\n  clf = GridSearchCV(LogisticRegression(random_state=0, class_weight=\"balanced\"), parameters).fit(X_train, y_train)\n  print(clf.best_params_)\n\n  #clf = LogisticRegression(random_state=0, max_iter=1000, class_weight=\"balanced\").fit(X_train, y_train)\n  y_train_pred = clf.predict(X_train)\n  y_test_pred = clf.predict(X_test)\n  y_train_proba = clf.predict_proba(X_train)[:, 1] #[proba({label=1}/row_data) for row_data in X_train]\n  y_test_proba = clf.predict_proba(X_test)[:, 1]\n  return y_test_proba, y_train_proba, y_train_pred, y_test_pred, len(clf.feature_names_in_)\n</code></pre> <ul> <li>Knn</li> </ul> <pre><code>def knn_classifier(X_train, y_train, X_test, y_test):\n  parameters = {\"n_neighbors\":[1,2,5,10,15,20]}\n  clf = GridSearchCV(KNeighborsClassifier(), parameters).fit(X_train, y_train)\n  print(clf.best_params_)\n  #clf = KNeighborsClassifier(n_neighbors=10).fit(X_train, y_train)\n  y_train_pred = clf.predict(X_train)\n  y_test_pred = clf.predict(X_test)\n  y_train_proba = clf.predict_proba(X_train)[:, 1] #[proba({label=1}/row_data) for row_data in X_train]\n  y_test_proba = clf.predict_proba(X_test)[:, 1]\n  return y_test_proba, y_train_proba, y_train_pred, y_test_pred, len(clf.feature_names_in_)\n</code></pre>"},{"location":"projects/bot_detection_in_auction/#algorithmes","title":"Algorithmes","text":""},{"location":"projects/bot_detection_in_auction/#configurations","title":"Configurations","text":"<ul> <li>Base de donn\u00e9es utilis\u00e9e\u00a0: Par d\u00e9faut, c'est une base de donn\u00e9es de 87 lignes qui contient tous les features qu'on a caucul\u00e9 auparavant</li> </ul> <ul> <li> <p>Oversampling\u00a0: par d\u00e9faut True. Dans ce cas, on utilise la m\u00e9thode SMOTE</p> </li> <li> <p>Test_size\u00a0: par d\u00e9faut 50%</p> </li> <li> <p>Feature_eng\u00a0: si on applique le binarizer aux donn\u00e9es avant l'entrainement. Oui par d\u00e9faut</p> </li> <li> <p>Important_cols\u00a0: la liste des colonnes \u00e0 s\u00e9lectionner\u00a0: par d\u00e9faut, toutes les colonnes sont utilis\u00e9es m\u00eame si on fera un filtre pendant le fine tuning du mod\u00e8le (RFE)</p> </li> </ul>"},{"location":"projects/bot_detection_in_auction/#resultats","title":"R\u00e9sultats","text":"<ul> <li>Algorithmique</li> </ul> <ul> <li>Dataset avec oversampling (SMOTE) appliqu\u00e9 juste au training set</li> </ul> <ul> <li>Dataset avec oversampling (Duplication) appliqu\u00e9 \u00e0 tout le dataset</li> </ul> <ul> <li>Dataset avec oversampling (SMOTE) appliqu\u00e9 \u00e0 tout le dataset</li> </ul> L\u00e9gende <ul> <li> <p>acc\u00a0: accuracy score</p> </li> <li> <p>f1\u00a0: f1 score</p> </li> <li> <p>pres\u00a0: precision score</p> </li> <li> <p>rec\u00a0: recall score</p> </li> <li> <p>roc\u00a0: area under the curve</p> </li> <li> <p>Ada: AdaBoost</p> </li> <li> <p>Forest: Random Forest</p> </li> <li> <p>Knn: K nearest neighbors</p> </li> <li> <p>Lda: Linear Discriminant analysis</p> </li> <li> <p>logistic: Logistic regression (sklearn)</p> </li> <li> <p>Logit: Logistic regression Analysis (statsmodels)</p> </li> <li> <p>Sgd: Descente de gradient stochastique</p> </li> <li> <p>Svc\u00a0: Support vecteur Machine</p> </li> <li> <p>Tree\u00a0: Arbre de decision</p> </li> <li> <p>Xgboost\u00a0: Gradient Bossting</p> </li> </ul>"},{"location":"projects/bot_detection_in_auction/#comparaison-des-modeles","title":"Comparaison des mod\u00e8les","text":"<p>Nous remarquons en g\u00e9n\u00e9ral, que nos mod\u00e8les sur des features limit\u00e9s ou non, donnent les r\u00e9sultats int\u00e9ressants</p> <ul> <li> <p>Des accuracy &gt; 0.9 sur tous nos datasets, ce qui n'est pas \u00e9tonnant vu qu'un tuning est fait sur les hyperparam\u00e8tres pendant et apr\u00e8s l'entrainement.</p> </li> <li> <p>Presque tous les recall des mod\u00e8les donnent un r\u00e9sultat de 100%  </p> </li> <li> <p>Pour les mod\u00e8les (random Forest, regression logistique, SVM et decision tree), nous avons s\u00e9lectionn\u00e9 4 param\u00e8tres et les r\u00e9sultats sont toujours aussi bons. Ces mod\u00e8les sont donc meilleurs que les autres car plus explicables notamment la regression logistique. Concernat les mod\u00e8les bas\u00e9s sur les arbres (random Forest et decision tree), les visualisations (qui seront montr\u00e9s en bas) montrent que les rabres sont tr\u00e8s lisibles (profondeur de 3 et largeur faible car limit\u00e9 dans le hyperparameter tuning)</p> </li> <li> <p>Concernant le choix de la m\u00e9thode de oversampling, nous pouvons, en comparer les 3 tables,</p> </li> <li> <p>remarquer que malgr\u00e9 de bon recall, la premi\u00e8re table montre des pr\u00e9cisions de moins de 50% sur le test alors que les pr\u00e9cisions sont sup\u00e9rieures \u00e0 90% sur le train, ce qui montre du overfitting. Cette premi\u00e8re table repr\u00e9sente le dataset o\u00f9 on a appliqu\u00e9 le oversampling juste sur le training set.</p> </li> <li> <p>Remarquer que la table 3 pr\u00e9sente en g\u00e9n\u00e9ral de meilleurs r\u00e9sultats que la table 2, ce qui montre que la m\u00e9thode SMOTE est plus explicative qu'une simple duplication des donn\u00e9es non repr\u00e9sentatives.</p> </li> </ul> <p>En conclusion, il est pr\u00e9f\u00e9rable d'utiliser des mod\u00e8les \u00e0 peu de variables car ils sont relativement aussi bons que les autres. Il est \u00e9galement pr\u00e9f\u00e9rable d'utiliser SMOTE comme m\u00e9thode d'oversampling et de l'appliquer \u00e0 tout le dataset avant l'entrainement</p>"},{"location":"projects/bot_detection_in_auction/#review","title":"Review","text":""},{"location":"projects/bot_detection_in_auction/#overfitting","title":"Overfitting","text":"<p>Il n'y a pas eu quelques ph\u00e9nom\u00e8nes d'overfitting pendant l'impl\u00e9mentation.</p> <p>Comme un tuning important a \u00e9t\u00e9 fait sur les hyperparam\u00e8tres et qu'on a un nombre assez limit\u00e9 de donn\u00e9es, l'overfitting est \u00e9vit\u00e9 en utilisant 50% de la base de donn\u00e9es pour le test. Pour compenser le nombre limit\u00e9 de donn\u00e9es en train et l'impact du d\u00e9s\u00e9quilibre des classes, nous appliquons la m\u00e9thode SMOTE sur notre base de donn\u00e9es. N\u00e9anmoins, m\u00eame quand on applique la m\u00e9thode SMOTE uniquement sur la base de donn\u00e9es d'entrainement, on a \u00e9galement de bons r\u00e9sultats</p>"},{"location":"projects/bot_detection_in_auction/#choix-techniques","title":"Choix techniques","text":"<ul> <li> <p>Dans la construction des features, nous avons calcul\u00e9 pour chaque bidder_id, des aggr\u00e9gations. Ce sont des sommes. On am\u00e9liore le pouvoir de s\u00e9parabilit\u00e9 en normalisant ces donn\u00e9es par rapport au nombre de bid auquels le bidder_id a particip\u00e9.</p> </li> <li> <p>Faire un oversampling sur toute le dataset ou juste sur le training set\u00a0: le choix est fait en fonction du mod\u00e8le</p> </li> </ul>"},{"location":"projects/bot_detection_in_auction/#explicabilite-des-resultats","title":"Explicabilit\u00e9 des r\u00e9sultats","text":""},{"location":"projects/bot_detection_in_auction/#regression-logistique","title":"R\u00e9gression Logistique","text":""},{"location":"projects/bot_detection_in_auction/#random-forest","title":"Random Forest","text":"<p>Ce mod\u00e8le utilise 5 arbres de d\u00e9cisions et 4 variables et obtient une precision de 87% et un recall de 85%. Les processus de d\u00e9cision est assez simple et intuitif. Par ailleurs, on peut avoir de meilleurs r\u00e9sultats en moyennant les calculs (par groupe de bidder_id) par le nombre de bid. On a une am\u00e9lioration des m\u00e9triques</p>"},{"location":"projects/bot_detection_in_auction/#random-forest-avec-moyenne-par-groupe","title":"Random Forest (avec moyenne par groupe)","text":"<p>Ce mod\u00e8le utilise 5 arbres de d\u00e9cisions et 4 variables</p> <ul> <li> <p>Nb_auction</p> </li> <li> <p>Nb_device</p> </li> <li> <p>Nb_country</p> </li> <li> <p>Nb_url</p> </li> </ul> <p>Ces variables ont \u00e9t\u00e9 moyenn\u00e9es par rapport au nombre de bid auxquels ils ont particip\u00e9. Ainsi, on remarque que conform\u00e9ment aux r\u00e9sultats obtenus dans l'\u00e9tape de feature engineering, les nombre d'appareils petits correspondent \u00e0 des humains\u00a0; les url et les ench\u00e8res et pays de petites valeurs correspondent \u00e0 des robots</p>"},{"location":"projects/bot_detection_in_auction/#support-vecteur-machine","title":"Support vecteur Machine","text":"<p>Avec 4 variables, on ne peut pas voir les s\u00e9parations lin\u00e9aires mais on n'a pas voulu appliquer des m\u00e9thodes suppl\u00e9mentaires de r\u00e9duction de dimension telles que PCA ou SVD. N\u00e9anmoins, le nombre limit\u00e9 de features et les bons r\u00e9sultats obtenus sur ce mod\u00e8le montre qu'on pourrait avec plus d'analyses expliquer les r\u00e9sultats.</p>"},{"location":"projects/bot_detection_in_auction/#decision-tree","title":"Decision Tree","text":"<p>Les r\u00e9sultats sont tr\u00e8s bons avec un processus de d\u00e9cision assez simple et explicable.</p>"},{"location":"projects/bot_detection_in_auction/#discussion","title":"Discussion","text":"<ul> <li> <p>Les mod\u00e8les donnent en g\u00e9n\u00e9ral de bons r\u00e9sultats avec la normalisation</p> </li> <li> <p>Nous avons utilis\u00e9 une table avec des unique bidder_id pour aggr\u00e9ger en \u00e9vitant les donn\u00e9es dupliqu\u00e9es, ce qui a r\u00e9duit le nombre de points de donn\u00e9es. Mais, ensuite, nous avons fait du oversampling. N\u00e9anmoins, nous avons utilis\u00e9 SMOTE, une m\u00e9thode de oversampling qui ne duplique pas simplement les donn\u00e9es. Mais unee base de donn\u00e9es de validation aurait pu \u00eatre utilis\u00e9e afin de s\u00e9parer totalement les m\u00e9thodes de feature engineering de l'\u00e9valuation des mod\u00e8les</p> </li> <li> <p>Comme il y a peu de points de donn\u00e9es, nous avons mis l'accent sur le feature engineering afin d'avoir des variables pertinentes, ce qui nous as permis d'avoir des r\u00e9sultats satisfaisants (precision, recall&gt;96%) sur un mod\u00e8le lin\u00e9aire tel que la r\u00e9gression logistique</p> </li> <li> <p>Des corr\u00e9lations entre les variables explicatives n'impactent pas les r\u00e9sultats gr\u00e2ce \u00e0 la s\u00e9lection de variables qui entre en compte pendant l'entrainement du mod\u00e8le.</p> </li> <li> <p>Architecture de d\u00e9ploiement\u00a0: A partir de notre mod\u00e8le, on prend le maximum d'informations sur un bidder puis, on fait du feature engineering avant d'utiliser les mod\u00e8les. Ainsi,</p> </li> <li> <p>Pendant l'exploration, on a remaqu\u00e9 que certains produits \u00e9taient achet\u00e9s que par des humains (dans les limites de ce datset). Ainsi, nous aurions pu enlever les lignes correspondantes car la pr\u00e9diction est \u00e9vidente. D'un autre point de vue, les garder peut apporter des informations de correlations sur les autres features.</p> </li> </ul>"},{"location":"projects/bot_detection_in_auction/#conclusion","title":"Conclusion","text":"<p>Ce projet de classification pr\u00e9sente des sp\u00e9cificit\u00e9s int\u00e9ressantes. D'une part, on a pu valider \u00e0 priori qu'on a sur les robots \u00e0 l'aide des donn\u00e9es qu'on avait m\u00eame s\\'il y a eu des paradoxes quelques fois qui peuvent \u00eatre li\u00e9s au nombre limit\u00e9 de donn\u00e9es ou \u00e0 des facteurs ext\u00e9rieurs. D'autre part, la base de donn\u00e9es brute contient beaucoup de champs texte. Ainsi, la transformation des donn\u00e9es est l'\u00e9tape cruciale de ce projet. En plus, la base de donn\u00e9es, d\u00e9s\u00e9quilibr\u00e9e, n\u00e9cessite d'utiliser des techniques particuli\u00e8res pour l'entrainement. Nous avons \u00e9galement adapt\u00e9 les m\u00e9triques de validation au probl\u00e8me de classification et avons obtenu des r\u00e9sultats satisfaisants (recall, pr\u00e9cision &gt; 90%) avec jusque 4 \u00e0 5 features maximum, ce qui montre le potentiel que le feature engineering peut apporter \u00e0 la mod\u00e9lisation et \u00e0 l'explicabilit\u00e9 des mod\u00e8les. Enfin, avec des mod\u00e8les simples qui proposent des recall parfaits, nous pouvons \u00eatre s\u00fbr de d\u00e9terminer les robots m\u00eame si certains (tr\u00e8s peu) d'humains sont mals pr\u00e9dits</p>"},{"location":"projects/file-hosting-app/","title":"Flask based File Hosting (web app & api & python module & cli app)","text":""},{"location":"projects/file-hosting-app/#flask-based-file-hosting-web-app-api-python-module-cli-app","title":"Flask based File Hosting (web app &amp; api &amp; python module &amp; cli app)","text":""},{"location":"projects/file-hosting-app/#introduction","title":"Introduction","text":"<p>I've implemented A simple Flask application designed for sharing files. The application can be hosted on a server so users can upload files and generate links for sharing.</p>"},{"location":"projects/file-hosting-app/#prerequistes","title":"Prerequistes","text":"<ul> <li>python &gt;=3.9</li> </ul>"},{"location":"projects/file-hosting-app/#installation","title":"Installation","text":"<ol> <li>Clone the repository:</li> </ol> <pre><code>git clone https://github.com/hermann-web/simple-file-hosting-with-flask.git\n</code></pre> <ol> <li>Create a virtual environment and activate it:</li> </ol> <pre><code>python3 -m venv env\nsource env/bin/activate\n</code></pre> <ol> <li>Install the required dependencies:</li> </ol> <pre><code>pip install -r requirements.txt\n</code></pre> <ol> <li>Start the Flask development server:</li> </ol> <pre><code>python app.py\n</code></pre> <ol> <li>Access the application by visiting http://localhost:5000 in your web browser.</li> </ol>"},{"location":"projects/file-hosting-app/#development-details","title":"Development Details","text":"<ul> <li>The application allows users to upload files and share them with others by providing a link to the file.</li> <li>To use the application, users need to clone the repository, create a virtual environment, install the required dependencies, and start the Flask development server.</li> <li>Once the server is running, users can access the application by visiting http://localhost:5000 in their web browser.</li> <li>The main page displays a list of shared files, and users can upload a file by clicking on \"Upload a File\" and selecting the file they want to share.</li> <li>The uploaded files will be listed on the main page for download.</li> <li>The application can be deployed on a remote server or cloud provider, such as AWS, Google Cloud, or Heroku.</li> <li>You can see the step by step implementation of the code</li> </ul>"},{"location":"projects/file-hosting-app/#usage","title":"Usage","text":""},{"location":"projects/file-hosting-app/#access-the-flask-web","title":"access the flask web","text":"<ul> <li>The main page displays a list of shared files.</li> <li>To upload a file, click on \"Upload a File\" and select the file you want to share.</li> <li>The uploaded files will be listed on the main page for download.</li> </ul>"},{"location":"projects/file-hosting-app/#access-the-app-through-an-api","title":"access the app through an api","text":"<ul> <li>you can access the api with the routes <code>http://localhost:5000/api/*</code></li> <li>The file cli_app/cli_app.py to access the api along with a context manager to handle sessions</li> <li>you can read the api documentation</li> </ul>"},{"location":"projects/file-hosting-app/#access-the-apps-api-with-a-cli-app","title":"access the app's api with a cli app","text":"<ul> <li>The file cli_app/sharefile.py provide a cli app to access the api context manager</li> <li>Using your cli, you can get list, upload and download files. The api will be called behind the hood by cli_app/cli_app.py</li> <li>you can read the api documentation</li> </ul>"},{"location":"projects/file-hosting-app/#deployment-guide","title":"Deployment Guide","text":"<p>To deploy the File Sharing App, follow these steps:</p> <ol> <li> <p>Choose a remote server or cloud provider to host your application. Popular options include AWS, Google Cloud, and Heroku.</p> </li> <li> <p>Set up an instance or virtual machine on your chosen server.</p> </li> <li> <p>Connect to your remote server.</p> </li> <li> <p>Install the required dependencies.</p> </li> <li> <p>Modify the Flask application's configuration to use a production-ready web server.</p> </li> <li> <p>Configure your domain or subdomain to point to the IP address of your remote server.</p> </li> <li> <p>Set up SSL/TLS certificates for secure HTTPS communication.</p> </li> <li> <p>Start the Flask application using the production-ready web server.</p> </li> <li> <p>Verify that your file sharing app is accessible.</p> </li> <li> <p>Monitor the deployed application for errors and performance issues.</p> </li> </ol> <p>Remember to follow best practices for securing your deployed application.</p>"},{"location":"projects/file-hosting-app/#license","title":"License","text":"<p>This project is licensed under the MIT License.</p>"},{"location":"projects/image-to-latex-formula/","title":"Streamlit App for Formula OCR using pix2tex","text":""},{"location":"projects/image-to-latex-formula/#introduction","title":"Introduction","text":"<p>I've implemented and deployed a Streamlit application designed for writing a LaTeX formula from an image containing that formula. The application is hosted on Hugging Face Spaces and github so users can upload images freely and copy LaTeX formulas.</p> <p>For the frontend, I use Streamlit, a framework that makes frontend for data projects easy. For the backend converter, I use a Python module named pix2tex that uses computer vision to generate the LaTeX formula.</p> <pre><code>from PIL import Image\nfrom pix2tex.cli import LatexOCR\n\nimg = Image.open('path/to/image.png')\nmodel = LatexOCR()\nprint(model(img))\n</code></pre> <p>You can access the application online at https://pix2tex.streamlit.app/ or https://huggingface.co/spaces/hermann-web/pix2tex.</p> <p></p> <p>You can read the following if you want to know the implementation details or clone the project.</p>"},{"location":"projects/image-to-latex-formula/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python &gt;=3.9</li> <li>torch &gt;=2.0.0</li> <li>Streamlit</li> <li>Pillow</li> <li>pix2tex</li> </ul>"},{"location":"projects/image-to-latex-formula/#installation","title":"Installation","text":"<ol> <li> <p>Clone the repository:</p> <pre><code>git clone https://huggingface.co/spaces/hermann-web/pix2tex\ncd pix2tex\n</code></pre> </li> <li> <p>Create a virtual environment and activate it:</p> </li> </ol> <code>For Linux &amp; Mac</code> <code>For Windows</code> <pre><code>python3 -m venv myenv\nsource myenv/bin/activate\n</code></pre> <pre><code>python -m venv myenv\n./myenv/Scripts/activate\n</code></pre> <ol> <li> <p>Install the required dependencies:</p> <pre><code>pip install -r requirements.txt\n</code></pre> </li> <li> <p>Start the Streamlit server:</p> <pre><code>streamlit run app.py\n</code></pre> </li> <li> <p>Access the application by visiting http://localhost:8501 in your web browser.</p> </li> </ol>"},{"location":"projects/image-to-latex-formula/#development-details","title":"Development Details","text":"<p>During the development of the Streamlit-based formula OCR app, several key components and decisions were made to ensure a smooth and efficient workflow.</p>"},{"location":"projects/image-to-latex-formula/#backend-with-pix2tex","title":"Backend with pix2tex","text":"<p>The core functionality of converting images into LaTeX formulas is powered by the <code>pix2tex</code> Python module. This module leverages computer vision techniques to perform LaTeX OCR on images. The following snippet illustrates how the image processing is handled:</p> <pre><code>from PIL import Image\nfrom pix2tex.cli import LatexOCR\n\ndef process_image(image):\n    # Perform LaTeX OCR on the image\n    img = Image.open(image)\n    model = LatexOCR()\n    latex_formula = model(img)\n    return latex_formula\n</code></pre> <p>The <code>LatexOCR</code> class from <code>pix2tex</code> is responsible for extracting LaTeX formulas from images, making the app suitable for mathematical content extraction.</p>"},{"location":"projects/image-to-latex-formula/#streamlit-for-frontend","title":"Streamlit for Frontend","text":"<p>The frontend of the application is built using Streamlit, a powerful Python library for creating web applications with minimal effort. Streamlit allowed for rapid prototyping and easy integration with the backend.</p> <p>The image is rendered with a streamlit function <code>st.latex</code>:</p> <pre><code>import streamlit as st\nst.latex(f\"\\n{latex_formula}\\n\")\n</code></pre>"},{"location":"projects/image-to-latex-formula/#usage","title":"Usage","text":"<p>Add information about how users can use the application, including any specific instructions or features.</p>"},{"location":"projects/image-to-latex-formula/#deployment-guide","title":"Deployment Guide","text":""},{"location":"projects/image-to-latex-formula/#with-streamlitio","title":"With streamlit.io","text":"<p>Deployment on streamlit.io is pretty fastforward <sup>1</sup>. You can put the application on a public repository. Then, Go to the website https://share.streamlit.io/deploy, signup with you github account for example, then choose the repository and your pyhton script.</p>"},{"location":"projects/image-to-latex-formula/#on-streamlit","title":"On streamlit","text":"<p>To deploy the app on Hugging Face, follow these steps:</p> <ol> <li> <p>Create a Hugging Face Account:</p> <ul> <li>If you don't have a Hugging Face account, create one on Hugging Face.</li> </ul> </li> <li> <p>Create a New Space:</p> <ul> <li>Log in to your Hugging Face account.</li> <li>Navigate to the Spaces page.</li> <li>Click on \"New Space\" and give it a name, e.g., \"FileHostingApp.\"</li> </ul> </li> <li> <p>Upload Your Code:</p> <ul> <li>Inside your local <code>pix2tex</code> directory, create a <code>.zip</code> archive of your code.</li> <li>Upload the <code>.zip</code> archive to your newly created Space.</li> </ul> </li> <li> <p>Set Up Environment Variables:</p> <ul> <li>In your Hugging Face Space, go to \"Settings.\"</li> <li>Under the \"Environment Variables\" section, add the necessary variables, such as any API keys or configuration parameters.</li> </ul> </li> <li> <p>Deploy the App:</p> <ul> <li>Go back to the \"Overview\" tab in your Space.</li> <li>Click on \"Deploy Model\" and follow the instructions to deploy your Streamlit app.</li> </ul> </li> <li> <p>Access Your Deployed App:</p> <ul> <li>Once deployed, you can access your app through the provided link.</li> </ul> </li> </ol>"},{"location":"projects/image-to-latex-formula/#license","title":"License","text":"<p>This project is licensed under the MIT License.</p> <ol> <li> <p>Streamlit documentation: Create an app \u21a9</p> </li> </ol>"},{"location":"projects/introducing-two-new-packages-for-streamlining-file-conversions-in-python/","title":"Introducing Two New Packages for Streamlining File Conversions in Python","text":"<p>In the realm of data processing and manipulation, efficient handling of file conversions is often a crucial requirement. Python, with its rich ecosystem of libraries and frameworks, provides powerful tools for tackling such tasks.</p> <p>I've developed two packages with the intention of offering a robust file conversion design pattern separated from specific implementation details. While many existing solutions focus on providing direct conversion implementations, these packages provide a structured approach that facilitates easy extension, customization, and integration into various projects.</p> <p>In this blog post, we'll explore how to unlock the magic of data manipulation by streamlining file conversion operations using two powerful packages: <code>file_conv_framework</code> and <code>file_conv_scripts</code>.</p>"},{"location":"projects/introducing-two-new-packages-for-streamlining-file-conversions-in-python/#introducing-file_conv_framework","title":"Introducing <code>file_conv_framework</code>","text":"<p>The <code>file_conv_framework</code> package serves as a solid foundation for building robust file conversion utilities in Python. It offers a modular and extensible architecture designed to simplify the process of reading from and writing to various file formats. Let's delve into some of its key features:</p>"},{"location":"projects/introducing-two-new-packages-for-streamlining-file-conversions-in-python/#modular-inputoutput-handlers","title":"Modular Input/Output Handlers","text":"<p>The framework defines abstract base classes for file readers and writers, allowing for easy extension and customization. This modular approach ensures flexibility in handling different types of input and output files.</p>"},{"location":"projects/introducing-two-new-packages-for-streamlining-file-conversions-in-python/#support-for-various-file-formats","title":"Support for Various File Formats","text":"<p>With built-in support for common file formats such as text, CSV, JSON, XML, Excel, and image files, the framework caters to diverse conversion needs out of the box.</p>"},{"location":"projects/introducing-two-new-packages-for-streamlining-file-conversions-in-python/#mime-type-detection","title":"MIME Type Detection","text":"<p>The inclusion of a MIME type guesser utility enables automatic detection of file MIME types based on content, facilitating seamless conversion operations.</p>"},{"location":"projects/introducing-two-new-packages-for-streamlining-file-conversions-in-python/#exception-handling","title":"Exception Handling","text":"<p>Custom exceptions are implemented to handle errors related to unsupported file types, empty suffixes, file not found, and mismatches between file types, ensuring robust error management during conversions.</p>"},{"location":"projects/introducing-two-new-packages-for-streamlining-file-conversions-in-python/#base-converter-class","title":"Base Converter Class","text":"<p>The framework provides an abstract base class for implementing specific file converters, offering a standardized interface for conversion operations.</p>"},{"location":"projects/introducing-two-new-packages-for-streamlining-file-conversions-in-python/#resolved-input-file-representation","title":"Resolved Input File Representation","text":"<p>A class is introduced to represent input files with resolved file types, ensuring consistency and correctness in conversion tasks.</p> <p>To start using <code>file_conv_framework</code>, you can install it using pip from the Test PyPI repository:</p> <pre><code>pip install -i https://test.pypi.org/simple/ file-conv-framework\n</code></pre> <p>To illustrate the usage of <code>file_conv_framework</code>, let's consider an example of converting a CSV file to JSON:</p> <pre><code>from file_conv_framework.io_handler import CSVReader, JSONWriter\nfrom file_conv_framework.base_converter import BaseConverter, ResolvedInputFile\nfrom file_conv_framework.filetypes import FileType\n\nclass CSVToJSONConverter(BaseConverter):\n    file_reader = CSVReader()\n    file_writer = JSONWriter()\n\n    @classmethod\n    def _get_supported_input_type(cls) -&gt; FileType:\n        return FileType.CSV\n\n    @classmethod\n    def _get_supported_output_type(cls) -&gt; FileType:\n        return FileType.JSON\n\n    def _convert(self, input_path: Path, output_path: Path):\n        # Implement conversion logic from CSV to JSON\n        pass\n\n# Usage\ninput_file_path = \"input.csv\"\noutput_file_path = \"output.json\"\ninput_file = ResolvedInputFile(input_file_path)\noutput_file = ResolvedInputFile(output_file_path)\nconverter = CSVToJSONConverter(input_file, output_file)\nconverter.convert()\n</code></pre>"},{"location":"projects/introducing-two-new-packages-for-streamlining-file-conversions-in-python/#exploring-file_conv_scripts","title":"Exploring <code>file_conv_scripts</code>","text":"<p>Building upon the <code>file_conv_framework</code>, the <code>file_conv_scripts</code> package offers a collection of Python scripts tailored for common file format conversions. These scripts leverage the capabilities of the framework to provide convenient solutions for handling various data transformations. Let's take a closer look:</p>"},{"location":"projects/introducing-two-new-packages-for-streamlining-file-conversions-in-python/#extensive-conversion-support","title":"Extensive Conversion Support","text":"<p>The package includes scripts for converting between a wide range of file formats, including text, XML, JSON, CSV, and Excel. This broad support caters to diverse conversion needs across different domains.</p>"},{"location":"projects/introducing-two-new-packages-for-streamlining-file-conversions-in-python/#integration-with-file_conv_framework","title":"Integration with <code>file_conv_framework</code>","text":"<p>Utilizing classes from the <code>file_conv_framework</code> package for file I/O operations, MIME type detection, and exception handling ensures consistency and reliability in conversion tasks.</p>"},{"location":"projects/introducing-two-new-packages-for-streamlining-file-conversions-in-python/#command-line-interface","title":"Command-Line Interface","text":"<p>Each conversion script is equipped with a command-line interface, allowing users to specify input and output file paths, as well as input and output file types, for seamless execution of conversion tasks.</p>"},{"location":"projects/introducing-two-new-packages-for-streamlining-file-conversions-in-python/#extensibility","title":"Extensibility","text":"<p>The modular converter classes provided by <code>file_conv_scripts</code> make it easy to add support for additional file formats or customize existing conversion functionalities as per project requirements.</p> <p>To start using <code>file_conv_scripts</code>, you can clone the repository and set up the environment as follows:</p> <pre><code>git clone https://github.com/Hermann-web/file-converter\ncd file-converter/file-conv-framework\npython -m venv venv\nsource venv/bin/activate  # for Unix/Linux\n.\\venv\\Scripts\\activate   # for Windows\npip install -r requirements.txt\n</code></pre> <p>To demonstrate the usage of <code>file_conv_scripts</code>, let's consider an example of converting an XML file to JSON using the provided CLI:</p> <pre><code>python app.py input.xml -t XML -o output.json -ot JSON\n</code></pre>"},{"location":"projects/introducing-two-new-packages-for-streamlining-file-conversions-in-python/#upcoming-enhancements","title":"Upcoming Enhancements","text":"<p>In future iterations of <code>file_conv_scripts</code>, we plan to extend the conversion methods along with reader and writer classes. Additionally, contributions to ameliorate <code>file_conv_framework</code> are welcome. However, it's worth noting that the <code>file_conv_framework</code> repository can still be reused independently. For example, it can be utilized to create custom versions of <code>file_conv_scripts</code> or other file conversion utilities.</p>"},{"location":"projects/introducing-two-new-packages-for-streamlining-file-conversions-in-python/#conclusion","title":"Conclusion","text":"<p>Efficient handling of file conversion tasks is essential in various data-centric applications. With the <code>file_conv_framework</code> and <code>file_conv_scripts</code> packages, Python developers have powerful tools at their disposal to tackle such challenges effectively. Whether you're building custom conversion utilities or integrating conversion functionalities into larger projects, these packages provide a solid foundation for streamlining your workflow.</p> <p>To begin leveraging the capabilities of these packages, simply install them along with their dependencies using your preferred package manager. You can then explore the provided examples and documentation to kickstart your file conversion endeavors.</p> <p>Start exploring the world of file conversion with Python today and unlock new possibilities in data processing and manipulation!</p> <p>For more information and detailed usage instructions, please refer to the documentation and README files available in the respective package repositories:</p> <ul> <li><code>file_conv_framework</code></li> <li><code>file_conv_scripts</code></li> </ul> <p>Happy coding!</p>"},{"location":"projects/search-engine-for-domain-specific-french-users/","title":"Search Engine for domain specific french users","text":"<p>This tutorial is a work i've made as part of my internship at Safran. The objective is to create a search engine that take as input, a user query and return a shortlist of stored documents highly relevant to the search query. Parsing both the user query and the document in a same comparison space is a must. We use a projection in a vector space after common NLP tasks as tokenisation, stop word removal, lemmatization correction. Finally, unlike english as a standard for common NLP projects, we use tools that worked very well with french texts.</p> <p>In this post, we will be building a semantic documents search engine</p>"},{"location":"projects/search-engine-for-domain-specific-french-users/#prerequistes","title":"Prerequistes","text":"<ul> <li>Python &gt;=3.7</li> <li>NLTK</li> <li>Pandas</li> <li>Scikit-learn</li> </ul>"},{"location":"projects/search-engine-for-domain-specific-french-users/#imports","title":"Imports","text":"<pre><code>import re, json\nimport unicodedata, string\nimport time\nimport operator\nimport numpy as np \nimport pandas as pd\nfrom collections import Counter\n</code></pre> <pre><code>from collections import defaultdict\nimport nltk \nfrom nltk.tokenize import word_tokenize\nfrom nltk import pos_tag\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import wordnet as wn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n</code></pre> <pre><code>nltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('stopwords')\n</code></pre>"},{"location":"projects/search-engine-for-domain-specific-french-users/#data","title":"data","text":"<p>Files used in the notebook are stored in the folder data of the repository</p>"},{"location":"projects/search-engine-for-domain-specific-french-users/#1-creer-les-keywords-a-partir-dune-phrase","title":"**1: Cr\u00e9er les keywords \u00e0 partir d'une phrase","text":"<p>en se basant sur les mots d'un dictionnaire et un corpus de texte en passant par la tokenization, la correction, la lemmatization et le removeStopWords**</p>"},{"location":"projects/search-engine-for-domain-specific-french-users/#preprocessing","title":"preprocessing","text":"<pre><code>def get_dico():\n    textdir = \"liste.de.mots.francais.frgut_.txt\"\n    try:DICO = open(textdir,'r',encoding=\"utf-8\").read()\n    except: DICO = open(textdir,'r').read()\n\n    return DICO\n\n\ndef remove_accents(input_str):\n    \"\"\"This method removes all diacritic marks from the given string\"\"\"\n    norm_txt = unicodedata.normalize('NFD', input_str)\n    shaved = ''.join(c for c in norm_txt if not unicodedata.combining(c))\n    return unicodedata.normalize('NFC', shaved)\n\ndef clean_sentence(texte):\n    # Replace diacritics\n    texte = remove_accents(texte)\n    # Lowercase the document\n    texte = texte.lower()\n    # Remove Mentions\n    texte = re.sub(r'@\\w+', '', texte)\n    # Remove punctuations\n    texte = re.sub(r'[%s]' % re.escape(string.punctuation), ' ', texte)\n    # Remove the doubled space\n    texte = re.sub(r'\\s{2,}', ' ', texte)\n    #remove whitespaces at the beginning and the end\n    texte = texte.strip()\n\n    return texte\n\n\ndef tokenize_sentence(texte):\n        #clean the sentence \n    texte = clean_sentence(texte)\n        #tokenize \n    liste_words = texte.split()\n        #return \n    return liste_words\n\ndef strip_apostrophe(liste_words):\n    get_radical = lambda word: word.split('\\'')[-1]\n    return list(map(get_radical,liste_words))\n\ndef pre_process(sentence):\n    #remove '_' from the sentence \n    sentence = sentence.replace('_','')\n\n    #get words fro the sentence \n    liste_words = tokenize_sentence(sentence)\n    #cut out 1 or 2 letters ones \n    liste_words = [elt for elt in liste_words if len(elt)&gt;2]\n    #prendre le radical apr\u00e8s l'apostrophe\n    liste_words = strip_apostrophe(liste_words)\n    print('\\nsentence to words : ',liste_words)\n    return liste_words\n</code></pre>"},{"location":"projects/search-engine-for-domain-specific-french-users/#correction-des-mots","title":"correction des mots","text":"<pre><code>def edits1(word):\n    \"All edits that are one edit away from `word`.\"\n    letters    = 'abcdefghijklmnopqrstuvwxyz'\n    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n    deletes    = [L + R[1:]               for L, R in splits if R]\n    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)&gt;1]\n    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n    inserts    = [L + c + R               for L, R in splits for c in letters]\n    return set(deletes + transposes + replaces + inserts)\n\ndef edits2(word): \n    \"All edits that are two edits away from `word`.\"\n    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n\ndef known(words): \n    \"The subset of `words` that appear in the dictionary of WORDS.\"\n    return set(w for w in words if w in WORDS)\n\ndef candidates(word): \n    \"Generate possible spelling corrections for word.\"\n    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n\n\n\n\ndef DICO_ET_CORRECTEUR():\n    \"cette fonction retourne la liste des mots de dictionnaire\"\n    DICO = get_dico()\n    WORDS = Counter(pre_process(DICO)) #Counter prends un str et retourne une sorte de liste enrichie\n    \"correction des mots \"\n    N = sum(WORDS.values())\n    P = lambda word: WORDS[word] / N #\"Probability of `word`.\"\n\n    correction = lambda word: max(candidates(word), key=P) #\"Most probable\n    return WORDS,correction\n\nWORDS,CORRECTION = DICO_ET_CORRECTEUR()\n</code></pre>"},{"location":"projects/search-engine-for-domain-specific-french-users/#stopwords-et-stemmingpremier-exemple","title":"stopwords et stemming(premier exemple)","text":"<pre><code>## stopwords #//https://www.ranks.nl/stopwords/french\nwith open('stp_words_.txt','r') as f:\n    STOPWORDS = f.read()\n\n## bdd de stemmer\nwith open(\"sample_.json\",'r',encoding='cp1252') as json_file:\n    #json_file.seek(0)\n    LISTE = json.load(json_file)\nmy_stemmer = lambda word: LISTE[word] if word in LISTE else word\n</code></pre>"},{"location":"projects/search-engine-for-domain-specific-french-users/#fonction-sentence_to_correct_words","title":"fonction: SENTENCE_TO_CORRECT_WORDS","text":"<pre><code>def SENTENCE_TO_CORRECT_WORDS(sentence):\n    \"cette fonction retourne la liste des mots du user\"\n    print('\\n------------pre_process--------\\n')\n    liste_words = pre_process(sentence)\n    print(liste_words)\n    print('\\n------------correction--------\\n')\n    liste_words = list(map(CORRECTION,liste_words))\n    print(liste_words)\n    print('\\n------------stemming--------\\n')\n    liste_words = list(map(my_stemmer,liste_words))\n    print(liste_words)\n    print('\\n------------remove stop-words--------\\n')\n    liste_words = [elt for elt in liste_words if elt not in STOPWORDS]\n    print(liste_words)\n    print('\\n-------------------------------------\\n')\n    return liste_words\n</code></pre> SENTENCE_TO_CORRECT_WORDS usage <pre><code>SENTENCE_TO_CORRECT_WORDS('La PR reste au statut \u00ab\\xa0Approuve(e)\\xa0\u00bb et il n\u2019y a pas de commande\\\"\\'')\n</code></pre> <pre><code>------------pre_process--------\n['reste', 'statut', 'approuve', 'n\u2019y', 'pas', 'commande']\n\n------------correction--------\n['reste', 'statut', 'approuve', 'non', 'pas', 'commande']\n\n------------stemming--------\n['rester', 'statut', 'approuver', 'non', 'pas', 'commander']\n\n------------remove stop-words--------\n['rester', 'statut', 'approuver', 'commander']\n\n-------------------------------------\n['rester', 'statut', 'approuver', 'commander']\n</code></pre>"},{"location":"projects/search-engine-for-domain-specific-french-users/#create-dataset","title":"Create dataset","text":"Create dataset <pre><code>def open_file(textdir):\nfound = False\ntry:texte = open(textdir,'r',encoding=\"utf-8\").read();found=True\nexcept:pass\ntry: texte = open(textdir,'r').read();found=True \nexcept: pass\nif not found:\n    texte = open(textdir,'r',encoding='cp1252').read();found=True\nreturn  texte\ndef add_col(df_news,titre,keywords):\nreturn df_news.append(dict(zip(df_news.columns,[titre, keywords])), ignore_index=True)\n\nliste_pb = [elt for elt in open_file('liste_pb_.txt').split('\\n') if elt]\ndf_new = df_news.drop(df_news.index)\nfor i,titre in enumerate(liste_pb):\nkeywords = ','.join(SENTENCE_TO_CORRECT_WORDS(titre))\ndf_new = add_col(df_new,titre,keywords)\ndf_new.head()\n</code></pre> Output <pre><code>                    Subject                                    Clean_Keyword\n0  Message d'erreur : \"Le fournisseur ARIBA n'exi...  message,erreur,fournisseur,aria,exister\n1  Message d'erreur : \"Commande d\u2019article non aut...  message,erreur,commander,article,autoriser,oto\n2  Message d'erreur : \"Statut utilisateur FERM ac...  message,erreur,statut,utilisateur,actif,oto\n3  Message d'erreur : \"Statut systeme TCLO actif ...  message,erreur,statut,systeme,col,actif,nord\n4  Message d'erreur \"___ Cost center change could...  message,erreur,coat,centrer,changer,cold,affecter\n5  Messaeg d'erreur \"___ OTP change could not be ...  message,erreur,otp,changer,cold,affecter\n6  Messaeg d'erreur \"Entrez Centre de couts\"          message,erreur,entrer,centrer,cout\n7  Message d'erreur \"Indiquez une seule imputatio...  message,erreur,indiquer,imputation,statistique\n8  Message d'erreur \"Imputations CO ont des centr...  message,erreur,imputation,centrer,profit\n9  Message d'erreur \"Poste ___ Ordre ___ depassem...  message,erreur,poster,ordre,depassement,budget\n10  Message d'erreur \"Entrez une quantite de comma...  message,erreur,entrer,quantite,commander\n11  Message d'erreur \"Indiquez la quantite\"          message,erreur,indiquer,quantite\n12  Message d'erreur \"Le prix net doit etre superi...  message,erreur,prix,net,superieur\n...  ...  ...\n...  ...  ...\n...  ...  ...\n57  UO4-5 Commande | Envoi d'une commande manuelle  uo4,commander,envoi,commander,manuel\n58  UO5-4 Reception | Anomalie workflow  uo5,reception,anomalie,workflow\n59  UO5-1 Reception | Modification(s) de reception(s)  uo5,reception,modification,reception\n60  UO5-2 Reception | Annulation(s) de reception(s)  uo5,reception,annulation,reception\n61  UO5-3 Reception | Forcer la reception  uo5,reception,forcer,reception\n62  UO3-5 Demande d'achat | Demande de support cre...  uo3,demander,achat,demander,support,creation\n63  UO3-6 Demande d'achat | Demande de support mod...  uo3,demander,achat,demander,support,modification\n64  UO3-7 Demande d'achat | Demande de support ann...  uo3,demander,achat,demander,support,annulation\n65  UO4-2 Commande | Demande de support modificati...  uo4,commander,demander,support,modification,co...\n</code></pre>"},{"location":"projects/search-engine-for-domain-specific-french-users/#tokenize-and-stemmingsecond-exemple","title":"tokenize and stemming(second exemple)","text":"<pre><code># WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\ndef wordLemmatizer(data,colname):\n    tag_map = defaultdict(lambda : wn.NOUN)\n    tag_map['J'] = wn.ADJ\n    tag_map['V'] = wn.VERB\n    tag_map['R'] = wn.ADV\n    file_clean_k =pd.DataFrame()\n    for index,entry in enumerate(data):\n\n        # Declaring Empty List to store the words that follow the rules for this step\n        Final_words = []\n        # Initializing WordNetLemmatizer()\n        word_Lemmatized = WordNetLemmatizer()\n        # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n        for word, tag in pos_tag(entry):\n            # Below condition is to check for Stop words and consider only alphabets\n            if len(word)&gt;1 and word not in stopwords.words('french') and word.isalpha():\n                word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n                Final_words.append(word_Final)\n            # The final processed set of words for each iteration will be stored in 'text_final'\n                file_clean_k.loc[index,colname] = str(Final_words)\n                file_clean_k.loc[index,colname] = str(Final_words)\n                file_clean_k=file_clean_k.replace(to_replace =\"\\[.\", value = '', regex = True)\n                file_clean_k=file_clean_k.replace(to_replace =\"'\", value = '', regex = True)\n                file_clean_k=file_clean_k.replace(to_replace =\" \", value = '', regex = True)\n                file_clean_k=file_clean_k.replace(to_replace ='\\]', value = '', regex = True)\n\n    return file_clean_k\n\n\ndef wordLemmatizer_(sentence):\n    #prendre une phrase que retourner un str (les mots sont separes par des ,)\n    preprocessed_query = preprocessed_query = re.sub(\"\\W+\", \" \", sentence).strip()\n    tokens = word_tokenize(str(preprocessed_query))\n    q_df = pd.DataFrame(columns=['q_clean'])\n    idx = 0\n    colname = 'keyword_final'\n    q_df.loc[idx,'q_clean'] =tokens\n    print('\\n\\n---inputtoken');print(q_df.q_clean)\n    print('\\n\\n---outputlemma');print(wordLemmatizer(q_df.q_clean,colname).loc[idx,colname])\n    return wordLemmatizer(q_df.q_clean,colname).loc[idx,colname]\n</code></pre>"},{"location":"projects/search-engine-for-domain-specific-french-users/#2-trouver-la-meilleure-phrase-dans-une-liste-de-phrase","title":"2: trouver la meilleure phrase dans une liste de phrase","text":""},{"location":"projects/search-engine-for-domain-specific-french-users/#method-tf-idf","title":"method: TF-Idf","text":"<p>TfIdf stands for: Term Frequency Inverse Document Frequency</p> <p>In order to compare the user input to existing sentence in database, we will go throught two process</p> <ul> <li>Normalize database: Apply the pre-processing method to all sentences in the database. We then have, for each sentence, a list of keywords</li> <li>For each keyword kw for each sentence st, we compute,</li> <li>\\(frqc(word,sentence)\\) : occurrence of the keyword word in the sentence</li> <li>\\(doc\\_frqc(word)\\): number of sentences where the word appears</li> <li>\\(N\\) = Number of sentences</li> </ul> <p>\\(tf(wd,stc) =  \\frac {frqc(wd,stc)}{ \\sum_{stc} frqc(wd,stc) }\\)</p> <p>\\(idf(wd) =  \\log(\\frac{N}{doc\\_frqc(wd)})\\)</p> <p>\\(tfidf(wd,stc) = tf(wd,stc) *idf(wd)\\)</p> Example <ul> <li>st1: The computer is down</li> <li>st2: We need to change the computers</li> <li>st3: Changements have to be handle by the IT</li> </ul> <p>keywords per sentence</p> <pre><code>st1: [computer , down]\nst2: [need, change, computer]\nst3: [change, handle, IT]\n</code></pre> <p>vocabulary: [computer , down, need, change, handle, IT]</p> tf sentence1 sentence2 sentence3 computer \u00bd \u00bd 0 down 1 0 0 need 0 1 0 change 0 \u00bd \u00bd handle 0 0 1 IT 0 0 1 <p>idf values for the keywords</p> <p>N =number_of_sentences =  3</p> idf computer log(3/2) down log(3/1) need log(3/1) change log(3/2) handle log(3/1) IT log(3/1) <p>example for sentence 2: computing of the keywords tfidf values</p> \\[ tfidf('computer') = tf('computer', sentence2)*idf('computer') = 1/2 * log(3/2)\\\\ tfidf('down') = 0 * log(3/1)\\\\ tfidf('need') = 1 * log(3/1)\\\\ tfidf('change') = 1/2 * log(3/2)\\\\ tfidf('handle') = 0 * log(3/1)\\\\ tfidf('IT') = 0 * log(3/1)\\\\ \\] <p>vectorisation of the sentence 2</p> <pre><code>sentence2 &lt;==&gt; [ 0.5 * log(3 / 2), 0, 1 * log(3 / 2), 0.5 *  log(3 /2) , 0, 0]                                      \n</code></pre> <p>vectorisation of the sentences</p> \\[ sentence1 &lt;==&gt; [\\ 0.5*log(3/2),\\ log(3/1),\\ 0 ,\\ 0,\\ 0]\\\\ sentence2 &lt;==&gt; [\\ 0.5*log(3/2),\\ 0, 1*log(3/2),\\ 0.5* log(3/2) ,\\ 0,\\ 0]\\\\ sentence3 &lt;==&gt; [\\ 0, \\ 0, \\ 0,\\ 0.5 * 1*log(3/2),\\  log(3/1),\\ 1*log(3/1)] \\] <p>similarities between the user input and the sentences</p> <ul> <li>user input: The IT have replaced all of the computers</li> <li>keywords: [ 'IT', 'all',  'computer']</li> <li>keywords found in dictionnary: [ 'IT','computer']</li> <li>vectorization: [1,0,0,0,1]</li> </ul> <p>scores</p> \\[ sentence1: tfidf(sentence1)*vector =  [\\ 0.5*log(3/2),\\ log(3/1),\\ 0 ,\\ 0,\\ 0] *[1,0,0,0,1] =  0.5*log(3/2) \\\\ sentence1:0.5*log(3/2)\\\\ sentence2:  0.5*log(3/2)\\\\ sentence3: log(3/1) \\]"},{"location":"projects/search-engine-for-domain-specific-french-users/#fonction-cosine_similarity_t","title":"fonction: cosine_similarity_T","text":"<pre><code>def init(df_news):\n  ##  Create Vocabulary\n  vocabulary = set()\n  for doc in df_news.Clean_Keyword:\n      vocabulary.update(doc.split(','))\n  vocabulary = list(vocabulary)# Intializating the tfIdf model\n  tfidf = TfidfVectorizer(vocabulary=vocabulary)# Fit the TfIdf model\n  tfidf.fit(df_news.Clean_Keyword)# Transform the TfIdf model\n  tfidf_tran=tfidf.transform(df_news.Clean_Keyword)\n  globals()['vocabulary'],globals()['tfidf'],globals()['tfidf_tran'] = vocabulary,tfidf,tfidf_tran\n</code></pre>"},{"location":"projects/search-engine-for-domain-specific-french-users/#create-a-vector-for-querysearch-keywords","title":"Create a vector for Query/search keywords","text":"<pre><code>def gen_vector_T(tokens,df_news,vocabulary,tfidf,tfidf_tran):\n  Q = np.zeros((len(vocabulary)))    \n  x= tfidf.transform(tokens)\n  #print(tokens[0].split(','))\n  #print(keywords)\n  for token in tokens[0].split(','):\n\n      try:\n          ind = vocabulary.index(token)\n          Q[ind]  = x[0, tfidf.vocabulary_[token]]\n          print(token,':',ind)\n      except:\n          print(token,':','not found')\n          pass\n  return Q\n</code></pre>"},{"location":"projects/search-engine-for-domain-specific-french-users/#cosine-similarity-function","title":"Cosine Similarity function","text":"<pre><code>def cosine_sim(a, b):\n    if not np.linalg.norm(a) and not np.linalg.norm(b): return -3\n    if not np.linalg.norm(a):return -1\n    if not np.linalg.norm(b):return -2\n    cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n    return cos_sim   \n\ndef cosine_similarity_T(k, query,df_news,vocabulary=None,tfidf=None,tfidf_tran=None,mine=True):\n    try:\n      vocabulary = globals()['vocabulary']\n      tfidf = globals()['tfidf']\n      tfidf_tran = globals()['tfidf_tran']\n    except:\n      print('up exception')\n      init(df_news)\n    q_df = pd.DataFrame(columns=['q_clean'])\n    if mine:q_df.loc[0,'q_clean'] =','.join(SENTENCE_TO_CORRECT_WORDS(query))\n    else:q_df.loc[0,'q_clean'] = wordLemmatizer_(query)\n\n\n    print('\\n\\n---q_df');print(q_df)\n\n    print('\\n\\n')\n    d_cosines = []\n    query_vector = gen_vector_T(q_df['q_clean'],df_news,vocabulary,tfidf,tfidf_tran )\n    for d in tfidf_tran.A:\n        d_cosines.append(cosine_sim(query_vector, d ))\n\n    out = np.array(d_cosines).argsort()[-k:][::-1]\n    #print(\"\")\n    d_cosines.sort()\n    a = pd.DataFrame()\n    for i,index in enumerate(out):\n        a.loc[i,'index'] = str(index)\n        a.loc[i,'Subject'] = df_news['Subject'][index]\n    for j,simScore in enumerate(d_cosines[-k:][::-1]):\n        a.loc[j,'Score'] = simScore\n    return a\n</code></pre>"},{"location":"projects/search-engine-for-domain-specific-french-users/#test-cosine_similarity_t","title":"Test: cosine_similarity_T","text":"<pre><code>def test(data,sentence,init_=False,mine=True):\n  if not init_:\n    deb = time.time();print('\\n\\n## ## ## ## ## #')\n    init(df_news)\n    print('\\n## ## ## ## ## #temps init: ', time.time()-deb)\n  deb = time.time();print('\\n\\n## ## ## ## ## #')\n  print(cosine_similarity_T(10, sentence,df_news))\n  print('\\n## ## ## ## ## #temps methode 1: ', time.time()-deb)\nsentence = 'Message d\\'erreur \\\"La qte livree est differente de la qte facturee ; fonction impossible\"'\nsentence = 'erreur de conversion'\nsentence = 'message d\\'erreur'\nsentence = \"groupe d'acheteurs non d\u00e9fini\"\nsentence = \"UO4\"\nsentence = \"le fournisseur MDM n'existe pas\"\ninit(df_new) \n\ncosine_similarity_T(10,sentence,df_new )\n</code></pre> output <pre><code>------------pre_process--------\n['fournisseur', 'mdm', 'existe', 'pas']\n\n------------correction--------\n['fournisseur', 'mdm', 'existe', 'pas']\n\n------------stemming--------\n['fournisseur', 'mdm', 'exister', 'pas']\n\n------------remove stop-words--------\n['fournisseur', 'mdm', 'exister']\n\n-------------------------------------\n\n    index                   Subject                           Score\n0  19  Message d'erreur \"Le fournisseur MDM___ n\u2019exis...  0.781490\n1  0  Message d'erreur : \"Le fournisseur ARIBA n'exi...  0.600296\n2  20  Message d'erreur \"Le fournisseur MDM___ est bl...  0.587467\n3  14  Message d'erreur \"Le centre de profit __ n'exi...  0.236420\n4  33  Message d'erreur \"Il existe des factures pour ...  0.214371\n5  53  Message d'erreur \"Fournisseur non present dans...  0.142208\n6  18  Message d'erreur \"Validation ___ : le compte _...  0.000000\n7  30  Message d'erreur \"Renseigner correctement le d...  0.000000\n8  29  Message d'erreur \"Article ___ non gere dans la...  0.000000\n9  28  Message d'erreur \"Fonctions oblig. Suivantes n...  0.000000\n...  ...  ...\n</code></pre>"},{"location":"projects/search-engine-for-domain-specific-french-users/#conclusion-and-discussions","title":"Conclusion and Discussions","text":"<ul> <li>The system is very fast and reliable</li> <li>The shortlist, after tests is highly relevant to the user query with the best answer, appearing either first or second</li> <li>Some methods employed must be adapted for english or other languages different of french</li> </ul>"},{"location":"projects/js-course/","title":"The JavaScript Edge: Intermediate to Advanced Proficiency","text":"<ul> <li>JavaScript Fundamentals: A Beginner's Guide to Essential Concepts and Best Practices</li> <li>Intermediate JavaScript Concepts: Bridging Theory and Practice</li> </ul>"},{"location":"projects/js-course/javascript-concepts-intermediate/","title":"Intermediate JavaScript Concepts: Bridging Theory and Practice","text":""},{"location":"projects/js-course/javascript-concepts-intermediate/#introduction","title":"Introduction","text":"<p>Ready to take your JavaScript skills beyond the basics and delve into intermediate concepts?</p> <p>Embark on a journey into the realm of intermediate JavaScript, where theory meets practice and foundational knowledge evolves into practical application. This guide is tailored for developers seeking to expand their understanding and proficiency in JavaScript, bridging the gap between beginner and advanced levels.</p>"},{"location":"projects/js-course/javascript-concepts-intermediate/#overview","title":"Overview","text":"<p>In this document, we will cover:</p> <ul> <li>Intermediate data manipulation techniques for primitive and reference types</li> <li>Asynchronous programming patterns and their implementation in JavaScript</li> <li>Functional programming concepts and their relevance in modern JavaScript development</li> <li>Object-oriented programming principles applied to JavaScript</li> <li>Best practices for writing maintainable and scalable JavaScript code</li> <li>Testing methodologies and debugging strategies for intermediate-level applications</li> </ul> <p>By exploring these topics, you'll enhance your JavaScript proficiency and gain insights into the theoretical foundations and practical applications of intermediate-level concepts. Whether you're transitioning from beginner to intermediate or seeking to deepen your existing knowledge, this guide will equip you with the skills to tackle more complex JavaScript challenges with confidence.</p> <p>Ready to elevate your JavaScript skills to the next level? Let's dive in!</p>"},{"location":"projects/js-course/javascript-concepts-intermediate/#objects-beyond-simple-dictionaries","title":"Objects: Beyond Simple Dictionaries","text":"<p>In JavaScript, objects serve as fundamental data structures, akin to dictionaries in other programming languages. But what exactly are objects? Think of them as containers that can hold various pieces of related information. Each piece of information, called a property, consists of a key-value pair. For example, an object representing a book might have properties like \"title,\" \"author,\" and \"number of pages.\"</p> <p>In web development, JavaScript objects serve as fundamental data structures, allowing developers to represent real-world entities and their attributes. Let's consider an example of a book object from a fictional online bookstore:</p> <pre><code>const book = {\n    title: \"The Great Gatsby\",\n    author: \"F. Scott Fitzgerald\",\n    genre: \"Fiction\",\n    yearPublished: 1925,\n    price: 12.99,\n    available: true\n};\n\nconsole.log(book.title); // Output: The Great Gatsby\n</code></pre> <p>Understanding the syntax and manipulation of objects is crucial for dynamic web development. Objects allow us to interact with users, dynamically update web pages, and communicate with external services. In simpler terms, they enable the interactive and responsive behavior we see in modern websites.</p>"},{"location":"projects/js-course/javascript-concepts-intermediate/#integrated-development-environments-ides","title":"Integrated Development Environments (IDEs)","text":"<p>When writing JavaScript code, developers often rely on Integrated Development Environments (IDEs) like Visual Studio Code. An IDE is a software application that provides comprehensive tools for writing, testing, and debugging code. It offers features like code auto-completion, syntax highlighting, and integrated debugging tools, streamlining the development process.</p>"},{"location":"projects/js-course/javascript-concepts-intermediate/#servers-and-asynchronous-programming","title":"Servers and Asynchronous Programming","text":"<p>JavaScript's versatility extends beyond client-side scripting to server-side development. But what does \"server-side development\" mean? In web development, servers are computers that store and deliver web pages to users. Server-side JavaScript allows developers to write code that runs on these servers, enabling dynamic server-side interactions.</p> <p>Let's consider an example of asynchronous programming in a Node.js server application:</p> <pre><code>// Example of fetching data asynchronously from a server\napp.get('/api/books', async (req, res) =&gt; {\n    try {\n        const books = await Book.find(); // Asynchronous database query\n        res.json(books);\n    } catch (error) {\n        console.error(error);\n        res.status(500).json({ message: \"Server Error\" });\n    }\n});\n</code></pre> <p>One of the key features of JavaScript on the server-side is its support for asynchronous programming. Asynchronous programming allows tasks to be executed concurrently, without blocking the main execution thread. This means that while one task is being processed, other tasks can continue to run in the background. As a result, JavaScript applications can handle multiple operations simultaneously, enhancing responsiveness and efficiency.</p> <p>Asynchronous programming allows tasks to be executed concurrently, enhancing responsiveness and efficiency in JavaScript applications.</p>"},{"location":"projects/js-course/javascript-concepts-intermediate/#manipulating-values-primitives-vs-references","title":"Manipulating Values: Primitives vs. References","text":"<p>JavaScript distinguishes between primitive types, such as numbers and strings, and reference types, such as objects and arrays. But what's the difference between them?</p> <p>Primitive types represent simple data values and are stored directly in memory. When you assign a primitive value to a variable, you're essentially storing the value itself. For example, if you assign the number 42 to a variable, that variable will directly hold the value 42.</p> <p>Reference types, on the other hand, represent complex data structures and are stored as references in memory. When you assign a reference type to a variable, you're actually storing a reference to the memory location where the data is stored. This distinction becomes important when manipulating values, as changes to a reference type affect all variables that reference the same data.</p> <p>Let's illustrate the difference with an example:</p> <pre><code>let x = 10; // Primitive type (number)\nlet y = { name: 'John' }; // Reference type (object)\n\nlet a = x; // 'a' holds the value of 'x' (copying)\nlet b = y; // 'b' holds a reference to the same object as 'y'\n\na = 20; // Modifying 'a' does not affect 'x'\nb.name = 'Jane'; // Modifying 'b' affects the original object 'y'\n\nconsole.log(x, y); // Output: 10 { name: 'Jane' }\n</code></pre> <p>Understanding how JavaScript handles primitive and reference types is essential for effective value manipulation and data management.</p>"},{"location":"projects/js-course/javascript-concepts-intermediate/#functions-and-functional-programming","title":"Functions and Functional Programming","text":"<p>Functions play a central role in JavaScript, allowing developers to encapsulate and reuse code. But JavaScript also supports functional programming paradigms, which emphasize the use of functions as first-class citizens.</p> <p>In functional programming, functions are treated as values that can be passed around, assigned to variables, and returned from other functions.</p> <p>Let's see an example of a higher-order function:</p> <pre><code>// Example of a higher-order function\nfunction applyOperation(x, y, operation) {\n    return operation(x, y);\n}\n\nfunction add(x, y) {\n    return x + y;\n}\n\nconst result = applyOperation(5, 3, add);\nconsole.log(result); // Output: 8\n</code></pre> <p>This enables powerful programming techniques like higher-order functions, which take other functions as arguments or return them as results.</p> <p>Understanding functional programming concepts can lead to cleaner, more modular code that is easier to understand and maintain.</p>"},{"location":"projects/js-course/javascript-concepts-intermediate/#classes-and-object-oriented-programming","title":"Classes and Object-Oriented Programming","text":"<p>JavaScript supports object-oriented programming (OOP) through the use of classes and prototypes. But what exactly are classes and prototypes?</p> <p>Classes are blueprints for creating objects with similar properties and behaviors. They encapsulate data (in the form of properties) and behavior (in the form of methods) into a single entity. When you create an object from a class, you're essentially creating an instance of that class with its own set of properties and methods.</p> <p>Let's consider an example of creating objects using classes:</p> <pre><code>// Example of using classes for object creation\nclass Product {\n    constructor(name, price) {\n        this.name = name;\n        this.price = price;\n    }\n\n    display() {\n        console.log(`${this.name}: $${this.price}`);\n    }\n}\n\nconst product1 = new Product('Laptop', 999);\nproduct1.display(); // Output: Laptop: $999\n</code></pre> <p>Prototypes, on the other hand, are mechanisms for sharing behavior between objects. Every JavaScript object has a prototype, which serves as a template for the object's properties and methods. By leveraging prototypes, developers can create inheritance hierarchies and reuse code more efficiently.</p> <p>Classes provide a blueprint for creating objects with similar properties and behaviors, facilitating code organization and reusability.</p>"},{"location":"projects/js-course/javascript-concepts-intermediate/#clean-code-practices","title":"Clean Code Practices","text":"<p>Writing clean, readable, and maintainable code is essential for long-term project success. But what does \"clean code\" mean?</p> <p>Clean code adheres to principles like DRY (Don't Repeat Yourself) and separation of concerns. It is well-organized, with meaningful variable names, consistent formatting, and clear comments. By following clean code practices, developers can improve code quality, enhance collaboration, and reduce the likelihood of bugs.</p> <p>Adhering to clean code principles improves code quality, enhances collaboration, and reduces the likelihood of bugs.</p>"},{"location":"projects/js-course/javascript-concepts-intermediate/#testing-and-debugging-strategies","title":"Testing and Debugging Strategies","text":"<p>Robust testing and debugging practices are crucial for ensuring the reliability and stability of JavaScript applications. But what do these practices entail?</p> <p>Testing involves verifying that individual components of a system function correctly. This can include unit tests, which test individual functions or modules, as well as integration tests, which test the interaction between different components. Additionally, end-to-end tests simulate user interactions with the application to ensure that it behaves as expected.</p> Example of testing with Jest <pre><code>// Example of testing with Jest\nfunction sum(a, b) {\n    return a + b;\n}\n\nmodule.exports = sum;\n</code></pre> <p>Debugging, on the other hand, involves identifying and fixing errors in code. This can be done using tools like browser developer tools or integrated development environments (IDEs), which allow developers to inspect variables, set breakpoints, and step through code execution.</p> Example of debugging with Chrome DevTools <pre><code>// Example of debugging with Chrome DevTools\nfunction fetchData() {\n    return new Promise((resolve, reject) =&gt; {\n        setTimeout(() =&gt; {\n            resolve(\"Data fetched successfully!\");\n        }, 2000);\n    });\n}\n\nasync function getData() {\n    console.log(\"Fetching data...\");\n    const data = await fetchData();\n    console.log(data);\n}\n\ngetData(); // Debug in Chrome DevTools to analyze async behavior\n</code></pre> <p>Effective testing and debugging practices ensure the reliability and performance of JavaScript applications.</p>"},{"location":"projects/js-course/javascript-concepts-intermediate/#advanced-debugging-techniques","title":"Advanced Debugging Techniques","text":"<p>Mastering advanced debugging techniques can help developers diagnose and troubleshoot complex issues in JavaScript codebases. These techniques include setting conditional breakpoints, monitoring network activity, and profiling code performance.</p> <p>By leveraging debugging tools effectively, developers can identify bottlenecks, optimize code, and improve the overall quality of their applications.</p>"},{"location":"projects/js-course/javascript-concepts-intermediate/#conclusion","title":"Conclusion","text":"<p>As we explore these advanced JavaScript concepts, remember that practice and experimentation are key to mastering the language. By building projects, solving problems, and seeking feedback from peers, you'll continue to deepen your understanding and become a proficient JavaScript developer.</p>"},{"location":"projects/js-course/javascript-fundamentals-simplified/","title":"JavaScript Fundamentals: A Beginner's Guide to Essential Concepts and Best Practices","text":""},{"location":"projects/js-course/javascript-fundamentals-simplified/#introduction","title":"Introduction","text":"<p>Are you new to JavaScript or looking to refresh your understanding of its fundamental concepts?</p> <p>Dive into this beginner-friendly guide to explore the core principles and best practices in JavaScript!</p> <p>JavaScript (ES6), short for ECMAScript 6, is a major version of the JavaScript programming language. It introduces new features, syntax enhancements, and advanced programming concepts. This documentation is tailored for beginners and individuals seeking to recall the fundamentals of JavaScript. Let's embark on this learning journey together!</p>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#overview","title":"Overview","text":"<p>In this comprehensive guide, you will learn:</p> <ul> <li>Essential JavaScript data types, control structures, and functions</li> <li>Error handling techniques and best practices</li> <li>The use of classes, objects, arrays, and maps in JavaScript</li> <li>Comparison operators and logical operations</li> <li>Control structures including if/else, switch/case, for loops, and while loops</li> <li>Best practices for variable declaration and scope</li> <li>Tips for writing clean and maintainable JavaScript code</li> </ul> <p>Ready to elevate your JavaScript skills? Let's get started!</p>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#variables-and-constants","title":"Variables and Constants","text":""},{"location":"projects/js-course/javascript-fundamentals-simplified/#declaration-and-initialization","title":"Declaration and Initialization","text":"<ul> <li>Use <code>let</code> to declare variables whose value can change.</li> <li>Use <code>const</code> to declare constants whose value cannot change.</li> </ul> <p>Example:</p> <pre><code>let numberOfEpisodes = 9;\nconst pi = 3.14;\n</code></pre>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#data-types","title":"Data Types","text":"<ul> <li>Data types include <code>number</code>, <code>string</code>, <code>boolean</code>, <code>object</code>, <code>undefined</code>, <code>null</code>, etc.</li> <li>Use <code>typeof</code> to check the type of a variable.</li> </ul>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#numbers","title":"Numbers","text":"<p>In JavaScript, integers are represented as the <code>number</code> data type. You can declare a variable and assign it an integer value as follows:</p> <pre><code>let a = 15;\nconsole.log(typeof a);  // Displays \"number\"\n</code></pre>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#booleans","title":"Booleans","text":"<p>Booleans represent true or false values in JavaScript. You can declare a variable and assign it a boolean value like this:</p> <pre><code>let userIsSignedIn = true;\nconsole.log(typeof userIsSignedIn); // Displays \"boolean\"\n</code></pre>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#strings","title":"Strings","text":"<p>Strings are sequences of characters enclosed in single or double quotes. You can concatenate two strings using the <code>+</code> operator. Here's an example:</p> <pre><code>let nom = 'Ag';\nlet prenom = 'He';\nconsole.log(typeof nom); // Displays \"string\"\nconsole.log(nom + prenom); // Displays \"AgHe\"\n</code></pre>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#outputs","title":"Outputs","text":"<ul> <li>Use <code>console.log()</code> to display messages in the console.</li> <li>Use <code>alert()</code> to display messages in a dialog box.</li> </ul> <p>Example:</p> <pre><code>console.log(5);\nlet yyy = \"me\";\nconsole.log(\"retdy\" + \" \" + yyy);\nalert('some things'); // Display a message in a dialog box\n</code></pre>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#objects","title":"Objects","text":"<p>JavaScript objects behave similarly to dictionaries in Python.</p>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#syntax","title":"Syntax","text":"<p>Objects in JavaScript are declared using curly braces <code>{}</code> and consist of key-value pairs. Keys are always strings. Each value can be an instance of any other type.</p> <pre><code>let mybook = { \n    title: 'Allah is not obliged',\n    author: 'Un Mec',\n    numberOfPages: 200,\n    isAvailable: true\n};\n</code></pre>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#accessing-data","title":"Accessing Data","text":"<p>You can access data in an object using dot notation (<code>.</code>) or square bracket notation (<code>[]</code>). For example:</p>  using dot notation (<code>.</code>) using bracket notation (<code>[]</code>) <pre><code>let titre = mybook.title;\nlet auteur = mybook.author;\nlet isdisponible = mybook.isAvailable;\n</code></pre> <pre><code>let titre = mybook[\"title\"];\nlet auteur = mybook[\"author\"];\nlet isdisponible = mybook[\"isAvailable\"];\n</code></pre> <p>Note that the keys are case-sensitive. So <code>mybook.title</code> is not the same as <code>mybook.Title</code>.</p>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#classes","title":"Classes","text":"<p>In JavaScript, classes allow you to define blueprints for creating objects with properties and methods.</p>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#class-definition","title":"Class Definition","text":"<p>Classes are defined using the <code>class</code> keyword followed by the class name.</p> <pre><code>class Book {\n    constructor(title, author, numberOfPages) {\n        this.title = title;\n        this.author = author;\n        this.numberOfPages = numberOfPages;\n    }\n}\n</code></pre>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#creating-instances","title":"Creating Instances","text":"<p>You can create new instances of a class using the <code>new</code> keyword followed by the class name and passing the required parameters to the constructor.</p> <pre><code>let aBook = new Book(\"le moi int\u00e9rieur\", \"Hermann\", 250);\n</code></pre>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#methods","title":"Methods","text":"<p>Methods can be defined within the class using normal function syntax. Here's an example:</p> method example <pre><code>class BankAccount {\n    constructor(owner, balance) {\n        this.owner = owner;\n        this.balance = balance;\n    }\n\n    showBalance() {\n        console.log(\"Solde: \" + this.balance + \" EUR\");\n    }\n\n    deposit(amount) {\n        console.log(\"D\u00e9p\u00f4t de \" + amount + \" EUR\");\n        this.balance += amount;\n        this.showBalance();\n    }\n\n    withdraw(amount) {\n        if (amount &gt; this.balance) {\n            console.log(\"Retrait refus\u00e9 !\");\n        } else {\n            console.log(\"Retrait de \" + amount + \" EUR\");\n            this.balance -= amount;\n            this.showBalance();\n        }\n    }\n}\n</code></pre>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#instantiation-and-method-invocation","title":"Instantiation and Method Invocation","text":"<p>You can create an instance of a class and then call its methods as shown below:</p> <pre><code>const newAccount = new BankAccount(\"Will Alexander\", 500);\nnewAccount.showBalance(); // Prints \"Solde: 500 EUR\" to the console\n</code></pre> <p>This code creates a new bank account with an initial balance of 500 EUR and then displays its balance using the <code>showBalance</code> method.</p>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#arrays-and-maps","title":"Arrays and Maps","text":"<p>In JavaScript, arrays are used to store collections of elements. They are zero-indexed, meaning the index of the first element is 0.</p>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#array-definition","title":"Array Definition","text":"<p>Arrays can be defined without initialization, or directly initialized with elements.</p> <pre><code>// Definition without initialization\nlet myList;\n\n// Definition with direct initialization\nlet guests = [];\nlet invitedPeoples = [\"Sarah\", \"Jean-Pierre\", \"Claude\"];\n</code></pre>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#accessing-elements","title":"Accessing Elements","text":"<p>You can access elements in an array using square brackets and the index of the element. Remember that array indexing starts from 0.</p> <pre><code>let guest1 = invitedPeoples[0]; // Accesses the first element\nlet guest3 = invitedPeoples[2]; // Accesses the third element\n</code></pre>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#array-length","title":"Array Length","text":"<p>You can determine the length of an array using the <code>length</code> property.</p> <pre><code>console.log(invitedPeoples.length); // Prints the length of the array\n</code></pre>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#array-operations","title":"Array Operations","text":"<p>Arrays support various operations like adding elements, removing elements, and inserting elements.</p> <ul> <li>To add an element to the end of an array, you can use the <code>push</code> method.</li> <li>To remove the last element from an array, you can use the <code>pop</code> method.</li> <li>To add an element to the beginning of an array, you can use the <code>unshift</code> method.</li> </ul>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#maps","title":"Maps","text":"<p>Maps in JavaScript are similar to arrays but are unordered collections. They do not allow duplicates, and you can check if an element exists in a map.</p> <p>These are some basic operations you can perform with arrays in JavaScript.</p>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#comparison-operators","title":"Comparison Operators","text":"<p>Comparison operators in JavaScript allow you to compare values and determine the relationship between them. These operators are commonly used in conditional statements like <code>if</code> and loops.</p>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#common-comparison-operators","title":"Common Comparison Operators","text":"<ul> <li><code>&lt;</code>    Less than</li> <li><code>&lt;=</code>   Less than or equal to</li> <li><code>&gt;</code>    Greater than</li> <li><code>&gt;=</code>   Greater than or equal to</li> <li><code>==</code>   Equal to (checks value only)</li> <li><code>!=</code>   Not equal to (checks value only)</li> <li><code>===</code>  Equal to (checks value and type)</li> <li><code>!==</code>  Not equal to (checks value and type)</li> </ul>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#logical-operators","title":"Logical Operators","text":"<p>Logical operators allow you to combine multiple conditions in a single statement.</p> <ul> <li><code>&amp;&amp;</code>   Logical AND: Returns <code>true</code> if both conditions are true</li> <li><code>||</code>   Logical OR: Returns <code>true</code> if at least one condition is true</li> <li><code>!</code>    Logical NOT: Negates the result, returns <code>true</code> if the condition is false</li> </ul> <p>comparison evaluation</p> <pre><code>let a = 5;\nlet b = 10;\n\nif (a &lt; b) {\n    console.log(\"a is less than b\");\n} else {\n    console.log(\"a is greater than or equal to b\");\n}\n</code></pre> <p>In this example, the condition <code>a &lt; b</code> evaluates to <code>true</code>, so the message \"a is less than b\" will be logged to the console.</p>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#control-structures","title":"Control Structures","text":""},{"location":"projects/js-course/javascript-fundamentals-simplified/#ifelse","title":"If/else","text":"<p>In JavaScript, <code>if</code>, <code>else if</code>, and <code>else</code> statements are used to execute blocks of code based on conditions.</p>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#basic-structure","title":"Basic Structure","text":"<p>The basic structure of an <code>if</code> statement is as follows:</p> <pre><code>if (condition) {\n    // Code to execute if the condition is true\n} else {\n    // Code to execute if the condition is false\n}\n</code></pre> example of using an <code>if</code> statement <pre><code>let userLoggedIn = true;\n\nif (userLoggedIn) {\n    console.log(\"User logged in!\");\n} else {\n    console.log(\"Alert, intruder!\");\n}\n</code></pre> <p>In this example, if the <code>userLoggedIn</code> variable is <code>true</code>, the message \"User logged in!\" will be logged to the console; otherwise, \"Alert, intruder!\" will be logged.</p> multiple conditions with if / else if / else <p>You can also use <code>else if</code> statements to test multiple conditions. Here's an example:</p> <pre><code>if (numberOfGuests == numberOfSeats) {\n    // All seats are occupied\n} else if (numberOfGuests &lt; numberOfSeats) {\n    // Allow more guests\n} else {\n    // Do not allow new guests\n}\n</code></pre> <p>In this example, if the number of guests equals the number of seats, the first block of code will execute. If the number of guests is less than the number of seats, the second block of code will execute. Otherwise, the third block of code will execute.</p>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#conditions-evaluated-as-true-or-false","title":"Conditions Evaluated as True or False","text":"Conditions Evaluated as True Conditions Evaluated as False <p>Conditions that can be evaluated as true in an <code>if</code> statement include:</p> <ul> <li>Numbers that are not zero</li> <li>Strings that are not empty</li> <li>Boolean <code>true</code></li> <li>Objects (including arrays and functions)</li> </ul> <p>Conditions that are evaluated as false in an <code>if</code> statement include:</p> <ul> <li>Number <code>0</code></li> <li>Empty string <code>''</code></li> <li>Boolean <code>false</code></li> <li><code>null</code></li> <li><code>undefined</code></li> <li><code>NaN</code></li> </ul>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#switchcase","title":"Switch/Case","text":"<p>The <code>switch</code> statement in JavaScript allows you to execute different blocks of code based on different conditions. It's particularly useful when you have a single value that you want to compare to multiple possible variants.</p>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#basic-structure_1","title":"Basic Structure","text":"<p>The basic structure of a <code>switch</code> statement looks like this:</p> <pre><code>switch (expression) {\n    case value1:\n        // Code to execute if expression === value1\n        break;\n    case value2:\n        // Code to execute if expression === value2\n        break;\n    default:\n        // Code to execute if expression doesn't match any case\n}\n</code></pre> swich case example <pre><code>let guestType = \"star\";\nlet vipStatus;\n\nswitch (guestType) {\n    case \"artist\":\n        vipStatus = \"Normal\";\n        break;\n    case \"star\":\n        vipStatus = \"Important\";\n        break;\n    default:\n        vipStatus = \"None\";\n}\n\nconsole.log(\"VIP status:\", vipStatus);\n</code></pre> <p>In this example, if the <code>guestType</code> is \"artist\", the <code>vipStatus</code> will be set to \"Normal\". If the <code>guestType</code> is \"star\", the <code>vipStatus</code> will be set to \"Important\". Otherwise, the <code>vipStatus</code> will be set to \"None\".</p>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#handling-unknown-values","title":"Handling Unknown Values","text":"<p>The <code>default</code> case is used to handle values that don't match any of the specified cases. This is useful for providing a fallback option or handling unexpected input.</p> Importance of Break <p>It's important to include <code>break</code> statements after each case block to prevent fall-through behavior, where execution continues to the next case block regardless of whether the condition is met. Here's an example illustrating the importance of <code>break</code>:</p> <pre><code>let vipStatus = \"\";\nlet guest = {\n    name: \"Sarah Kate\",\n    age: 21,\n    ticket: true,\n    guestType: \"artist\"\n};\n\nswitch (guest.guestType) {\n    case \"artist\":\n        vipStatus = \"Normal\";\n    case \"star\":\n        vipStatus = \"Important\";\n        break;\n    case \"presidential\":\n        vipStatus = \"Mega-important\";\n        break;\n    default:\n        vipStatus = \"None\";\n}\n</code></pre> <p>In this example, the <code>vipStatus</code> variable is erroneously assigned \"Normal\" because the <code>break</code> statement is missing after the <code>\"artist\"</code> case. Without the <code>break</code>, execution falls through to the <code>\"star\"</code> case, causing <code>vipStatus</code> to be overwritten with \"Important\". To avoid this, ensure that each case block ends with a <code>break</code> statement.</p>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#for-loops","title":"For Loops","text":"<p>In JavaScript, <code>for</code> loops are used to iterate over elements in an array or perform a specific action a certain number of times.</p>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#basic-structure_2","title":"Basic Structure","text":"<p>The basic structure of a <code>for</code> loop is as follows:</p> <pre><code>for (initialization; condition; increment/decrement) {\n    // Code to execute for each iteration\n}\n</code></pre> <p>for loop: basic usage example</p> <pre><code>for (let i = 0; i &lt; numberOfPassengers; i++) {\n    console.log(\"Passenger boarded!\");\n}\n</code></pre>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#for-loop-on-arrays","title":"For Loop on Arrays","text":"<p>This section provides an overview of <code>for</code> loops in JavaScript, including examples of both <code>for...in</code> and <code>for...of</code> loops and their respective use cases</p> Example 1: Using <code>for...in</code> Loop <p>The <code>for...in</code> loop iterates over the enumerable properties of an object, such as the indices of an array. Here's an example:</p> <pre><code>const passengers = [\n    \"Will Alexander\",\n    \"Sarah Kate\",\n    \"Audrey Simon\",\n    \"Tao Perkington\"\n]\n\nfor (let i in passengers) {\n    console.log(\"Boarding passenger: \" + passengers[i]);\n}\n</code></pre> Example 2: Using <code>for...of</code> Loop <p>The <code>for...of</code> loop is used to iterate over iterable objects, such as arrays. It provides a more concise syntax compared to the <code>for...in</code> loop. Here's an example:</p> <pre><code>const passengers = [\n    \"Will Alexander\",\n    \"Sarah Kate\",\n    \"Audrey Simon\",\n    \"Tao Perkington\"\n]\n\nfor (let passenger of passengers) {\n    console.log(\"Boarding passenger: \" + passenger);\n}\n</code></pre> <p>In both examples, each passenger's name is logged to the console, indicating that they are boarding the vehicle or entering some other context.</p>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#while-loops","title":"While Loops","text":"<p>In JavaScript, <code>while</code> loops are used to execute a block of code repeatedly as long as a specified condition is true.</p>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#basic-structure_3","title":"Basic Structure","text":"<p>The basic structure of a <code>while</code> loop is as follows:</p> <pre><code>while (condition) {\n    // Code to execute as long as the condition is true\n}\n</code></pre> Example: Using a While Loop <p>Here's an example of using a <code>while</code> loop to repeatedly perform a task until a condition is no longer true:</p> <pre><code>let seatsLeft = 10;\nlet passengersStillToBoard = 8;\nlet passengersBoarded = 0;\n\nwhile (seatsLeft &gt; 0 &amp;&amp; passengersStillToBoard &gt; 0) {\n    passengersBoarded++; // A passenger boards\n    passengersStillToBoard--; // Decrease the number of passengers still to board\n    seatsLeft--; // Decrease the number of seats left\n}\n\nconsole.log(passengersBoarded); // Logs the total number of passengers boarded\n</code></pre> <p>In this example, the loop continues as long as there are seats available (<code>seatsLeft &gt; 0</code>) and passengers still to board (<code>passengersStillToBoard &gt; 0</code>). Each iteration of the loop represents a passenger boarding the vehicle or entering some other context.</p> <p>The loop terminates when either there are no more seats available or there are no more passengers to board.</p>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#error-handling","title":"Error Handling","text":"<p>In JavaScript, error handling is crucial for managing unexpected situations or errors that may occur during code execution. There are various types of errors, and JavaScript provides mechanisms like <code>try</code> and <code>catch</code> to handle them gracefully.</p> Types of Errors <ol> <li> <p>Syntax Errors: These errors occur when there is a mistake in the syntax of the code, such as missing semicolons <code>;</code>, brackets <code>{}</code>, or incorrect expressions.</p> </li> <li> <p>Logical Errors: Logical errors happen when the code executes but produces unexpected results due to incorrect logic or reasoning in the code.</p> </li> <li> <p>Runtime Errors: Runtime errors occur during the execution of the program, typically caused by factors such as incorrect user input, resource unavailability, or unexpected behavior of external dependencies.</p> </li> <li> <p>Reference Errors: Reference errors occur when trying to access variables or functions that are not declared or out of scope.</p> </li> <li> <p>Type Errors: Type errors occur when an operation is performed on a value of the wrong type, such as using a method on a non-object or passing incorrect arguments to a function.</p> </li> <li> <p>Range Errors: Range errors occur when trying to access an invalid index of an array or perform an invalid operation within a certain numeric range.</p> </li> </ol>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#exception-handling-with-trycatch","title":"Exception Handling with Try/Catch","text":"<p>JavaScript provides the <code>try</code> and <code>catch</code> blocks for handling exceptions and managing errors effectively.</p>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#basic-structure_4","title":"Basic Structure","text":"<pre><code>try {\n    // Code that may cause an error\n} catch (error) {\n    // Handling the error\n}\n</code></pre> <p>In this structure:</p> <ul> <li>The <code>try</code> block contains the code that might throw an error.</li> <li>If an error occurs within the <code>try</code> block, control is transferred to the <code>catch</code> block.</li> <li>The <code>catch</code> block is responsible for handling the error. It receives the error object as a parameter, which can be used to identify and respond to the error appropriately.</li> </ul> Example <pre><code>try {\n    // Attempting to execute code that may throw an error\n    let result = 10 / 0; // This will cause a division by zero error\n    console.log(result); // This line will not execute due to the error\n} catch (error) {\n    // Handling the error\n    console.error(\"An error occurred:\", error.message);\n}\n</code></pre> <p>In this example, if a division by zero error occurs within the <code>try</code> block, the control will be transferred to the <code>catch</code> block. The <code>catch</code> block then handles the error by logging a descriptive message to the console.</p> <p>Error handling with <code>try</code> and <code>catch</code> is an essential aspect of writing robust JavaScript code, ensuring that your applications can gracefully handle unexpected errors and provide a better user experience.</p>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#functions","title":"Functions","text":"<p>In JavaScript, functions are blocks of reusable code that can be invoked (called) to perform a specific task. They play a crucial role in organizing and structuring code, making it easier to manage and maintain.</p>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#basic-syntax","title":"Basic Syntax","text":"<p>Functions in JavaScript can be defined using different syntaxes, including arrow functions and traditional function declarations.</p> <code>Arrow Functions:</code> <code>Traditional Function Declarations:</code> <p>Arrow functions are a concise way to write functions in JavaScript. They are commonly used for short, single-expression functions.</p> <pre><code>const sum = (a, b) =&gt; {\n    return a + b;\n};\n</code></pre> <p>Traditional function declarations use the <code>function</code> keyword to define functions.</p> <pre><code>function sum(a, b) {\n    return a + b;\n}\n</code></pre>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#function-invocation","title":"Function Invocation","text":"<p>Once a function is defined, it can be invoked (called) by its name, followed by parentheses containing any arguments required by the function.</p>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#example","title":"Example","text":"<pre><code>const result = sum(3, 4);\nconsole.log(result); // Output: 7\n</code></pre> Example 1: Function to Calculate Sum <pre><code>const sum = (number1, number2) =&gt; {\n    const result = number1 + number2;\n    return result;\n}\n\nconsole.log(sum(4, 7)); // Output: 11\n</code></pre> <p>In this example, the <code>sum</code> function takes two parameters <code>number1</code> and <code>number2</code>, calculates their sum, and returns the result.</p> Example 2: Function to Calculate Average Rating <pre><code>const calculateAverageRating = (ratings) =&gt; {\n    if (ratings.length === 0) {\n        return 0;\n    }\n\n    let sum = 0;\n    for (let rating of ratings) {\n        sum += rating;\n    }\n\n    return sum / ratings.length;\n}\n\nconst tauRatings = [5, 4, 5, 5, 1, 2];\nconst colinRatings = [5, 5, 5, 4, 5];\n\nconst tauAverage = calculateAverageRating(tauRatings);\nconst colinAverage = calculateAverageRating(colinRatings);\n\nconsole.log(tauAverage); // Output: 3.6666666666666665\nconsole.log(colinAverage); // Output: 4.8\n</code></pre> <p>In this example, the <code>calculateAverageRating</code> function calculates the average rating based on the provided array of ratings.</p> <p>Additional Notes</p> <ul> <li>It's common to use arrow functions (<code>=&gt;</code>) for defining functions in modern JavaScript.</li> <li>Constants declared with <code>const</code> are used to define functions to prevent accidental reassignment.</li> <li>Functions can take parameters and return values, making them versatile and powerful tools for organizing code.</li> </ul> <p>Functions are essential for structuring JavaScript code and promoting code reuse and maintainability.</p>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#scope-of-variables","title":"Scope of Variables","text":"<p>In JavaScript, the scope of a variable determines where the variable is accessible within the code. Understanding variable scope is essential for writing maintainable and bug-free code.</p>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#variable-declaration","title":"Variable Declaration","text":"<p>When declaring variables in JavaScript, it's recommended to use <code>let</code> or <code>const</code> to define variables.</p> <ul> <li> <p><code>let</code>: Variables declared with <code>let</code> have block scope, meaning they are only accessible within the block (enclosed by <code>{}</code>) in which they are defined, as well as any nested blocks (e.g., inside an <code>if</code> statement or loop) within that block.</p> </li> <li> <p><code>const</code>: Constants declared with <code>const</code> also have block scope and cannot be reassigned. They follow the same scoping rules as variables declared with <code>let</code>.</p> </li> <li> <p><code>var</code>: Variables declared with <code>var</code> have function scope. This means they are accessible throughout the entire function in which they are defined, regardless of block boundaries.</p> </li> </ul> Example <pre><code>{\n    let localVar = 'I am a local variable';\n    console.log(localVar); // Output: 'I am a local variable'\n}\n\nconsole.log(localVar); // Throws ReferenceError: localVar is not defined\n</code></pre> <p>In this example, the variable <code>localVar</code> is declared using <code>let</code> inside a block. It is accessible within that block but not outside of it. Attempting to access <code>localVar</code> outside the block results in a <code>ReferenceError</code>.</p> Avoid using <code>var</code> for variable declaration whenever possible <p>Using <code>let</code> and <code>const</code> for variable declaration helps prevent accidental variable hoisting and unintended side effects. It also promotes better code readability and maintenance by clearly defining the scope of variables.</p> <p>Avoid using <code>var</code> for variable declaration whenever possible, as it can lead to unexpected behavior due to its function scope and variable hoisting characteristics.</p> <p>Understanding variable scope is crucial for writing clean, predictable, and bug-free JavaScript code.</p> <p>Variables declared within a function are only accessible within that function, unless they are declared using the <code>var</code> keyword. Using <code>let</code> or <code>const</code> ensures that variables have block scope, making them accessible only within the block they are defined in.</p> <p>Best Practices</p> <ul> <li>Use meaningful variable names.</li> <li>Indent your code properly to make it readable.</li> <li>Comment your code to explain its functionality.</li> <li>Avoid ambiguous variable names.</li> </ul> <p>Additional Notes</p> <ul> <li>Remember to use semicolons to terminate statements.</li> <li>Use <code>{}</code> to define code blocks.</li> <li>Be mindful of the scope of variables when using <code>let</code>, <code>const</code>, and <code>var</code>.</li> <li>Always handle exceptions to prevent unexpected behavior.</li> <li>Utilize console methods such as <code>console.error()</code> for error messages.</li> </ul> Resources <p>Here are some useful resources to enhance your JavaScript skills and productivity:</p> <ul> <li> <p>ECMAScript Compatibility Table: A comprehensive table detailing the compatibility of various ECMAScript features across different JavaScript engines and environments.</p> </li> <li> <p>JS Bin: An online tool for quickly experimenting with and testing JavaScript code snippets. It provides a live-coding environment with a built-in console for immediate feedback.</p> </li> <li> <p>W3Schools JavaScript Tutorial: W3Schools offers a comprehensive and beginner-friendly JavaScript tutorial covering all fundamental concepts, syntax, and features of the language.</p> </li> <li> <p>OpenClassrooms JavaScript Course (English) or in French: OpenClassrooms provides interactive JavaScript courses suitable for beginners and intermediate learners. These courses cover topics ranging from basic syntax to advanced JavaScript programming techniques.</p> </li> </ul> <p>These resources offer valuable insights, tutorials, and tools to help you master JavaScript programming and become a proficient developer.</p>"},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/category/blog/","title":"Blog","text":""},{"location":"blog/category/security/","title":"Security","text":""},{"location":"blog/category/authentication/","title":"Authentication","text":""},{"location":"blog/category/best-practices/","title":"Best Practices","text":""},{"location":"blog/category/osx/","title":"OSX","text":""},{"location":"blog/category/docker/","title":"docker","text":""},{"location":"blog/category/containers/","title":"containers","text":""},{"location":"blog/category/python/","title":"python","text":""},{"location":"blog/category/packaging/","title":"packaging","text":""},{"location":"blog/category/linux/","title":"linux","text":""},{"location":"blog/category/rdp/","title":"RDP","text":""},{"location":"blog/category/windows/","title":"windows","text":""},{"location":"blog/category/poetry/","title":"poetry","text":""},{"location":"blog/category/dependency-management/","title":"dependency-management","text":""},{"location":"blog/category/frameworks/","title":"frameworks","text":""},{"location":"blog/category/web/","title":"web","text":""},{"location":"blog/category/fullstack/","title":"fullstack","text":""},{"location":"blog/category/laravel/","title":"laravel","text":""},{"location":"blog/category/pwa/","title":"pwa","text":""},{"location":"blog/category/deployment/","title":"deployment","text":""},{"location":"blog/category/php/","title":"php","text":""},{"location":"blog/category/programming/","title":"programming","text":""},{"location":"blog/category/code-quality/","title":"code-quality","text":""},{"location":"blog/category/tools-comparison/","title":"tools-comparison","text":""},{"location":"blog/category/logging/","title":"logging","text":""},{"location":"blog/category/package-manager/","title":"package-manager","text":""},{"location":"blog/category/project-management/","title":"project-management","text":""},{"location":"blog/category/frontend/","title":"frontend","text":""},{"location":"blog/category/github-pages/","title":"github-pages","text":""},{"location":"blog/category/domain-migration/","title":"domain-migration","text":""},{"location":"blog/category/flask/","title":"flask","text":""},{"location":"blog/category/troubleshooting/","title":"troubleshooting","text":""},{"location":"blog/category/version-control/","title":"version-control","text":""},{"location":"blog/category/git/","title":"git","text":""},{"location":"blog/category/nodejs/","title":"nodejs","text":""},{"location":"blog/category/beginners/","title":"beginners","text":""},{"location":"blog/category/microservices/","title":"microservices","text":""},{"location":"blog/category/gprc/","title":"gprc","text":""},{"location":"blog/category/networking/","title":"networking","text":""},{"location":"blog/category/devops/","title":"devops","text":""},{"location":"blog/category/command-line/","title":"command-line","text":""},{"location":"blog/category/file-handling/","title":"file-handling","text":""},{"location":"blog/category/mkdocs/","title":"mkdocs","text":""},{"location":"blog/category/documentation/","title":"documentation","text":""},{"location":"blog/category/remote-access/","title":"remote-access","text":""},{"location":"blog/category/ssh/","title":"ssh","text":""},{"location":"blog/category/mysql/","title":"mysql","text":""},{"location":"blog/category/recovery/","title":"recovery","text":""},{"location":"blog/category/data/","title":"data","text":""},{"location":"blog/category/huggingface/","title":"huggingface","text":""},{"location":"blog/category/streamlit/","title":"streamlit","text":""},{"location":"blog/category/sgbd/","title":"sgbd","text":""},{"location":"blog/category/mongodb/","title":"mongodb","text":""},{"location":"blog/category/conversion-tools/","title":"conversion-tools","text":""},{"location":"blog/category/markdown/","title":"markdown","text":""},{"location":"blog/category/data-visualization/","title":"data-visualization","text":""},{"location":"blog/page/2/","title":"Blog","text":""},{"location":"blog/page/3/","title":"Blog","text":""},{"location":"blog/page/4/","title":"Blog","text":""},{"location":"blog/archive/2024/page/2/","title":"2024","text":""},{"location":"blog/archive/2023/page/2/","title":"2023","text":""},{"location":"blog/archive/2023/page/3/","title":"2023","text":""},{"location":"blog/category/beginners/page/2/","title":"beginners","text":""},{"location":"blog/category/python/page/2/","title":"python","text":""}]}