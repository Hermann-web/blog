{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Personal Website","text":"<p>Welcome to Material for MkDocs.</p>"},{"location":"license/","title":"License","text":"<p>MIT License</p> <p>Copyright \u00a9 2016-2023 Martin Donath</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/a-roadmap-for-web-developper/","title":"A Roadmap for Web Development: Lessons and Stories","text":""},{"location":"blog/a-roadmap-for-web-developper/#chapter-1-html-and-css-crafting-digital-experiences","title":"Chapter 1: HTML and CSS - Crafting Digital Experiences","text":"<p>In the vast landscape of web development, the journey often starts with understanding the language of the web: HTML and CSS. It's more than just syntax and techniques; it's the brush and canvas, where we paint the interfaces of the digital world. I embarked on this journey through an inspiring course that not only taught me the technicalities but also guided me to build my first project, allowing me to witness the magic of transforming code into a visual experience. Link to the course</p>"},{"location":"blog/a-roadmap-for-web-developper/#chapter-2-javascript-embracing-dynamic-interactions","title":"Chapter 2: JavaScript - Embracing Dynamic Interactions","text":"<p>JavaScript, the language that breathes life into web pages, offers endless possibilities. You can choose to skip it and learn the language in the next course (course 3), as we'll be talking about variables, loops and functions here. But it is always good to learn the basics of a programming language before using any of his frameworks. Link to the course</p>"},{"location":"blog/a-roadmap-for-web-developper/#chapter-3-javascript-for-web-making-web-pages-dynamic","title":"Chapter 3: JavaScript for Web - Making Web Pages Dynamic","text":"<p>The world of web development thrives on dynamic experiences. This course is the profect continuation in the web dev journey after html, css. On coursera, there is a course that introduces the javascript tools we use to manipulate objects on a web page to make it less static.</p> <p>Towards the end, there's a quiz where you do a project using Node JS (a framework used to run javascript code) but I notice they've adopted a new, simpler format. Link to the course</p>"},{"location":"blog/a-roadmap-for-web-developper/#the-challenge-testing-time-cloning-a-web-page","title":"The Challenge: Testing Time - Cloning a Web Page","text":""},{"location":"blog/a-roadmap-for-web-developper/#chapter-4-python-unsupervised-ml-algorithms","title":"Chapter 4: Python: Unsupervised ML Algorithms","text":"<p>My journey expanded beyond web development into the realms of Python. This detour into the world of unsupervised machine learning algorithms was not entirely new. It offered a different perspective, unlocking doors to exploring and analyzing data with a sense of wonder. Link to the course</p>"},{"location":"blog/a-roadmap-for-web-developper/#chapter-5-php-for-backend-connecting-frontend-to-databases","title":"Chapter 5: PHP for Backend - Connecting Frontend to Databases","text":"<p>The bridge between frontend and databases fascinated me. PHP became my vessel in understanding the interaction between a website and a database. While focusing on MySQL, I explored the installations and syntax, essential for crafting PHP code. Link to reference</p> <p>I must admit, i learning out of pure curiosity, after my first internship in cloud computing. I wanted to know more are a crucial component of the 3-tiers architecture, which is the database and two other reseons as important</p> <ul> <li>i know php is one of the most use tools for backend development</li> <li>i know php integrate very well with html, css</li> <li>i've learned html, css, js before hand</li> </ul> <p>My only experiences with php, for at least years, after learning was about helping every time a student come to be with a problem to fix or a functionnality to add. But after a while, i got into a project where we decided with the team, to use laravel. Laravel is based on php. I got onto it very quick because, even if i have 0 real project experience in php, i've coded in php to help many students. All the tools needed was already on my computer and i've added the first feature assigned to me within a week while the others, took at least 3 weeks to undertand the framework and add the feature assigned to them.</p>"},{"location":"blog/a-roadmap-for-web-developper/#chapter-6-django-for-backend-in-python-navigating-the-web-development-framework","title":"Chapter 6: Django for Backend in Python - Navigating the Web Development Framework","text":"<p>It's a Python framework for web development. I took part in a course on Coursera to learn about Django architecture (how to create a project, how files are organized, what each file is used for).</p> <p>I learned a lot more about the framework during my internship at Safran than during the apprenticeship. With the bugs to slove and features to be added, I needed to read stackoverflow discussions, try and fail, visit the documentation very often (very rich by the way, cudo to django project team). So the real learning began amidst challenges and practical applications.</p>"},{"location":"blog/a-roadmap-for-web-developper/#second-internship-web-development","title":"Second internship: Web development","text":"<p>That was the internship i needed to be sure i wasn't just a part time web development learner. My task was to develop a web app that will enable members of a department (i wont say more) to search and access useful document through a simple search engine adpated to their need. The project started as a django project. Before coding, i did design the web app with figma. After, the web app validated, i started the core features.</p> <p>!!!   If i have learn one important thing during the first month, it is to start with the login components if login is a required part of the process.</p> <p>I've developed iteratively the web app. Even though i was told to not waste time on frontend, i could just stand ugly pages. So i was adding css/bootstrap codes of my own. In the second month, i have mainly worked on three backend features</p> <ul> <li>one that give informations about a document. In that part of the app, users can also appli some modifications. By modifications, i'm talking about the classic ones you can make on a word document (change colors of some words, letters, surligne, bold, italic, ...). I've use a js tool as i recall.</li> <li>the frontend of the search engine: As simple of the google search interface. I've removed my styles on this.</li> <li>By the end of the month, i started working on the search engine. Well, the web dev project quickly become a Natural language processing project too. Without going into the details, i was doing NLP with javascript then switched to python to use sklearn methods.</li> </ul>"},{"location":"blog/a-roadmap-for-web-developper/#chapter-7-bootstrap-streamlining-css-development","title":"Chapter 7: Bootstrap - Streamlining CSS Development","text":"<p>Bootstrap is a framework that makes it easy to add CSS styles to a web page. But, skip to this course if you're working on a project where you need it. You'll learn faster. Same advice as for the next framework: Jquerry. For my part, I needed it to speed up the frontend part of my internship, so I went to the w3schools site. Link to a course I figured long after, tailwing, as a rigid alternative to bootstrap</p>"},{"location":"blog/a-roadmap-for-web-developper/#chapter-8-jquery-simplifying-javascript","title":"Chapter 8: jQuery - Simplifying JavaScript","text":"<p>While JavaScript is the heart of web interactions, jQuery offered a streamlined path. Mastering it could be an asset in certain scenarios, enhancing the development process. JQuerry is a framework that makes it easy to write javascript code for the web. Some developers don't know Jquerry as well as they know javascript, but in that case it's not always easy to work in a team that doesn't use Jquerry. Link to a course</p>"},{"location":"blog/a-roadmap-for-web-developper/#chapter-9-ajax-seamlessly-connecting-frontend-and-backend","title":"Chapter 9: Ajax - Seamlessly Connecting Frontend and Backend","text":"<p>Ajax, the invisible thread connecting frontend and backend without the need for page reloads. This technology became essential in my journey, especially in tandem with the Django framework, enabling seamless exchanges and a more interactive web experience. Link to a course</p>"},{"location":"blog/a-roadmap-for-web-developper/#chapter-10-reactjs-shaping-dynamic-user-experiences","title":"Chapter 10: ReactJS - Shaping Dynamic User Experiences","text":"<p>ReactJS is a NodeJS framework used a lot by frontend developers. A lot of developers learn reactJS, right after html, css and js, which puts them on the right track for front-end development. React Js became a vital part of my skill set during a significant gap year. It was very much in demand and used on the market in 2022. But the real reason I decided to learn ReactJS was for an internship in 2021-2022. My first assignment was to develop a web interface. I had a choice between reactJS and AngularJS. After discussions with the lead dev, we chose ReactJS. Long story short, we wanted the user to have a one-page experience on the application.</p>"},{"location":"blog/a-roadmap-for-web-developper/#chapter-11-nodejs-unleashing-javascript-on-the-server","title":"Chapter 11: NodeJS - Unleashing JavaScript on the Server","text":"<p>Javascript is well known for the front-end, but was limited to it until the development of NodeJS. NodeJS is a tool for executing JS scripts on the server side, opening doors to multifaceted web development.</p>"},{"location":"blog/a-roadmap-for-web-developper/#chapter-12-electronjs-exploring-beyond-the-browser","title":"Chapter 12: ElectronJS - Exploring Beyond the Browser","text":""},{"location":"blog/a-roadmap-for-web-developper/#chapter-13-laravel-embracing-a-new-horizon","title":"Chapter 13: Laravel - Embracing a New Horizon","text":"<p>The journey continues with Laravel, a framework that beckons with new possibilities and a new chapter to explore.</p> <p>Each chapter in this roadmap of web development is more than a course; it's a story, an experience, a transformation that shaped me into the software engineer I am today. The roadmap evolves, the journey continues, and the quest for learning never ceases.</p>"},{"location":"blog/a-roadmap-for-web-developper/#related-posts","title":"Related Posts","text":"<ul> <li>Cheat on Python Package Managers</li> </ul>"},{"location":"blog/evolution-of-a-project-from-python-apps-to-a-web-interface/","title":"Evolution of a Project: From Python Apps to a Web Interface","text":"<p>How a solo coder turned a repo full of data scripts into a full-stack web app with beautiful interface in one day ?</p> <p>In the world of development, projects tend to evolve, morphing and reshaping themselves to meet ever-changing needs and preferences. What starts as a collection of Python applications can transform into something entirely different\u2014a cohesive web interface with a backend to match.</p> <p>This evolution was made possible by harnessing a comprehensive tech stack, blending various cutting-edge technologies:</p> <ul> <li>Python: Served as the foundational language for backend logic and functionality.</li> <li>FastAPI: Empowered the backend with rapid server capabilities and smooth API integration.</li> <li>Next.js: Spearheaded the frontend development, providing dynamic and responsive user interfaces.</li> <li>Prisma: Efficiently managed databases, optimizing data operations and queries.</li> <li>Clerk: Handled authentication and user management, ensuring secure and streamlined user experiences.</li> <li>Docker: Ensured modern development practices and facilitated seamless execution across various environments.</li> </ul> <p>This amalgamation of technologies enabled the evolution from fragmented Python apps to a unified, feature-rich web interface, promising an immersive user experience while optimizing efficiency and elegance in development.</p>"},{"location":"blog/evolution-of-a-project-from-python-apps-to-a-web-interface/#motivation","title":"Motivation","text":"<p>At the end of 2023, in the very last 2 days, I felt a sense of completion. It was time to bring life to a project that had been waiting for attention.</p> <p>Looking back at the same time one year ago \u2014from 2022 to 2023\u2014 I remembered working on statistical tools, learning through coding all methods i've encountered in a dedicated coursera certification. The work is open source on github at Hermann-web/some-common-statistical-methods</p> <p>In the last months of 2023, i've started this blog, as a mean to put on the web what tech tools i learn and how to use them. I rewrite my notes as tutorials i find interesting enough to share, in a way i would want to read them right before having tested them.</p> <p>Before that, i've created an application that has an api, a frontend view then on top of it, a python module that gave birth to a cli tool. I've written a tutorial about it showing how i build it brick by brick and it also open source on github at Hermann-web/simple-file-hosting-with-flask</p> <p>But, as the year ended, a project lingered, a collection of scattered Python apps. This project had evolved into a vision\u2014a web interface combined with a backend, a unified creation born from various ideas.</p>"},{"location":"blog/evolution-of-a-project-from-python-apps-to-a-web-interface/#consolidating-python-apps","title":"Consolidating Python Apps","text":"<p>Initially, my computer housed several Python applications, each serving a specific purpose. As these apps expanded in functionality, I recognized the potential for a more streamlined experience. That's when I transitioned them into command-line interfaces (CLI apps), offering users a smoother interaction.</p>"},{"location":"blog/evolution-of-a-project-from-python-apps-to-a-web-interface/#structuring-for-scalability","title":"Structuring for Scalability","text":"<p>However, as the collection grew further, the need for organization became apparent. Inspired by the structured approach of ETL (Extract, Transform, Load) in data engineering, I restructured the repository, employing a folder hierarchy to enhance manageability.</p>"},{"location":"blog/evolution-of-a-project-from-python-apps-to-a-web-interface/#the-resurrection-of-dev-notes-markdown-mkdocs-and-docker","title":"The Resurrection of Dev Notes: Markdown, MkDocs, and Docker","text":"<p>This restructuring revelation extended beyond just applications. I figured i've gone through a similar for my development notes. These notes\u2014ranging from syntax references to frequently used functions and tutorials\u2014were upgraded significantly. Migrating from plain text to Markdown format, these notes underwent a transformation. Leveraging the prowess of MkDocs and Docker, these notes metamorphosed into a dynamic web application, offering a seamless browsing experience. This gave me an idea for my small python projects too.</p>"},{"location":"blog/evolution-of-a-project-from-python-apps-to-a-web-interface/#the-expansion-taking-projects-online","title":"The Expansion: Taking Projects Online","text":"<p>Embracing this analogy of transformation, it became evident that bringing these smaller python projects to the web was the next logical step. This expansion wasn't merely about accessibility; it was about a richer, more interactive experience. While I could have chosen familiar tools like Flask and HTML, my inclination toward exploring new frameworks led me elsewhere: Fast and modern tools for both the server (FastAPI/Tornado) and the ui (ReactJS/NextJS)</p>"},{"location":"blog/evolution-of-a-project-from-python-apps-to-a-web-interface/#framework-selection","title":"Framework Selection","text":"<p>Seeking equilibrium between rapid backend operations and a responsive frontend, this quest led me to opt for FastAPI for the server and Next.js for the frontend. These choices aligned with my desire for speed and versatility, given FastAPI's rapid server capabilities and Next.js's dynamic UI.</p>"},{"location":"blog/evolution-of-a-project-from-python-apps-to-a-web-interface/#starting-with-the-backend","title":"Starting with the backend","text":"<p>With prior experience in FastAPI, integrating one of my Python projects as starter was a swift process.  </p> <p>I opted for poetry for dependency management, though encountered a hiccup when it failed to create a virtual environment. Resorting to traditional methods to create one, I used poetry to manage packages.</p> <p>Then, i've added my packages with poetry: fastapi and uvicorn. then, black and isort, as dev dependencies. In comparison to pip, poetry or pipenv let you separate dev dependencies and you can ever group dependencies or add command to be run with poetry.</p> <p>See more on python package managers practical comparison here</p> <p>Despite encountering issues, tweaking the <code>pyproject.toml</code> scripts addressed the obstacles.</p> <pre><code># [tool.poetry.scripts]\n# format = \"isort . &amp;&amp; black .\"\n\n# [tool.poetry.scripts]\n# start = \"uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload\"\n</code></pre> <p>I've just made progress on the Swagger docs!</p> <p>Check out the snapshot of the Swagger documentation below:</p> <p></p> <p>The existing endpoints are operational, and to ensure smooth integration before introducing new ones, I've taken a step to containerize the application and connect the backend with the frontend.</p>"},{"location":"blog/evolution-of-a-project-from-python-apps-to-a-web-interface/#embracing-docker-for-modernization","title":"Embracing Docker for Modernization","text":"<p>Docker-compose emerged as my choice for modern development practices and to ensure seamless project execution across different environments. After locally testing the API with uvicorn, I streamlined the setup, crafting a <code>docker-compose.yml</code> file.</p> <p>Before creating the <code>docker-compose.yml</code>, i've exported my requirements into a file so i could use pip instead of poetry inside the container</p> <pre><code>poetry export -f requirements.txt --output requirements.txt --without-hashes\n</code></pre> <p>Then, i've refactorised <code>docker-compose.yml</code>, into a <code>Dockerfile</code> use by <code>docker-compose.yml</code>. Having a separate Dockerfile allowed me use it in the root folder of the project so i can run the <code>Dockerfile</code>s from client and server in a simple <code>docker-compose.yml</code></p> <p>So, i had an architecture like this</p> <pre><code>project-root/\n\u2502\n\u251c\u2500\u2500 client/                 # Next.js client folder\n\u2502\n\u251c\u2500\u2500 server/                 # FastAPI server folder\n\u2502   \u251c\u2500\u2500 app/                # FastAPI application logic\n\u2502   \u251c\u2500\u2500 requirements.txt    # Server dependencies\n\u2502   \u251c\u2500\u2500 Dockerfile          # Dockerfile for FastAPI server\n\u2502   \u251c\u2500\u2500 docker-compose.yml  # Docker Compose configuration\n\u2502   \u251c\u2500\u2500 .gitignore          # Server-specific .gitignore\n\u2502   \u251c\u2500\u2500 pyproject.toml      # poetry dependency file\n\u2502   \u251c\u2500\u2500 poetry.lock         # poetry lock file\n\u2502   \u2514\u2500\u2500 ...                \n\u2502\n\u251c\u2500\u2500 docker-compose.yml      # Docker Compose configuration\n\u2514\u2500\u2500 readme.md               # Project README or documentation\n</code></pre> <p>I've added a module in my fastapi app and tested on swagger on the browser. It was working fine. So my version 0 will be ready righr after the setup of the nextjs app.</p>"},{"location":"blog/evolution-of-a-project-from-python-apps-to-a-web-interface/#the-frontend-odyssey","title":"The Frontend Odyssey","text":"<p>Choosing to build a bespoke tool, I commenced with a command to create a Next.js project. The CLI app provided an interactive interface, offering language options (js vs ts), formatting tool choices and more.</p>"},{"location":"blog/evolution-of-a-project-from-python-apps-to-a-web-interface/#navigating-the-ui-dilemma","title":"Navigating the UI Dilemma","text":"<p>In my quest to fashion a unique frontend, fate introduced me to DevToolboxWeb, a project resonating with my aspirations.</p> <p>Despite deliberations on building from scratch, leveraging their UI became my choice for multiple reasons:</p> <ul> <li>An opportunity to contribute to an interesting open source project</li> <li>If i were to build from scratch, I would have done it like them</li> <li>I was on a clock ! And i prefer build-from-template over build-from-scratch when it comes to frontend</li> </ul>"},{"location":"blog/evolution-of-a-project-from-python-apps-to-a-web-interface/#merging-frontend-and-backend","title":"Merging Frontend and Backend","text":"<p>While DevToolboxWeb's UI aligned with my vision, it lacked a backend\u2014a void I intended to fill. Forking their repository, I ventured into integrating my FastAPI backend with their feature-rich UI, striving for a synergy of functionality and aesthetics. That's why i've created a fork of their code and add a server into it.</p>"},{"location":"blog/evolution-of-a-project-from-python-apps-to-a-web-interface/#integrating-fastapi-and-nextjs","title":"Integrating FastAPI and Next.js","text":"<p>To bridge the gap between FastAPI and Next.js, I explored digitros/nextjs-fastapi from the Next.js and FastAPI starter templates on Vercel. This resource offered insights into integrating the next js based frontend and fastapi backend seamlessly.</p>"},{"location":"blog/evolution-of-a-project-from-python-apps-to-a-web-interface/#culmination-a-glimpse-of-the-horizon","title":"Culmination: A Glimpse of the Horizon","text":"<p>Projects in development constantly change and borrow from each other. My journey from scattered Python apps to a unified web interface has been an ongoing quest for efficiency and elegance.</p> <p>As I combine backend strength with frontend finesse, the vision of a user-friendly toolbox gets closer. And in this pursuit, the journey itself is as rewarding as the destination.</p>"},{"location":"blog/evolution-of-a-project-from-python-apps-to-a-web-interface/#related-posts","title":"Related Posts","text":"<ul> <li>Cheat on Python Package Managers</li> <li>How to publish your python project (to pypi or testPypi) with Poetry then install as dependency in another project ?</li> </ul>"},{"location":"blog/security-practices-for-authentication-a-guide-for-developers/","title":"Security Practices for Authentication: A Guide for Developers","text":""},{"location":"blog/security-practices-for-authentication-a-guide-for-developers/#introduction","title":"Introduction","text":"<p>In the realm of digital security, authentication plays a critical role in safeguarding user data and system integrity. However, implementing robust authentication mechanisms is often a daunting task, fraught with potential vulnerabilities and pitfalls. This guide aims to shed light on common authentication practices, highlighting both their strengths and weaknesses, while providing insights into best practices for developers.</p>"},{"location":"blog/security-practices-for-authentication-a-guide-for-developers/#authentication-methods-overview","title":"Authentication Methods Overview","text":"<p>Authentication methods vary widely in their implementation details, security levels, and use cases. Below, we briefly discuss some common methods before delving into a detailed comparison.</p> <ul> <li> <p>Clear Text Authentication: Involves sending credentials (username and password) without encryption. This method is highly insecure and susceptible to eavesdropping, especially by Man-in-the-Middle (MITM) attackers.</p> </li> <li> <p>Basic Authentication: Utilizes a Base64 encoding scheme to transmit credentials. While slightly better than clear text, it's still easily decodable and not recommended for use without HTTPS.</p> </li> <li> <p>Session Tokens: A server-generated token is sent to the client, which stores it and includes it in subsequent requests. This method requires server-side storage to validate sessions.</p> </li> <li> <p>JSON Web Tokens (JWT): A compact, URL-safe means of representing claims between two parties. JWTs can be encrypted (JWE) for added security and are typically used in stateless authentication scenarios.</p> </li> <li> <p>Bearer Tokens: Essentially a type of access token, \"Bearer\" refers to the method of sending the token in HTTP headers. Bearer tokens can encapsulate various forms of authentication, including JWTs.</p> </li> </ul>"},{"location":"blog/security-practices-for-authentication-a-guide-for-developers/#detailed-comparison","title":"Detailed Comparison","text":"<p>To better understand the nuances between these methods, refer to the comparison table below:</p> Feature / Method Clear Text Basic Authentication Session Tokens JWT Bearer Tokens Encryption No Base64 (Not secure) Optional Yes* Yes* HTTPS Recommended Yes Yes Yes Yes Yes Client Storage No No Cookies Cookies/Header Header Server Storage No No Yes No No Vulnerability to MITM High High Low (with HTTPS) Low (with HTTPS) Low (with HTTPS) Statefulness Stateless Stateless Stateful Stateless Stateless Expiration Control No No Yes Yes Yes Logout Capability N/A N/A Yes No** Yes** Revocation Capability N/A N/A Yes No** Yes** Complexity Low Low Medium Medium Medium Use Case Avoid Simple Auth Web Applications APIs/Microservices APIs/Microservices <p>* While JWT and Bearer tokens can use encryption, JWT typically involves signing rather than encrypting. Encryption is possible with JWE (JSON Web Encryption).</p> <p>** Logging out or revoking JWTs and Bearer Tokens without server-side tracking involves client-side action to discard the token. However, server-side mechanisms can be implemented for more control, such as token blacklisting or using short-lived tokens with a refresh mechanism.</p> <p>This table serves as a guide to choosing the right authentication method based on your application's needs. For instance:</p> <ul> <li>Clear Text: Highly insecure and should be avoided. Use HTTPS to protect data in transit.</li> <li>Basic Authentication: Simple but requires HTTPS for security. Suitable for simple authentication needs.</li> <li>Session Tokens: Ideal for web applications where state can be maintained on the server.</li> <li>JWT (JSON Web Tokens): Best suited for stateless APIs or microservices, allowing for scalable and flexible authentication mechanisms.</li> <li>Bearer Tokens: Similar to JWT, used for authenticating requests in APIs and microservices, providing a secure method for client-server communication.</li> </ul> <p>Each method has its context where it excels, as well as its drawbacks. The choice among them depends on various factors such as security requirements, the architecture of the application, scalability needs, and user experience considerations.</p> <p>By considering the features outlined in the table, developers and architects can make informed decisions that balance security and functionality, ensuring that their applications are both secure and user-friendly.</p>"},{"location":"blog/security-practices-for-authentication-a-guide-for-developers/#security-enhancements-and-best-practices","title":"Security Enhancements and Best Practices","text":"<p>Beyond choosing the right authentication method, ensuring the security of the authentication process involves several best practices:</p> <ol> <li> <p>Always Use HTTPS: Regardless of the authentication method, securing the channel with HTTPS is critical to prevent eavesdropping and MITM attacks.</p> </li> <li> <p>Secure Token Storage: When tokens are used (Session, JWT, Bearer), secure storage on the client side is essential. For web applications, HttpOnly cookies can provide added security by preventing client-side script access to the token.</p> </li> <li> <p>Proper Encryption and Hashing: For any method involving passwords or sensitive information, ensure that data is encrypted during transit and securely hashed (using algorithms like bcrypt or Argon2) when stored.</p> </li> <li> <p>Regularly Update Security Measures: With the evolving nature of threats and security standards, regularly reviewing and updating authentication and security practices is vital.</p> </li> </ol> <p>By carefully selecting an appropriate authentication method and adhering to security best practices, you can significantly enhance the security posture of your applications.</p>"},{"location":"blog/security-practices-for-authentication-a-guide-for-developers/#advanced-techniques-oauth2-and-machine-learning-in-authentication","title":"Advanced Techniques: OAuth2 and Machine Learning in Authentication","text":"<ul> <li>OAuth2: Implements token-based authentication, frequently refreshing tokens to enhance security.</li> <li>Machine Learning: Can differentiate between legitimate users and attackers by analyzing behavioral patterns, thus improving security measures.</li> </ul>"},{"location":"blog/security-practices-for-authentication-a-guide-for-developers/#mobile-app-considerations","title":"Mobile App Considerations","text":"<ul> <li>Token Storage: Securely store tokens using platform-specific secure storage solutions to protect against unauthorized access, including on rooted devices.</li> <li>Data Encryption: Essential for protecting sensitive information stored within mobile applications.</li> </ul>"},{"location":"blog/security-practices-for-authentication-a-guide-for-developers/#device-identification-who-vs-what","title":"Device Identification: \"Who\" vs. \"What\"","text":"<ul> <li>Who: Identifies the user, typically through tokens representing their authenticated session.</li> <li>What: Refers to the device, recognized via identifiers like IP and MAC addresses, adding an additional layer of security by validating not just who is accessing the system but also from which device.</li> </ul>"},{"location":"blog/security-practices-for-authentication-a-guide-for-developers/#conclusion","title":"Conclusion","text":"<p>Secure authentication is a multifaceted challenge that requires a comprehensive approach, including secure transmission of credentials, robust password storage practices, and considerations for both user and device authentication. By implementing the recommendations outlined in this document, developers and system architects can significantly enhance the security of their authentication mechanisms.</p>"},{"location":"blog/unveiling-the-code-chronicles-navigating-software-licenses/","title":"Unveiling the Code Chronicles: Navigating Software Licenses","text":""},{"location":"blog/unveiling-the-code-chronicles-navigating-software-licenses/#introduction","title":"Introduction","text":"<p>Ever glanced at those cryptic licenses in software projects\u2014like the <code>MIT License</code>, <code>Apache License</code>, or <code>GPL</code>\u2014and wondered, 'What do they mean for me?'</p> <p>Whether you're a code wizard or just dipping your toes in tech, understanding these licenses is like decoding a secret language. Which one suits your project best? What's the deal when you borrow or tweak someone else's code?</p> <p>This document unravels the mystery, diving into the world of software licenses. Discover their quirks, choose wisely between the MIT, Apache, or GPL licenses, and learn the ropes for handling borrowed or tweaked code. Get ready to crack the code of software licenses!</p> <p>Please note that the information provided here is for educational purposes only and should not be construed as legal advice.</p>"},{"location":"blog/unveiling-the-code-chronicles-navigating-software-licenses/#motivation-for-choosing-licenses","title":"Motivation for Choosing Licenses","text":"<p>Understanding software licenses is crucial for project development. Choosing a license depends on factors like project goals, desired openness, collaboration, and legal obligations when utilizing or modifying a project.</p>"},{"location":"blog/unveiling-the-code-chronicles-navigating-software-licenses/#what-to-choose-for-your-project","title":"What to Choose for Your Project?","text":"<ul> <li>Consider the project's goals, community involvement, and desired freedoms for users when selecting a license.</li> <li>Assess the implications of each license on collaboration, distribution, and derivative works.</li> </ul>"},{"location":"blog/unveiling-the-code-chronicles-navigating-software-licenses/#actions-with-copiedmodified-projects","title":"Actions with Copied/Modified Projects","text":"<ul> <li>Adhering to the original license terms is crucial when using, modifying, or distributing a project.</li> <li>Respect the license obligations to the original authors while exercising your rights under the chosen license.</li> </ul>"},{"location":"blog/unveiling-the-code-chronicles-navigating-software-licenses/#open-source-licenses","title":"Open Source Licenses","text":"<p>A software is <code>Open source</code> when his code source is available (Not quite the same for llm though). It can be permissive (MIT, Apache, ...), copyleft (you should distribute under the same license and/or keep track of the modification and/or not remove the claim in the license).</p>"},{"location":"blog/unveiling-the-code-chronicles-navigating-software-licenses/#permissive-licenses","title":"Permissive Licenses","text":"<p>They allow anyone to use, modify, distribute, and sell the software with few restrictions. Though you're not obligated to share modifications, distributing in source code form implies sharing. Here are the most common exemples.</p> <code>MIT License:</code> <code>Apache License / Microsoft Public License (Ms-PL)</code> <code>BSD License</code> <ul> <li>Allows users <code>U1</code> to use, modify, and distribute (release to user <code>U2</code>) the software ( which a part is a derivative from <code>U0</code> work) for any purpose, including commercial use, without having to share ( to user <code>U2</code>) their modifications or contributions</li> <li>Essentially, it offers considerable freedom.</li> </ul> <ul> <li>Permits users <code>U1</code> to use, modify, and distribute the software without having to share their modifications with <code>U2</code></li> <li>Requires users to provide attribution (a mention of the original author' name <code>U0</code>) and a copy of the license when distributing the software to <code>U2</code>.</li> <li>Includes a patent license, ensuring users won't be sued for patents related to the software.</li> <li>So basically, keep the original author's name and no patent (warranties) problem</li> </ul> <ul> <li>Allows redistribution and modification</li> <li>Requires that the original copyright notice and disclaimer be retained.</li> <li>Essentially, it ensures the original author (<code>U0</code> or <code>U1</code>) are not legally responsible for any issues arising from User <code>U2</code>'s use.</li> </ul>"},{"location":"blog/unveiling-the-code-chronicles-navigating-software-licenses/#copyleft-licenses","title":"Copyleft Licenses","text":"<p>They</p> <ul> <li>Impose restrictions on how the software can be used and distributed.</li> <li>Require that any modifications or derivative works (In programming for example, the part of a software that use/modify a copyleft licenced code) of the software also be released under the same license, maintaining the software's open-source nature.</li> </ul> <p>This helps ensure that the software remains open source and that any improvements made to it are shared with the community. Examples of copyleft licenses include the GNU General Public License (GPL) and the Lesser General Public License (LGPL) or AGPL or  SSPL or BSD (Berkeley Software Distribution) or MPL ( Mozillah Public Licence) or Eclipse Public License (EPL) or CC ( Creative Common)</p> <p>Examples include</p> <code>LGPL (Lesser General Public License)</code> <code>GPL (General Public License)</code> <code>AGPL (Affero General Public License)</code> <code>SSPL (Server Side Public License)</code> <code>MPL (Mozilla Public License) / EPL (Eclipse Public License)</code> <code>Others</code> <ul> <li>Requires users (<code>U1</code>) to share modifications and contributions (only the part of the software that use a LGPL licenced code) to the software under the same license (LGPL).</li> <li>Mandates <code>U1</code> making the source code available to anyone (<code>U2</code>) receiving the software and providing a copy of the license along with it.</li> <li>So basically, requires that modifications to the code be released under the same license.</li> </ul> <ul> <li>Extend LGPL rules</li> <li>Require the entire codebase become GPL licenced if any part uses GPL-licensed code, ensuring the software remains open source.</li> <li>Require the entire codebase should to be shared with user <code>U2</code> when distributing the software</li> <li>So basically, requires that any software that use a GPL licenced code become a GPL licence software that share its codebase.</li> </ul> <ul> <li>Extends GPL's rules to apply even when software is accessed via a network.</li> </ul> <ul> <li>Extend AGPL rules</li> <li>Requires sharing the entire codebase (at least, containing one SSPL-licensed code) with the public (not only user <code>U2</code> whose you distribute it to) when using SSPL-licensed code.</li> <li>Mongo DB uses this license.</li> </ul> <ul> <li>Similar to LGPL, these licenses require modified code portions to be released under the same license but not the entire program. So it allows linking with non-free software</li> <li>Similar to LGPL, requires that modifications to the code be released under the same license</li> </ul> <ul> <li>Microsoft Reciprocal License (Ms-RL)</li> <li>LGPL + disclaimer</li> </ul>"},{"location":"blog/unveiling-the-code-chronicles-navigating-software-licenses/#proprietary-licenses","title":"Proprietary Licenses","text":"<p>Those are the most restrictive, controlling software use and distribution and may involve fees. Examples include Microsoft Windows EULA, Adobe Photoshop License Agreement, etc.</p>"},{"location":"blog/unveiling-the-code-chronicles-navigating-software-licenses/#dual-licensing","title":"Dual Licensing","text":"<p>Dual licenses use multiple licenses for different parts of software. Challenges arise when incorporating GPL-licensed components as they may necessitate the entire product to be GPL-licensed and shared accordingly with <code>U2</code> (those who use your product in a compiled form), unless a separation like mere-aggregation is possible. <code>Examples like WordPress, which is GPL-licensed, but premium themes might use different licenses.</code></p>"},{"location":"blog/unveiling-the-code-chronicles-navigating-software-licenses/#gpl-license-challenges-and-tricks","title":"GPL license: Challenges and Tricks","text":"<p>As a GPL licence requires that any software that use a GPL licenced code become a GPL licence software that share its codebase, these strict obligations can potentially affect the licensing of the entire project.</p> <p>But if you use a non-free program in your codebase or just don't want to share your codebase, a GPL part is a bug. That's why alternatives like MLP or EPL or LGPL are used to \"link\" a GPL.</p> <p>Another trick is to claim a \"mere aggregation.\"  of a (derivative of a gpl licenced component) and (anothers components).     - Like Android OS which use a modified linux kernel but as the kernel runs at the top of a android os, only the modified linux kernel become gpl. So they have a dual ( Apache licenced Android-OS + Gpl licenced modified-linux-kernel )     - Or the difference between worpress ( under gpl) vs the themes ( under gpl) (because they use or modify worpress core) vs anothers that don't use or modify the core</p>"},{"location":"blog/unveiling-the-code-chronicles-navigating-software-licenses/#dual-licensing-and-open-source-preferences","title":"Dual Licensing and Open Source Preferences","text":"<p>This section delves deeper into the dynamics of dual licensing, open source preferences, and their implications within the software development sphere:</p>"},{"location":"blog/unveiling-the-code-chronicles-navigating-software-licenses/#dual-licensing-consideration","title":"Dual Licensing Consideration","text":"<p>Some view dual licensing as problematic. Users often lean towards open source due to its security, global expertise utilization, and customization capabilities. However, integrating open-source solutions can pose challenges.</p>"},{"location":"blog/unveiling-the-code-chronicles-navigating-software-licenses/#open-source-and-security","title":"Open Source and Security","text":"<p>Open-source solutions are favored for security reasons and customization but might involve integration challenges. Notable examples include:     - OpenStack: Uses a permissive Apache license.     - Mozilla Firefox: Utilizes a permissive MPL (Mozilla Public License).     - Linux: Governed by GPL (General Public License), chosen for rapid development and innovation adoption.</p>"},{"location":"blog/unveiling-the-code-chronicles-navigating-software-licenses/#proprietary-vs-open-source","title":"Proprietary vs. Open Source","text":"<p>While some prefer proprietary solutions for security and maintenance contracts, others argue in favor of copyleft (e.g., LGPL or GPL). They believe copyleft licenses compel companies to share their work, favoring open source in competition.</p>"},{"location":"blog/unveiling-the-code-chronicles-navigating-software-licenses/#software-composition-analysis-sca-tools","title":"Software Composition Analysis (SCA) Tools","text":"<p>SCA tools automatically scan projects for license compliance, ensuring adherence to licensing requirements.</p>"},{"location":"blog/unveiling-the-code-chronicles-navigating-software-licenses/#open-source-policy","title":"Open Source Policy","text":"<p>Establishing an open-source policy involves:     - Tracking external code licenses and their respective requirements.     - Utilizing SCA tools for scanning purposes.     - Maintaining records of license purchases, expiration dates, and repository registrations.     - Noting that expired licenses may place code in the public domain (if unaltered), while patents and trademarks remain.     - Keeping up to date with licence change</p>"},{"location":"blog/unveiling-the-code-chronicles-navigating-software-licenses/#license-management","title":"License Management","text":"<p>Constant vigilance is required to stay updated with license changes and adhere to compliance standards.</p>"},{"location":"blog/unveiling-the-code-chronicles-navigating-software-licenses/#conclusion-the-significance-of-open-source-and-challenges-with-dual-licensing","title":"Conclusion: The Significance of Open Source and Challenges with Dual Licensing","text":"<p>Open source licenses come in various forms, ranging from permissive ones like MIT and Apache to copyleft licenses where you should distribute under the same license and/or keep track of the modification abd/or not remove the claim in the license.  For example, for GPL, if you distribute ( give a software which one part of the code source is under gpl), you should give the source too. It's crucial to understand their implications for your project and how they impact collaboration, distribution, and legal obligations.</p> <p>Beside open source and proprietary licenses, the dual licences has emerged, combining multiples licenses for differents parts of the same project.</p> <p>So, dual licensing is a strategy used to address different needs within a project, but it can introduce complexities and limitations. While open source fosters collaboration, innovation, and community-driven development, navigating through licensing choices requires careful consideration and understanding.</p> <p>Please remember that the information provided in this document is for educational purposes only and should not be considered legal advice. It's essential to consult with legal professionals for specific legal guidance related to software licenses and intellectual property matters.</p>"},{"location":"blog/unveiling-the-code-chronicles-navigating-software-licenses/#related-links","title":"Related links","text":"<ul> <li>https://choosealicense.com</li> <li>https://www.tldrlegal.com</li> </ul>"},{"location":"blog/seaborn-in-practice-syntax-and-guide/","title":"Seaborn in Practice: Syntax and Guide","text":"<p>Seaborn is a powerful data visualization library in Python that provides a high-level interface for drawing attractive and informative statistical graphics. One common misconception about Seaborn and programming in general is the necessity to remember all the syntax. In reality, it's more about understanding the tool's capabilities and how to leverage its functions to visualize data effectively.</p> So, what can i do exactly with seaborn ?"},{"location":"blog/seaborn-in-practice-syntax-and-guide/#import-the-library","title":"import the library","text":"<pre><code>import seaborn as sns\nsns.set(style=\"whitegrid\")\n</code></pre>"},{"location":"blog/seaborn-in-practice-syntax-and-guide/#load-some-dataset","title":"Load some dataset","text":"<p>we will be using a dataset containing tips from a restaurant. We will know more more about it down the road</p> <pre><code>df = sns.load_dataset(\"tips\")\n</code></pre> <p>Let's see a preview of the dataset</p> <pre><code>df\n</code></pre> total_bill tip sex smoker day time size 0 16.99 1.01 Female No Sun Dinner 2 1 10.34 1.66 Male No Sun Dinner 3 2 21.01 3.50 Male No Sun Dinner 3 3 23.68 3.31 Male No Sun Dinner 2 4 24.59 3.61 Female No Sun Dinner 4 ... ... ... ... ... ... ... ... 239 29.03 5.92 Male No Sat Dinner 3 240 27.18 2.00 Female Yes Sat Dinner 2 241 22.67 2.00 Male Yes Sat Dinner 2 242 17.82 1.75 Male No Sat Dinner 2 243 18.78 3.00 Female No Thur Dinner 2 <p>We have an overview of the data but it is not enough. We will do a broad visualisation of the columns with pairplot.</p>"},{"location":"blog/seaborn-in-practice-syntax-and-guide/#pairplot","title":"Pairplot","text":"<p>A quick way to visualize relationships in a dataset is by using the method <code>pairplot</code>.</p> <pre><code>sns.pairplot(df)\n</code></pre> Result <p></p> <p>You can see here, we have a table of graphs. The 3 rows and 3 columns correpond to the 3 numerical values in out dataset: <code>tip</code>, <code>total_bill</code> and <code>size</code> In each cell, one column is plot against another:</p> <ul> <li>In the diagonals, a column is plotted againt itselt and you have histograms</li> <li>In the anti-diagonals, 2 columns are plotted against each other and you have a scatterplot</li> </ul> <p>You can also notice only 3 columns of our dataframe is here. It is because they contain numerical values. The 3 others (<code>sex</code>, <code>smoker</code>, <code>day</code> and <code>time</code>) are</p>"},{"location":"blog/seaborn-in-practice-syntax-and-guide/#histogram","title":"Histogram","text":"<p>Histograms are used both in univariate statistics and multivariate statistics To display the average notes by gender, you can use a bar plot:</p> <pre><code>import seaborn as sns\n# group by gender, then get the column \"notes\" then, compute the mean of notes in a group\ndf1 = df.groupby('gender')[['notes']].mean().reset_index()\nsns.set(style=\"whitegrid\")\n# show a barplot of out new dataframe (mean_notes = fct(gender))\nax = sns.barplot(x=\"gender\", y=\"notes\", data=df1)\n</code></pre> <p>Counting occurrences can be visualized using a categorical plot:</p> <pre><code>sns.catplot(x='gender', kind='count', data=ratings_df)\n</code></pre> <p>You can extend this to visualize counts by gender and skin color:</p> <pre><code>sns.catplot(x='gender', hue='couleur', kind='count', data=ratings_df)\n</code></pre> <p>Further stratifying by region:</p> <pre><code>sns.catplot(x='gender', hue='couleur', row='region', kind='count', data=ratings_df, height=3, aspect=2)\n</code></pre>"},{"location":"blog/seaborn-in-practice-syntax-and-guide/#scatterplot","title":"Scatterplot","text":"<p>Scatterplots offer a powerful way to visualize relationships between two variables:</p> <p>To represent points based on 'eval' as a function of 'age':</p> <pre><code>ax = sns.scatterplot(x='age', y='eval', data=ratings_df)\n</code></pre> <p>You can distinguish points by gender using different colors:</p> <pre><code>ax = sns.scatterplot(x='age', y='eval', hue='sex', data=ratings_df)\n</code></pre> <p>For more complex visualizations involving multiple categorical variables:</p> <pre><code>sns.relplot(x=\"age\", y=\"eval\", hue=\"sex\", row=\"region\", data=ratings_df, height=3, aspect=2)\n</code></pre> <p>Including regression lines on scatterplots:</p> <pre><code>sns.lmplot(data=ratings_df, x=\"var1\", y=\"var2\", height=5, aspect=1.5)  # Height 5, width 1.5 times larger than height\n</code></pre> <p>Creating scatter plots with histograms for marginal distributions:</p> <pre><code>sns.jointplot(data=df, x=\"var1\", y=\"var2\", height=3.5)\n</code></pre>"},{"location":"blog/seaborn-in-practice-syntax-and-guide/#boxplot","title":"Boxplot","text":"<p>Boxplots provide a visual summary of the distribution of data:</p> <p>To view the average ages and percentiles at 5% and 95%:</p> <pre><code>sns.boxplot(ratings_df['age'], orient='v')\n</code></pre> <p>Visualizing the average notes and percentiles for each gender:</p> <pre><code>ax = sns.boxplot(x='sexe', y='notes', data=ratings_df)\n</code></pre> <p>Further stratifying data for insights:</p> <pre><code>df[\"xxx_grp\"] = pd.cut(df.xxx, [18, 30, 40, 50, 60, 70, 80])  # Creating age strata\nsns.boxplot(x=\"xxx_grp\", y=\"xxx\", hue=\"yyy\", data=df)  # Optional hue for differentiation\n</code></pre>"},{"location":"blog/seaborn-in-practice-syntax-and-guide/#distribution-plot","title":"Distribution Plot","text":"<p>Understanding the distribution of data:</p> <pre><code>ax = sns.distplot(ratings_df['notes'], kde=False)\n</code></pre> <p>Analyzing note distribution by gender:</p> <pre><code>sns.distplot(ratings_df[ratings_df['sexe'] == 'female']['eval'], color='green', kde=False)\nsns.distplot(ratings_df[ratings_df['sexe'] == 'male']['eval'], color=\"orange\", kde=False)\nplt.show()\n</code></pre>"},{"location":"blog/seaborn-in-practice-syntax-and-guide/#heatmap","title":"Heatmap","text":"<p>Utilizing heatmaps to visualize numerical data:</p> <pre><code>corr = new_df.corr()  # Calculating feature correlations\nax = sns.heatmap(corr, vmin=0, vmax=1, cmap=\"YlGnBu\", annot=True)\nplt.savefig('seabornPandas.png')\nplt.show()\n</code></pre>"},{"location":"blog/seaborn-in-practice-syntax-and-guide/#conclusion","title":"Conclusion","text":"<p>These examples showcase how Seaborn can be effectively utilized for various visualization needs without the necessity to memorize all the syntax.</p> <p>Understanding the basic syntax and functionality of Seaborn allows you to explore various plots and graphs that suit your data analysis requirements. Through simple examples and by focusing on the visual representation of data, you can gain deeper insights without the burden of remembering intricate details.</p> <p>Remember, Seaborn is designed to assist in the visual exploration of your data, offering a wide range of options for customizing and fine-tuning plots to suit your specific needs.</p> <p>Experiment with different plot types and functionalities to better understand the story your data has to tell. And don't hesitate to refer to the documentation and various online resources available to enrich your understanding and application of Seaborn.</p> <p>Let the visualization journey begin, and may your data tell its story vividly through Seaborn!</p>"},{"location":"blog/seaborn-in-practice-syntax-and-guide/#related-posts","title":"Related Posts","text":"<ul> <li>Cheat on Python Package Managers</li> </ul>"},{"location":"blog/flask-based-file-hosting-web-app--api--python-module--cli-app/","title":"Flask based File Hosting (web app & api & python module & cli app)","text":"<p>This guide will walk you through creating a basic file hosting web application using Flask, a lightweight web framework for Python. The application will include features such as user login, file uploads, and file listing. We'll also explore adding a simple API for interacting with the application.</p>"},{"location":"blog/flask-based-file-hosting-web-app--api--python-module--cli-app/#prerequistes","title":"Prerequistes","text":"<ul> <li>python &gt;=3.9</li> </ul>"},{"location":"blog/flask-based-file-hosting-web-app--api--python-module--cli-app/#setup-environment","title":"Setup Environment","text":"<ol> <li>Create a <code>requirements.txt</code> file:</li> </ol> <pre><code>python-slugify\npython-dotenv\nFlask~=2.0.1\n</code></pre> <ol> <li>Set up a Python virtual environment:</li> </ol> <p>Open your terminal and follow these steps</p> <code>For Linux &amp; Mac</code> <code>For Windows</code> <pre><code>cd path/to/folder\n\n# check the python version and localisation\npython -V\nwhich python\n\n# create the env\npython -m venv venv\n\n# activate the env\n./venv/bin/activate\n\n# check the python version and localisation\npython -V\nwhich python\n\n# install requirements\npip install -r requirements.txt\n</code></pre> <pre><code>cd path/to/folder\n\n# check the python version and localisation\npython -V\nwhere.exe python\n\n# create the env\npython -m venv venv\n\n# activate the env\n./venv/Scripts/activate\n\n# check the python version and localisation\npython -V\nwhere.exe python\n\n# install requirements\npip install -r requirements.txt\n</code></pre>"},{"location":"blog/flask-based-file-hosting-web-app--api--python-module--cli-app/#constants-configuration","title":"Constants Configuration","text":"<p>Create a <code>constants.py</code> file:</p> <pre><code>import os\nfrom pathlib import Path\nfrom dotenv import load_dotenv\n\nUPLOAD_FOLDER = Path('uploads')\nUPLOAD_FOLDER.mkdir(exist_ok=True)\n\nDATA_FILE = Path('data.json')\n\nDEBUG = False  # Set this to True in the development environment\n\n# Load environment variables from .env file\nload_dotenv()\n\nif not DEBUG:\n    USERNAME = os.getenv(\"S_USERNAME\")\n    PASSWORD = os.getenv(\"S_PASSWORD\")\n    FLASK_SECRET_KEY = os.getenv(\"FLASK_SECRET_KEY\")\nelse:\n    USERNAME = \"my-username\"\n    PASSWORD = \"my-password\"\n    FLASK_SECRET_KEY = 'my-secret-key'\n\nassert USERNAME\nassert PASSWORD\nassert FLASK_SECRET_KEY\n</code></pre>"},{"location":"blog/flask-based-file-hosting-web-app--api--python-module--cli-app/#creating-the-flask-app","title":"Creating the Flask App","text":"<p>Create a file named <code>app.py</code> and set up the initial Flask app:</p> <pre><code>from datetime import datetime, timedelta\nfrom functools import wraps\nimport json\nimport os\nimport mimetypes\nfrom slugify import slugify\nfrom flask import Flask, render_template, request, redirect, send_from_directory\nfrom flask import session, jsonify\nimport constants\n\napp = Flask(__name__)\napp.config['UPLOAD_FOLDER'] = constants.UPLOAD_FOLDER\napp.config['DATA_FILE'] = constants.DATA_FILE\napp.config['SECRET_KEY'] = constants.FLASK_SECRET_KEY\napp.config['DEBUG'] = constants.DEBUG\napp.config['USERNAME'] = constants.USERNAME\napp.config['PASSWORD'] = constants.PASSWORD\n</code></pre>"},{"location":"blog/flask-based-file-hosting-web-app--api--python-module--cli-app/#optional-simple-hello-world-app","title":"(Optional) Simple Hello World App","text":"<p>Replace the contents of <code>app.py</code> with a basic \"Hello, World!\" Flask app:</p> <pre><code># ... (Previous Code: Initial Setup)\n\n@app.route('/')\ndef hello():\n    return 'Hello, World!'\n\nif __name__ == '__main__':\n    app.run(debug=True)\n</code></pre> <p>Run the app using:</p> <pre><code>python app.py\n</code></pre> <p>Visit http://127.0.0.1:5000/ to see the \"Hello, World!\" message.</p>"},{"location":"blog/flask-based-file-hosting-web-app--api--python-module--cli-app/#adding-a-web-page-to-list-uploaded-files","title":"Adding a Web Page to List Uploaded Files","text":"<ol> <li> <p>Create a <code>template</code> folder and download the index.html file into it.</p> </li> <li> <p>Modify the Flask app in <code>app.py</code> to include the file listing:</p> </li> </ol> <pre><code># ... (Initial Setup)\n\n@app.route('/')\ndef index():\n    list_files = os.listdir(app.config['UPLOAD_FOLDER'])\n    files = [(filename, \"\") for filename in list_files]\n    return render_template('index.html', files=files)\n\nif __name__ == '__main__':\n    if not os.path.exists(app.config['UPLOAD_FOLDER']):\n        os.makedirs(app.config['UPLOAD_FOLDER'])\n    app.run()\n</code></pre> <p>Ensure the 'uploads' folder contains some files, then run the app to see the list.</p>"},{"location":"blog/flask-based-file-hosting-web-app--api--python-module--cli-app/#adding-a-login-page","title":"Adding a Login Page","text":"<ol> <li> <p>Download the login.html file into the templates folder.</p> </li> <li> <p>Add login-related functions to <code>app.py</code>:</p> </li> </ol> <pre><code># ... (Previous code)\n\ndef validate_credentials(username, password):\n    res = (username == app.config['USERNAME'] and password == app.config['PASSWORD'])\n    session['logged_in'] = res\n    return res\n\ndef is_logged_in():\n    return 'logged_in' in session and session['logged_in']\n\ndef login_required(f):\n    @wraps(f)\n    def decorated_function(*args, **kwargs):\n        if not is_logged_in():\n            session['previous_url'] = request.url\n            return redirect('/login')\n        return f(*args, **kwargs)\n    return decorated_function\n</code></pre> Here is how it works <ul> <li> <p><code>login_required</code> is a wrapper that use the function <code>is_logged_in</code> to check if a user is logged in</p> </li> <li> <p><code>validate_credentials</code> check if the <code>username</code> and <code>password</code> sent by the user match those we have from <code>constants.py</code></p> </li> </ul> <p>Now, we will create the login page and add the login wrapper to the home page</p> <pre><code>@app.route('/')\n@login_required\ndef index():\n    list_files = os.listdir(app.config['UPLOAD_FOLDER'])\n    files = [(filename, \"\") for filename in list_files]\n    return render_template('index.html', files=files)\n\n@app.route('/login', methods=['GET', 'POST'])\ndef login():\n\n    sucessful_login_redirect = lambda : redirect(session.pop('previous_url') if 'previous_url' in session else \"\\\\\")\n    default_login_render = lambda : render_template('login.html')\n\n    if is_logged_in():\n        return sucessful_login_redirect()\n\n    if request.method != 'POST':\n        return default_login_render()\n\n    username = request.form['username']\n    password = request.form['password']\n\n    if validate_credentials(username, password):\n        return sucessful_login_redirect()\n\n    return default_login_render()\n</code></pre> Here is how it works <ul> <li>The login page use the template <code>index.html</code>, a form with two fields: <code>username</code> and <code>password</code>. But if he is already logged in, he may be redirected to another page.</li> <li>When he add his credentials and send them, we will get <code>username</code> and <code>password</code> from the form.</li> <li>Then we will use the function <code>validate_credentials</code> to check them.</li> <li>If the credentials match, the user is redirected to the page he asks for. For example, in an unauthenticated user go the the home page, he is redirected to the login page. And if his crede,tials match, he is redirected back to the home page.</li> </ul>"},{"location":"blog/flask-based-file-hosting-web-app--api--python-module--cli-app/#create-your-credentials-to-access-the-app","title":"create your credentials to access the app","text":"<p>As i've 've said, i use the most basic authentication for this simple login protected web app.To add the credentials for the web app, create a file <code>.env</code> like .env.example</p> <p><code>.env</code></p> <pre><code>USERNAME=myuser\nPASSWORD=mypassword\n</code></pre> <p>Warning</p> <p>Modify it to match the credentials for your app</p> <p>Also, go into the constants.py file and make sure <code>DEBUG</code> is set to <code>False</code> to use it</p>"},{"location":"blog/flask-based-file-hosting-web-app--api--python-module--cli-app/#optional-session-timeout-and-logout-page","title":"(Optional) Session Timeout and Logout Page","text":"<p>You can add a timeout of the session. So a user will not stay logged in forever. It is important for data sentivive related apps. You can also let the user logout if he wants. There is a logout bouton in the home page.</p> <ol> <li> <p>Set the session timeout:</p> <pre><code># ... (Previous code)\n\n# Set the session timeout to 30 minutes (1800 seconds)\napp.config['PERMANENT_SESSION_LIFETIME'] = timedelta(minutes=30)\n</code></pre> </li> <li> <p>Add a logout page:</p> <pre><code># ... (Previous code)\n\n@app.route('/logout')\ndef logout():\n    session.clear()\n    return redirect('/login')\n</code></pre> </li> </ol>"},{"location":"blog/flask-based-file-hosting-web-app--api--python-module--cli-app/#adding-file-download-and-upload-features","title":"Adding File Download and Upload Features","text":"<ol> <li> <p>Add functions for file download:</p> <pre><code># ... (Previous code)\n\n@app.route('/uploads/&lt;path:filename&gt;')\ndef download(filename):\n    return send_from_directory(app.config['UPLOAD_FOLDER'], filename)\n</code></pre> </li> <li> <p>Add functions for file upload:</p> <pre><code># ... (Previous code)\n\n@app.route('/uploads/&lt;path:filename&gt;')\ndef download(filename):\n    return send_from_directory(app.config['UPLOAD_FOLDER'], filename)\n\n\ndef slugify_filename(filename):\n    # Split the filename and extension\n    _ = filename.rsplit('.', 1)\n    if len(_)&lt;2: return \n    base, extension = _\n    # Slugify the base part\n    slug_base = slugify(base)\n    # Join the slugified base with the original extension\n    slug_filename = f\"{slug_base}.{extension}\"\n    return slug_filename\n\ndef handle_file_saving(file):\n    filename = slugify_filename(file.filename)\n    file_save = app.config['UPLOAD_FOLDER'] / filename\n    print(f\"saving {file_save.resolve()}\")\n    file.save(file_save)\n    return filename\n\n@app.route('/upload', methods=['POST'])\n@login_required\ndef upload():\n    file = request.files['file']\n    if file:\n        filename = handle_file_saving(file)\n    return redirect('/')\n</code></pre> </li> </ol> Here is how it works <p>We have added a upload endpoint</p> <ul> <li>that will receive user files and save them using the <code>handle_file_saving</code> function</li> <li>that is protected with <code>login_required</code></li> <li>The function <code>slugify_filename</code> will rewrite the filename to use only lowercase alphanumeric characters an <code>-</code> as separators instead of space</li> <li><code>handle_file_saving</code> will save the file in the <code>uploads</code> directory</li> </ul> <p>The complete code with file download and upload features can be found in the GitHub repository.</p>"},{"location":"blog/flask-based-file-hosting-web-app--api--python-module--cli-app/#optional-adding-endpoints-for-open-file-raw-content-and-api","title":"(Optional) Adding Endpoints for Open File, Raw Content, and API","text":"<ol> <li> <p>Add endpoints for opening a file, displaying raw content, and API:</p> <pre><code># ... (Previous code)\n\n@app.route('/open/&lt;path:filename&gt;')\n@login_required\ndef open_file(filename):\n    file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)\n\n    if not os.path.exists(file_path):\n        return \"File not found\"\n\n    mime_type = get_content_type(file_path)\n\n    # Map .md and .mmd extensions to text/plain\n    if mime_type == 'text/markdown' or mime_type == 'text/x-markdown':\n        mime_type = 'text/plain'\n\n    if mime_type:\n        with open(file_path, 'rb') as file:\n            file_content = file.read()\n        return Response(file_content, content_type=mime_type)\n\n    return \"Unknown file type\"\n\n@app.route('/raw/&lt;path:filename&gt;')\n@login_required\ndef raw_file(filename):\n    file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)\n\n    if not os.path.exists(file_path):\n        return \"File not found\"\n\n    with open(file_path, 'rb') as file:\n        file_content = file.read()\n    return file_content\n</code></pre> </li> </ol> <p>The code for these features is available in the GitHub repository.</p>"},{"location":"blog/flask-based-file-hosting-web-app--api--python-module--cli-app/#optional-modifying-file-upload-to-filter-files","title":"(Optional) Modifying File Upload to Filter Files","text":"<p>To filter the files, you can use a database to add which files to show. To be simple, i've used a json file.</p> <p>So the</p> <ul> <li>home page will look into the json file to show the files</li> <li> <p>the upload page will save the file on the server and also add it in the json file</p> </li> <li> <p>Modify the listing feature to filter files using a JSON file:</p> <pre><code>def load_data_from_json():\n    if os.path.exists(app.config['DATA_FILE']):\n        with open(app.config['DATA_FILE'], 'r') as file:\n            try:\n                return json.load(file)\n            except json.JSONDecodeError:\n                pass\n    return {}\n\ndef get_files_with_dates():\n    data = load_data_from_json()\n    return [(filename, data[filename]) for filename in sorted(data, key=data.get) if (app.config['UPLOAD_FOLDER']/filename).exists()]\n\n@app.route('/')\n@login_required\ndef index():\n    files = get_files_with_dates()\n    return render_template('index.html', files=files)\n</code></pre> </li> <li> <p>Modify the upload feature to filter files using a JSON file:</p> <pre><code>def update_data_file(filename):\n    data = load_data_from_json()\n    data[filename] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    with open(app.config['DATA_FILE'], 'w') as file:\n        json.dump(data, file)\n\ndef handle_file_saving(file):\n    filename = slugify_filename(file.filename)\n    file_save = app.config['UPLOAD_FOLDER'] / filename\n    print(f\"saving {file_save.resolve()}\")\n    file.save(file_save)\n    update_data_file(filename)\n    return filename\n\n@app.route('/upload', methods=['POST'])\n@login_required\ndef upload():\n    file = request.files['file']\n    if file:\n        filename = handle_file_saving(file)\n    return redirect('/')\n</code></pre> </li> </ul> <p>The complete code with file filtering and other features is available in the GitHub repository.</p>"},{"location":"blog/flask-based-file-hosting-web-app--api--python-module--cli-app/#optional-adding-an-api-along-the-web-page","title":"(Optional) Adding an api along the web page","text":"<ol> <li> <p>login</p> <pre><code>@app.route('/api/login', methods=['POST'])\ndef api_login():\n    username = request.json.get('username')\n    password = request.json.get('password')\n\n    if validate_credentials(username, password):\n        return jsonify({'message': 'Login successful'})\n    else:\n        return jsonify({'message': 'Invalid credentials'}), 401\n</code></pre> </li> <li> <p>get all the files</p> <pre><code>@app.route('/api')\ndef api_index():\n    if not is_logged_in():\n        return jsonify({'message': 'Unauthorized'}), 401\n\n    files = get_files()\n    return jsonify({'files': files})\n</code></pre> </li> <li> <p>upload a file</p> <pre><code>@app.route('/api/upload', methods=['POST'])\ndef api_upload():\n    if not is_logged_in():\n        return jsonify({'message': 'Unauthorized'}), 401\n\n    file = request.files['file']\n    if file:\n        filename = handle_file_saving(file)\n        return jsonify({'message': f'File uploaded: {filename}'})\n    else:\n        return jsonify({'message': 'No file provided'}), 400\n</code></pre> </li> <li> <p>download a file</p> <pre><code>@app.route('/api/uploads/&lt;path:filename&gt;')\ndef api_download(filename):\n    if not is_logged_in():\n        return jsonify({'message': 'Unauthorized'}), 401\n\n    return send_from_directory(app.config['UPLOAD_FOLDER'], filename)\n</code></pre> </li> </ol>"},{"location":"blog/flask-based-file-hosting-web-app--api--python-module--cli-app/#bonus-how-to-use-the-api","title":"(Bonus) How to use the api","text":"<ul> <li>You can access the api with the routes <code>http://localhost:5000/api/*</code></li> <li>The file cli_app/cli_app.py to access the api along with a context manager to handle sessions</li> <li>you can read the api documentation</li> </ul>"},{"location":"blog/flask-based-file-hosting-web-app--api--python-module--cli-app/#bonus-how-to-use-the-cli-app","title":"(Bonus) How to use the cli app","text":"<ul> <li>The script cli_app/sharefile.py provides a cli app to access the api context manager</li> <li>Using your cli, you can list, upload and download files. The api will be called behind the hood by cli_app/cli_app.py</li> <li>you can read the cli-app documentation</li> </ul>"},{"location":"blog/flask-based-file-hosting-web-app--api--python-module--cli-app/#bonus-serving-static-files","title":"(Bonus) Serving Static Files","text":"<p>If you want to serve static files, add the following endpoint:</p> <pre><code>@app.route('/&lt;path:filename&gt;')\ndef static_files(filename):\n    return send_from_directory('static', filename)\n</code></pre> <p>Create a 'static' folder and place your static files inside it.</p> <p>It can be interesting for custom css/js files and others</p> <p>You can find all this code in the repository https://github.com/Hermann-web/simple-file-hosting-with-flask</p>"},{"location":"blog/flask-based-file-hosting-web-app--api--python-module--cli-app/#related-posts","title":"Related Posts","text":"<ul> <li>Cheat on Python Package Managers</li> </ul>"},{"location":"blog/laravel-pwa-integration-guide-for-mobile-views/","title":"Laravel PWA Integration Guide for Mobile Views","text":""},{"location":"blog/laravel-pwa-integration-guide-for-mobile-views/#introduction","title":"Introduction","text":"<p>Have you ever wanted to provide mobile views for your web applications without the hassle of native mobile app development?</p> <p>With the Laravel PWA package, you can effortlessly create mobile views for your Laravel application, catering to both Android and iOS users.</p> <p>This documentation serves as a step-by-step guide to help you integrate the Laravel PWA package into your Laravel application effortlessly. Whether you're a seasoned developer or just starting your journey with Laravel, this guide will walk you through the process, making PWA implementation a breeze.</p>"},{"location":"blog/laravel-pwa-integration-guide-for-mobile-views/#motivation-pwa-vs-native","title":"Motivation: PWA vs Native","text":"<p>Before diving into the details, let's explore the motivation behind using Progressive Web Apps (PWAs) compared to native mobile app development:</p> <code>Pros of PWA</code> <code>Cons of PWA</code> <ul> <li> <p>Cross-Platform Compatibility: PWAs work seamlessly across various platforms, including Android, iOS, and desktop browsers, eliminating the need for separate development efforts for each platform.</p> </li> <li> <p>Cost-Effectiveness: Developing a PWA is often more cost-effective than building separate native apps for different platforms, as it requires less time and resources.</p> </li> <li> <p>Easy Maintenance: With a single codebase for both web and mobile, maintaining a PWA is simpler and more efficient compared to managing separate native apps.</p> </li> </ul> <ul> <li> <p>Limited Device Access: While PWAs offer broad platform compatibility, they may have limited access to certain device features compared to native apps.</p> </li> <li> <p>Performance: Although PWAs have made significant strides in performance, they may still lag behind native apps in terms of speed and responsiveness, especially for complex applications.</p> </li> <li> <p>App Store Distribution: Unlike native apps, PWAs do not have direct access to app stores, potentially limiting their discoverability and distribution.</p> </li> </ul> <p>Now that we've explored the pros and cons, let's proceed with the integration of the Laravel PWA package into your Laravel application for creating mobile views.</p>"},{"location":"blog/laravel-pwa-integration-guide-for-mobile-views/#installation","title":"Installation","text":"<p>To begin, you'll need to install the Laravel PWA package into your Laravel application using Composer. Open your terminal and run the following command:</p> <pre><code>composer require silviolleite/laravelpwa --prefer-dist\n</code></pre>"},{"location":"blog/laravel-pwa-integration-guide-for-mobile-views/#vendor-publishing","title":"Vendor Publishing","text":"<p>After installing the package, you'll need to publish its assets to your application. Use the following Artisan command to publish the assets:</p> <pre><code>php artisan vendor:publish --provider=\"LaravelPWA\\Providers\\LaravelPWAServiceProvider\"\n</code></pre>"},{"location":"blog/laravel-pwa-integration-guide-for-mobile-views/#implementation","title":"Implementation","text":"<p>Once the package's assets are published, you can start implementing PWA features in your Laravel application.</p> <ol> <li>Open your Blade template file (e.g., <code>resources/views/layouts/app.blade.php</code>).</li> <li>Inside the <code>&lt;head&gt;</code> section of your template, add the <code>@laravelPWA</code> directive:</li> </ol> <pre><code>&lt;html&gt;\n&lt;head&gt;\n    &lt;title&gt;My Laravel PWA&lt;/title&gt;\n    &lt;!-- Other meta tags and stylesheets --&gt;\n    @laravelPWA\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;!-- Your application content --&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>Note: Ensure that you include the <code>@laravelPWA</code> directive in every Blade template where you want to enable PWA features.</p>"},{"location":"blog/laravel-pwa-integration-guide-for-mobile-views/#customization-options","title":"Customization Options","text":"<p>Certain aspects of your PWA can be customized to match your application's branding and identity. These include:</p> <ul> <li>App Name: Customize the name of your PWA.</li> <li>Description: Add a description to provide users with information about your PWA.</li> <li>Icons and Splashes: Customize the icons and splash screens displayed when launching the PWA.</li> </ul> <p>These customization options can be adjusted in the <code>config/laravelpwa.php</code> file as mentioned in the module repo</p>"},{"location":"blog/laravel-pwa-integration-guide-for-mobile-views/#conclusion","title":"Conclusion","text":"<p>Congratulations! You've successfully integrated the Laravel PWA package into your Laravel application, enabling Progressive Web App functionality. Your users can now enjoy a seamless web experience with features like offline access, push notifications, and more.</p> <p>With this easy-to-follow guide, you've unlocked the potential of PWA in your Laravel application, enhancing user engagement and satisfaction.</p> <p>Explore the possibilities of PWA further and stay tuned for updates and enhancements to the Laravel PWA package.</p>"},{"location":"blog/laravel-pwa-integration-guide-for-mobile-views/#related-pages","title":"Related pages","text":"<ul> <li>Setting Up Laravel Environment on Linux</li> <li>Setting Up Laravel Environment on Windows</li> </ul>"},{"location":"blog/setting-up-laravel-environment-on-ubuntu/","title":"Setting Up Laravel Environment on Ubuntu","text":"<p>This guide will help you set up the necessary environment to run a Laravel application on an Ubuntu system.</p> <p>So this document provides a step-by-step guide to set up Apache, PHP, MySQL/MariaDB, Composer, and phpMyAdmin for managing databases, while also ensuring MySQL root user password setup for a Laravel environment on Ubuntu.</p>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/setting-up-laravel-environment-on-ubuntu/#install-apache-and-php","title":"Install Apache and PHP","text":"<ul> <li>Install Apache</li> </ul> <pre><code>sudo apt update \nsudo apt install apache2\n</code></pre> <ul> <li>Enable the Apache service and start it:</li> </ul> <pre><code>sudo systemctl enable apache2\nsudo systemctl start apache2\n</code></pre> When systemd is not running in this container <p>If you encounter the error message <code>systemd\" is not running in this container due to its overhead. Use the \"service\" command to start services instead. e.g.: service --status-all</code>, use the <code>service</code> command instead of <code>systemctl</code>.</p> <p>For example, instead of <code>sudo systemctl enable apache2 &amp; sudo systemctl start apache2</code>, use <code>sudo service apache2 start</code>. To stop or reload the service, use <code>sudo service apache2 stop</code> or <code>sudo service apache2 reload</code>, respectively.</p> <p>Similarly, for stopping or reloading other services, use the <code>service</code> command instead of <code>systemctl</code>.</p> <ul> <li>Install php</li> </ul> <pre><code>sudo apt install php php-cli php-common php-mbstring php-xml php-zip php-mysql php-pgsql php-sqlite3 php-json php-bcmath php-gd php-tokenizer php-xmlwriter\n</code></pre> <p>Check <code>localhost</code> in your browser to ensure Apache is running.</p>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/setting-up-laravel-environment-on-ubuntu/#install-mariadbmysql","title":"Install MariaDB/MySQL","text":"<ul> <li>Install MariaDB MariaDB is an open-source relational database management system. Install it by running the following command:</li> </ul> <pre><code>sudo apt install mariadb-server\n</code></pre> <ul> <li>Install MySQL</li> </ul> <pre><code>sudo mysql_secure_installation\n</code></pre> <ul> <li>Enable and start MySQL:</li> </ul> <pre><code>sudo systemctl enable mysql\nsudo systemctl start mysql\nsudo systemctl status mysql\n</code></pre>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/setting-up-laravel-environment-on-ubuntu/#install-composer","title":"Install Composer","text":"<p>Composer is a dependency management tool for PHP. Install with the command below</p> <pre><code>sudo apt install composer\n</code></pre> <ul> <li>here is an altenative</li> </ul> <pre><code>curl -sS https://getcomposer.org/installer | php\nsudo mv composer.phar /usr/local/bin/composer\nsudo chmod +x /usr/local/bin/composer\n</code></pre>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/setting-up-laravel-environment-on-ubuntu/#install-phpmyadmin-for-database-management","title":"Install phpMyAdmin for Database Management","text":"<pre><code>sudo apt update\nsudo apt install phpmyadmin\nsudo ln -s /etc/phpmyadmin/apache.conf /etc/apache2/conf-available/phpmyadmin.conf\nsudo a2enconf phpmyadmin\nsudo systemctl reload apache2\n</code></pre> <p>Access phpMyAdmin at <code>http://localhost/phpmyadmin</code> to manage your databases.</p>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/setting-up-laravel-environment-on-ubuntu/#configure-mysql-root-user-password","title":"Configure MySQL Root User Password","text":"<p>because phpmyadmin refuse the passwordless login and also for security purposes</p> <p>Modify the MySQL configuration file:</p> <pre><code>sudo nano /etc/mysql/mariadb.conf.d/50-server.cnf\n</code></pre> <p>In some setups or older versions, this file might exist as the main configuration file for the MySQL/MariaDB server</p> <pre><code>sudo nano /etc/mysql/mysql.conf.d/mysqld.cnf\n</code></pre> <p>Add the following line under the <code>[mysqld]</code> section:</p> <pre><code>skip-grant-tables\n</code></pre> <p>Restart MySQL in safe mode to update the config:</p> <pre><code>sudo systemctl stop mysql\nsudo mysqld_safe --skip-grant-tables --skip-networking &amp;\n</code></pre> <p>If there's a process conflict, kill the processes and restart MySQL.</p> <p>In another terminal:</p> <pre><code>mysql -u root -p\n</code></pre> <pre><code>use mysql;\nupdate user set authentication_string=PASSWORD(\"new_password\") where User='root';\nflush privileges;\nquit;\n</code></pre> <p>Restart MySQL:</p> <pre><code>sudo systemctl stop mysql\nsudo systemctl start mysql\n</code></pre> <p>Access phpMyAdmin at <code>http://localhost/phpmyadmin</code> and use the updated root password.</p> <p>This environment should now be ready for running your Laravel application.</p>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/setting-up-laravel-environment-on-ubuntu/#bonus-create-a-new-laravel-project","title":"(Bonus) Create a New Laravel Project","text":"<p>Let's conclude this guide with a practical example by demonstrating how to create a new Laravel project using Composer.</p> <p>So we will apply the newly configured environment by creating a new Laravel project and accessing it through a web browser</p> <p>Now that you have set up your Laravel environment, let's create a new Laravel project using Composer.</p> <p>Run the following command in your terminal:</p> <pre><code>composer create-project --prefer-dist laravel/laravel my-laravel-app\n</code></pre> <p>This command will create a new Laravel project named <code>my-laravel-app</code> in the current directory. Replace <code>my-laravel-app</code> with your preferred project name.</p> <p>Navigate to the project directory:</p> <pre><code>cd my-laravel-app\n</code></pre> <p>Then, start the Laravel development server:</p> <pre><code>php artisan serve\n</code></pre> <p>Access your Laravel application by visiting <code>http://localhost:8000</code> in your web browser. You should see the default Laravel welcome page, confirming that your new Laravel project is up and running!</p>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/setting-up-laravel-environment-on-ubuntu/#setup-a-laravel-project-case-of-lavsms","title":"Setup a laravel project: case of lavsms","text":"","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/setting-up-laravel-environment-on-ubuntu/#run-xampp","title":"Run XAMPP","text":"<p>XAMPP is used to provide a local server environment to run your Laravel application.</p> <ul> <li>Start the XAMPP GUI application.</li> <li>Launch the Apache web server and MySQL database server.</li> <li> <p>Create a new database named <code>lavsms</code> using phpMyAdmin or another MySQL client.</p> <pre><code>mysql -u your_username -p -e \"CREATE DATABASE lavsms;\"\n</code></pre> </li> </ul>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/setting-up-laravel-environment-on-ubuntu/#database-configuration","title":"Database Configuration","text":"<p>Configure the database settings for your Laravel application.</p> <ul> <li> <p>Create an environment (<code>.env</code>) file by making a copy of the example file:</p> <pre><code>cp .env.example .env\n</code></pre> </li> <li> <p>Modify the database connection settings in the <code>.env</code> file to match your XAMPP setup:</p> <pre><code>DB_DATABASE=lavsms\nDB_USERNAME=root\nDB_PASSWORD=\n</code></pre> </li> </ul>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/setting-up-laravel-environment-on-ubuntu/#install-project-dependencies","title":"Install Project Dependencies","text":"<p>Install the necessary dependencies for your Laravel project.</p> <pre><code># Navigate to the project directory\ncd path/to/project\n\n# Update Composer dependencies (if needed)\ncomposer update\n\n# Install Composer dependencies\ncomposer install\n\n# Install Node.js dependencies\nnpm install\n</code></pre>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/setting-up-laravel-environment-on-ubuntu/#build","title":"Build","text":"<p>Perform necessary build steps for your Laravel application.</p> <pre><code># Generate an application key\nphp artisan key:generate\n\n# Clear the configuration cache\nphp artisan config:clear\n\n# create a symbolic link (`/public/storage`) from the storage directory (`/storage/app/public`) to the public directory (`/public/`)\nphp artisan storage:link\n</code></pre>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/setting-up-laravel-environment-on-ubuntu/#build-db","title":"Build Db","text":"<p>Prepare and set up your database for the Laravel application.</p> <pre><code># Run database migrations to create database tables\nphp artisan migrate\n\n# Seed the database with initial data (if needed)\nphp artisan db:seed   \n</code></pre>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/setting-up-laravel-environment-on-ubuntu/#run-development","title":"Run Development","text":"<p>Start the development server for your Laravel application.</p> <pre><code># Start the Laravel development server\nphp artisan serve\n</code></pre>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/setting-up-laravel-environment-on-ubuntu/#bonus-a-comparison-laravel-php-vs-django-python-mvc-like-architecture","title":"(Bonus) A Comparison: Laravel (PHP) vs. Django (Python) MVC-like Architecture","text":"<p>A brief comparison of Laravel's architecture to Django's MVC pattern.</p> <p>Both Laravel (PHP) and Django (Python) frameworks use a MVC-like architecture. Here are the analogies.</p> <code>Laravel (PHP)</code> <code>Django (Python)</code> <ol> <li>Routes: Defined in <code>routes/web.php</code>.<ul> <li>Invokes PHP controllers.</li> <li>Calls <code>resources\\views\\partials\\js\\custom_js.blade.php</code> (JavaScript) on form submission, writing to the console.</li> </ul> </li> <li>Serializers: Located in <code>app/http/requests</code>.<ul> <li>Used in controllers for data validation.</li> </ul> </li> <li>Controllers: Found in <code>app/http/controllers</code>.<ul> <li>Utilizes serializers automatically for data validation.</li> <li>Uses models for CRUD operations.</li> <li>Returns <code>parse(a_view, data_for_client)</code> similar to Django.</li> </ul> </li> <li>Models: Reside in <code>app/models</code>.<ul> <li>Utilized in controllers for CRUD operations.</li> </ul> </li> <li>Views (Blade): Located in <code>resources/views</code>.<ul> <li>Similar to PHP-client in Django, handling the presentation layer.</li> </ul> </li> </ol> <ol> <li>URL Patterns: Defined in <code>urls.py</code>.<ul> <li>Maps to Python views.</li> <li>Handles HTTP requests and defines the view functions.</li> </ul> </li> <li>Serializers: Often part of Django REST framework in Python.<ul> <li>Used for serialization and deserialization of data.</li> </ul> </li> <li>Views: Python files corresponding to the application's logic.<ul> <li>Utilizes serializers for data validation.</li> <li>Performs database operations and returns rendered templates.</li> </ul> </li> <li>Models: Represented as Python classes in <code>models.py</code>.<ul> <li>Represents the application's data structure.</li> <li>Interacts with the database via Django's ORM.</li> </ul> </li> <li>Templates: HTML files residing in <code>templates</code> directory.<ul> <li>Renders the user interface based on data provided by views.</li> </ul> </li> </ol>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/setting-up-laravel-environment-on-ubuntu/#related-pages","title":"Related pages","text":"<ul> <li>Guide to Installing MySQL and Connecting to Databases</li> <li>Setting Up Laravel Environment on Windows</li> <li>Laravel PWA Integration Guide for Mobile Views</li> </ul>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/setting-up-laravel-environment-on-windows/","title":"Setting Up Laravel Environment on Windows","text":"<p>This guide provides step-by-step instructions to set up a Laravel project on your local environment using XAMPP. If you encounter any issues, please refer to the version details provided below for context and troubleshooting.</p>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/setting-up-laravel-environment-on-windows/#introduction","title":"Introduction","text":"<p>Welcome to the \"Setting Up Laravel Environment on Windows\" tutorial! This guide helps you set up a strong development environment on your Windows system for effortless Laravel web application creation. You'll navigate through configuring Apache, PHP, MySQL/MariaDB, Composer, and phpMyAdmin, making your Windows system a powerful platform for Laravel development. Whether you're new or experienced in web development, this step-by-step tutorial ensures a smooth setup, enabling you to dive into Laravel effortlessly.</p> <p>In this tutorial, we'll cover:</p> <ol> <li> <p>Installation of Essential Tools: We'll start by installing XAMPP, Composer, and Node.js. These tools are the building blocks of your Laravel environment.</p> </li> <li> <p>XAMPP Configuration: We'll guide you through launching the Apache web server, setting up the MySQL database server, and configuring a database using phpMyAdmin.</p> </li> </ol> <ol> <li> <p>Database Setup for Laravel: You'll learn how to configure your Laravel application to interact with the database through the <code>.env</code> file.</p> </li> <li> <p>Dependency Management: We'll explore installing project dependencies using Composer and Node.js for your Laravel project.</p> </li> <li> <p>Project Setup and Run: You'll run essential commands to generate keys, perform database migrations, seed initial data, and finally, start the Laravel development server.</p> </li> <li> <p>Bonus - Laravel Pattern Overview: We'll provide a brief comparison of Laravel's architecture to the MVC pattern, similar to Django's structure.</p> </li> </ol> <p>Each section is designed to streamline your Laravel setup process, ensuring you have a robust environment ready to build dynamic web applications. Let's dive in and set up your Windows system for Laravel development!</p>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/setting-up-laravel-environment-on-windows/#version-details","title":"Version Details","text":"<p>from xammp installation</p> <ul> <li>PHP Version: PHP 8.2.4 (cli) (built: Mar 14 2023)</li> <li>XAMPP Control Panel Version: XAMPP for Windows 8.2.4</li> <li>MySQL Version: MariaDB 10.4.28, for Win64 (AMD64)</li> </ul> <p>from composer intallation</p> <ul> <li>Composer Version: Composer version 2.6.2 (2023-09-03)</li> </ul> <p>from node intallation</p> <ul> <li>Node Version: Node version v20.5.1 (2023-09-03)</li> </ul>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/setting-up-laravel-environment-on-windows/#installation-steps","title":"Installation Steps","text":"","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/setting-up-laravel-environment-on-windows/#install-xampp","title":"Install XAMPP","text":"<p>XAMPP is a free and open-source cross-platform web server solution stack package developed by Apache Friends, consisting mainly of the Apache HTTP Server, MariaDB database, and interpreters for scripts written in the PHP and Perl programming languages.</p> <ul> <li>Download from the website: XAMPP Download Page</li> <li>Put the folder <code>C:\\xampp\\php</code> (or equivalent) in the variables environment</li> </ul>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/setting-up-laravel-environment-on-windows/#install-composer-php-dependency-manager","title":"Install Composer (PHP Dependency Manager)","text":"<p>Composer is a tool for dependency management in PHP. It allows you to declare the libraries your project depends on and it will manage them for you.</p> <pre><code># Download the Composer installer for Windows: link found at https://getcomposer.org/doc/00-intro.md\ncurl -O https://getcomposer.org/Composer-Setup.exe\n\n# Run the Composer installer (this will open a GUI installer)\nstart Composer-Setup.exe\n</code></pre>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/setting-up-laravel-environment-on-windows/#install-nodejs-and-npm-node-package-manager","title":"Install Node.js and npm (Node Package Manager)","text":"<p>Node.js is an open-source, cross-platform, JavaScript runtime environment that executes JavaScript code outside a web browser. npm is the default package manager for Node.js.</p> <ul> <li>Download the Node.js (<code>node 20</code> preferably) installer for Windows from the official website: https://nodejs.org/</li> <li>Run the Node.js installer (this will also install npm).</li> </ul>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/setting-up-laravel-environment-on-windows/#setup-a-laravel-project-case-of-lavsms","title":"Setup a laravel project: case of lavsms","text":"","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/setting-up-laravel-environment-on-windows/#run-xampp","title":"Run XAMPP","text":"<p>XAMPP is used to provide a local server environment to run your Laravel application.</p> <ul> <li>Start the XAMPP GUI application.</li> <li>Launch the Apache web server and MySQL database server.</li> <li> <p>Create a new database named <code>lavsms</code> using phpMyAdmin or another MySQL client.</p> <pre><code>mysql -u your_username -p -e \"CREATE DATABASE lavsms;\"\n</code></pre> </li> </ul>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/setting-up-laravel-environment-on-windows/#database-configuration","title":"Database Configuration","text":"<p>Configure the database settings for your Laravel application.</p> <ul> <li> <p>Create an environment (<code>.env</code>) file by making a copy of the example file:</p> <pre><code>cp .env.example .env\n</code></pre> </li> <li> <p>Modify the database connection settings in the <code>.env</code> file to match your XAMPP setup:</p> <pre><code>DB_DATABASE=lavsms\nDB_USERNAME=root\nDB_PASSWORD=\n</code></pre> </li> </ul>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/setting-up-laravel-environment-on-windows/#install-project-dependencies","title":"Install Project Dependencies","text":"<p>Install the necessary dependencies for your Laravel project.</p> <pre><code># Navigate to the project directory\ncd path/to/project\n\n# Update Composer dependencies (if needed)\ncomposer update\n\n# Install Composer dependencies\ncomposer install\n\n# Install Node.js dependencies\nnpm install\n</code></pre>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/setting-up-laravel-environment-on-windows/#build","title":"Build","text":"<p>Perform necessary build steps for your Laravel application.</p> <pre><code># Generate an application key\nphp artisan key:generate\n\n# Clear the configuration cache\nphp artisan config:clear\n\n# create a symbolic link (`/public/storage`) from the storage directory (`/storage/app/public`) to the public directory (`/public/`)\nphp artisan storage:link\n</code></pre>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/setting-up-laravel-environment-on-windows/#build-db","title":"Build Db","text":"<p>Prepare and set up your database for the Laravel application.</p> <pre><code># Run database migrations to create database tables\nphp artisan migrate\n\n# Seed the database with initial data (if needed)\nphp artisan db:seed   \n</code></pre>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/setting-up-laravel-environment-on-windows/#run-development","title":"Run Development","text":"<p>Start the development server for your Laravel application.</p> <pre><code># Start the Laravel development server\nphp artisan serve\n</code></pre>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/setting-up-laravel-environment-on-windows/#bonus-a-comparison-laravel-php-vs-django-python-mvc-like-architecture","title":"(Bonus) A Comparison: Laravel (PHP) vs. Django (Python) MVC-like Architecture","text":"<p>A brief comparison of Laravel's architecture to Django's MVC pattern.</p> <p>Both Laravel (PHP) and Django (Python) frameworks use a MVC-like architecture. Here are the analogies.</p> <code>Laravel (PHP)</code> <code>Django (Python)</code> <ol> <li>Routes: Defined in <code>routes/web.php</code>.<ul> <li>Invokes PHP controllers.</li> <li>Calls <code>resources\\views\\partials\\js\\custom_js.blade.php</code> (JavaScript) on form submission, writing to the console.</li> </ul> </li> <li>Serializers: Located in <code>app/http/requests</code>.<ul> <li>Used in controllers for data validation.</li> </ul> </li> <li>Controllers: Found in <code>app/http/controllers</code>.<ul> <li>Utilizes serializers automatically for data validation.</li> <li>Uses models for CRUD operations.</li> <li>Returns <code>parse(a_view, data_for_client)</code> similar to Django.</li> </ul> </li> <li>Models: Reside in <code>app/models</code>.<ul> <li>Utilized in controllers for CRUD operations.</li> </ul> </li> <li>Views (Blade): Located in <code>resources/views</code>.<ul> <li>Similar to PHP-client in Django, handling the presentation layer.</li> </ul> </li> </ol> <ol> <li>URL Patterns: Defined in <code>urls.py</code>.<ul> <li>Maps to Python views.</li> <li>Handles HTTP requests and defines the view functions.</li> </ul> </li> <li>Serializers: Often part of Django REST framework in Python.<ul> <li>Used for serialization and deserialization of data.</li> </ul> </li> <li>Views: Python files corresponding to the application's logic.<ul> <li>Utilizes serializers for data validation.</li> <li>Performs database operations and returns rendered templates.</li> </ul> </li> <li>Models: Represented as Python classes in <code>models.py</code>.<ul> <li>Represents the application's data structure.</li> <li>Interacts with the database via Django's ORM.</li> </ul> </li> <li>Templates: HTML files residing in <code>templates</code> directory.<ul> <li>Renders the user interface based on data provided by views.</li> </ul> </li> </ol>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/setting-up-laravel-environment-on-windows/#related-pages","title":"Related pages","text":"<ul> <li>Guide to Installing MySQL and Connecting to Databases</li> <li>Setting Up Laravel Environment on Linux</li> <li>Laravel PWA Integration Guide for Mobile Views</li> </ul>","tags":["Apache","PHP","MySQL","Composer","phpMyAdmin"]},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part2/","title":"Guide pour utiliser NotebookLM","text":"<p>Guide pour utiliser NotebookLM : Simplifiez vos t\u00e2ches et organisez vos informations</p> <p>Bienvenue dans ce guide d\u00e9taill\u00e9 pour utiliser NotebookLM ! Que vous soyez un jeune \u00e9tudiant, un professionnel exp\u00e9riment\u00e9 ou simplement curieux, ce guide vous aidera \u00e0 ma\u00eetriser NotebookLM pour all\u00e9ger vos t\u00e2ches administratives, stocker et classer vos recherches, et bien plus encore...</p>"},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part2/#1-creer-un-nouveau-carnet-de-notes-notebook","title":"1. Cr\u00e9er un nouveau carnet de notes (Notebook)","text":""},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part2/#etapes-simples","title":"\u00c9tapes simples","text":"<ul> <li>Rendez-vous sur notebooklm.google depuis votre navigateur.</li> <li>Cliquez sur le bouton \"+ Nouveau carnet de notes\" (ou \"New notebook\")<sup>1</sup>.</li> <li>Donnez un nom \u00e0 votre carnet de notes (ex: \"Projet Alpha\", \"Recettes de cuisine\", \"Recherches historiques\").</li> <li>Cliquez sur \"Cr\u00e9er\" pour ouvrir votre nouveau carnet<sup>1</sup>.</li> </ul>"},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part2/#2-ajouter-des-sources-a-votre-carnet-de-notes","title":"2. Ajouter des sources \u00e0 votre carnet de notes","text":"<p>NotebookLM est puissant gr\u00e2ce aux sources que vous lui fournissez. Il peut analyser diff\u00e9rents types de documents.</p>"},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part2/#comment-ajouter-des-sources","title":"Comment ajouter des sources","text":"<ul> <li> <p>Dans votre carnet de notes, cherchez le bouton \"Ajouter des sources\" (g\u00e9n\u00e9ralement \"+ Add sources\" ou une ic\u00f4ne similaire).</p> </li> <li> <p>Vous avez plusieurs options pour ajouter des sources<sup>2</sup>:</p> </li> <li> <p>Depuis votre ordinateur : Cliquez sur \"T\u00e9l\u00e9charger un fichier\" et s\u00e9lectionnez des documents PDF, Google Docs, ou d'autres fichiers texte<sup>2</sup>.</p> </li> <li>Depuis Google Drive : Connectez votre compte Google Drive pour importer des documents directement<sup>2</sup>.</li> <li>Copier-coller du texte : Collez du texte directement dans la zone pr\u00e9vue \u00e0 cet effet (par exemple, un article de blog, une transcription)<sup>2</sup>.</li> <li> <p>Coller une URL de site web : Ajoutez l'URL d'une page web pour que NotebookLM en tire des informations<sup>2</sup>.</p> </li> <li> <p>Une fois les sources ajout\u00e9es, NotebookLM les analyse. Cela peut prendre quelques instants selon la taille et le nombre de fichiers<sup>2</sup>.</p> </li> </ul>"},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part2/#3-discuter-avec-lia-et-obtenir-des-resumes","title":"3. Discuter avec l'IA et obtenir des r\u00e9sum\u00e9s","text":"<p>Une fois vos sources ajout\u00e9es, vous pouvez interroger NotebookLM pour obtenir des informations, des r\u00e9sum\u00e9s ou des id\u00e9es.</p>"},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part2/#comment-discuter-avec-lia","title":"Comment discuter avec l'IA","text":"<ul> <li>Apr\u00e8s avoir ajout\u00e9 vos sources, vous verrez une barre de discussion ou une zone de texte en bas ou sur le c\u00f4t\u00e9 de l'interface<sup>3</sup>.</li> <li>Tapez votre question ou votre demande dans cette barre. Par exemple : \"R\u00e9sumez les points cl\u00e9s du document sur le projet X\", \"Quels sont les avantages de la solution Y mentionn\u00e9s dans ce fichier ?\", ou \"G\u00e9n\u00e9rez un plan d'action bas\u00e9 sur ces notes\"<sup>3</sup>.</li> <li>Appuyez sur Entr\u00e9e ou cliquez sur l'ic\u00f4ne d'envoi.</li> <li>L'IA g\u00e9n\u00e9rera une r\u00e9ponse bas\u00e9e uniquement sur les sources que vous avez fournies, ce qui garantit la pertinence et la fiabilit\u00e9 des informations<sup>3</sup>.</li> </ul>"},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part2/#comprendre-les-citations-sources","title":"Comprendre les citations (Sources)","text":"<ul> <li>Les r\u00e9ponses de NotebookLM incluent des citations (par exemple, <code>[1]</code>, <code>[2]</code>) qui renvoient aux sources sp\u00e9cifiques que vous avez ajout\u00e9es<sup>4</sup>.</li> <li>Cliquez sur ces citations pour voir quelle partie de votre document a \u00e9t\u00e9 utilis\u00e9e pour g\u00e9n\u00e9rer la r\u00e9ponse. Cela vous aide \u00e0 v\u00e9rifier l'information et \u00e0 trouver rapidement la section originale<sup>4</sup>.</li> </ul>"},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part2/#4-organiser-vos-notes-et-informations","title":"4. Organiser vos notes et informations","text":"<p>NotebookLM vous aide \u00e0 structurer vos pens\u00e9es et vos recherches.</p>"},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part2/#comment-organiser-vos-informations","title":"Comment organiser vos informations","text":"<ul> <li>Ajouter des notes : Vous pouvez prendre des notes directement dans votre carnet, \u00e0 c\u00f4t\u00e9 de vos sources ou des discussions avec l'IA<sup>5</sup>.</li> <li>Cr\u00e9er des r\u00e9sum\u00e9s automatiques : Demandez \u00e0 l'IA de g\u00e9n\u00e9rer des r\u00e9sum\u00e9s pour des sections sp\u00e9cifiques de vos documents ou pour l'ensemble de votre carnet<sup>5</sup>.</li> <li>G\u00e9n\u00e9rer des plans et des id\u00e9es : Utilisez l'IA pour structurer des plans d'expos\u00e9, des \u00e9bauches d'articles, ou pour brainstormer des id\u00e9es en fonction de vos sources<sup>5</sup>.</li> <li>Mettre en \u00e9vidence les passages importants : S\u00e9lectionnez du texte dans vos sources et demandez \u00e0 NotebookLM de vous en extraire des points cl\u00e9s ou de les r\u00e9sumer<sup>6</sup>.</li> </ul>"},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part2/#5-utiliser-les-guides-generes-par-lia","title":"5. Utiliser les guides g\u00e9n\u00e9r\u00e9s par l'IA","text":"<p>NotebookLM peut cr\u00e9er des \"Guides\" bas\u00e9s sur vos sources pour vous aider \u00e0 explorer des sujets en profondeur.</p>"},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part2/#comment-utiliser-les-guides","title":"Comment utiliser les guides","text":"<ul> <li>Dans certains cas, NotebookLM peut sugg\u00e9rer de cr\u00e9er un guide bas\u00e9 sur le contenu de votre carnet<sup>7</sup>.</li> <li>Cliquez sur \"Cr\u00e9er un guide\" ou une option similaire.</li> <li>L'IA organisera les informations de vos sources en sections th\u00e9matiques, facilitant la navigation et la compr\u00e9hension des sujets complexes<sup>7</sup>.</li> <li>Les guides sont particuli\u00e8rement utiles pour \u00e9tudier, pr\u00e9parer des pr\u00e9sentations ou explorer de nouvelles th\u00e9matiques.</li> </ul>"},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part2/#6-partager-vos-carnets-de-notes","title":"6. Partager vos carnets de notes","text":"<p>Vous pouvez partager vos carnets de notes avec d'autres personnes pour collaborer ou simplement diffuser vos recherches.</p>"},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part2/#comment-partager-un-carnet","title":"Comment partager un carnet","text":"<ul> <li>Dans votre carnet de notes, cherchez l'ic\u00f4ne de partage (souvent un symbole de fl\u00e8che ou de trois points connect\u00e9s) ou le bouton \"Partager\"<sup>8</sup>.</li> <li>Vous pourrez alors g\u00e9n\u00e9rer un lien partageable.</li> <li>Attention : Assurez-vous de bien comprendre les options de partage (lecture seule, modification) avant d'envoyer le lien<sup>8</sup>.</li> </ul>"},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part2/#7-gerer-et-retrouver-vos-carnets","title":"7. G\u00e9rer et retrouver vos carnets","text":""},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part2/#ou-retrouver-vos-carnets-passes","title":"O\u00f9 retrouver vos carnets pass\u00e9s","text":"<ul> <li>Sur la page principale de NotebookLM, vous verrez la liste de tous vos carnets de notes.</li> <li>Cliquez sur un carnet pour l'ouvrir et reprendre votre travail l\u00e0 o\u00f9 vous l'avez laiss\u00e9.</li> <li>Vous pouvez rechercher des carnets par nom si vous en avez beaucoup.</li> </ul>"},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part2/#attention","title":"Attention","text":"<ul> <li>Si vous supprimez un carnet de notes, toutes les sources et discussions qu'il contient seront d\u00e9finitivement effac\u00e9es<sup>9</sup>.</li> </ul> <p>En utilisant NotebookLM, vous transformez vos documents en une base de connaissances interactive et personnalis\u00e9e, pr\u00eate \u00e0 r\u00e9pondre \u00e0 toutes vos questions et \u00e0 vous aider \u00e0 organiser vos id\u00e9es.</p> <p>\\&lt;div style=\"text-align: center\"&gt;\u2042\\&lt;/div&gt;</p> <ol> <li> <p>Google Workspace Updates: Build your own AI assistant with NotebookLM \u21a9\u21a9</p> </li> <li> <p>Google Chrome : Utiliser NotebookLM \u21a9\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>Google Workspace Updates: Get to know NotebookLM, a Google AI experiment \u21a9\u21a9\u21a9</p> </li> <li> <p>NotebookLM - Your AI-powered research assistant \u21a9\u21a9</p> </li> <li> <p>Google Workspace Updates: NotebookLM makes your research easier with AI-generated notes, outlines, and summaries \u21a9\u21a9\u21a9</p> </li> <li> <p>How to use NotebookLM - YouTube \u21a9</p> </li> <li> <p>Google Blog: How NotebookLM helps you learn anything \u21a9\u21a9</p> </li> <li> <p>How to Share NotebookLM Projects - YouTube \u21a9\u21a9</p> </li> <li> <p>NotebookLM support page \u21a9</p> </li> </ol>"},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part1/","title":"Guide Perplexity AI and NotebookLM: Tutoriels pour Tous (part1)","text":""},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part1/#guide-perplexity-ai-tutoriels-pour-tous","title":"Guide Perplexity AI : Tutoriels pour Tous","text":"<p>Bienvenue dans ce guide pas \u00e0 pas pour utiliser Perplexity AI, pens\u00e9 pour tous : jeunes, moins jeunes, d\u00e9butants ou curieux. Chaque section propose des instructions simples, des ic\u00f4nes pour rep\u00e9rer les actions, et des indications pour ins\u00e9rer vos propres captures d\u2019\u00e9cran (pensez \u00e0 ajouter des balises <code>alt</code> pour l\u2019accessibilit\u00e9).</p>"},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part1/#1-se-connecter-a-son-compte-perplexity","title":"1. Se connecter \u00e0 son compte Perplexity","text":""},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part1/#etapes-simples","title":"\u00c9tapes simples","text":"<ul> <li>Rendez-vous sur www.perplexity.ai depuis votre navigateur<sup>1</sup>.</li> <li>Entrez votre adresse e-mail et votre mot de passe, ou choisissez la connexion via Google<sup>1</sup>.</li> <li>Si vous avez oubli\u00e9 votre mot de passe, cliquez sur \u00ab Mot de passe oubli\u00e9 ? \u00bb et suivez les instructions re\u00e7ues par e-mail<sup>1</sup>.</li> <li>Apr\u00e8s connexion, vous acc\u00e9dez \u00e0 l\u2019interface principale.</li> </ul>"},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part1/#2-discuter-avec-lia-et-comprendre-les-sources","title":"2. Discuter avec l\u2019IA et comprendre les sources","text":""},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part1/#comment-discuter-avec-lia","title":"Comment discuter avec l\u2019IA","text":"<ul> <li>Tapez votre question dans la barre centrale, puis appuyez sur Entr\u00e9e ou cliquez sur l\u2019ic\u00f4ne fl\u00e8che<sup>2</sup><sup>3</sup>.</li> <li>L\u2019IA r\u00e9pond en quelques secondes, avec une r\u00e9ponse claire et facile \u00e0 lire<sup>2</sup><sup>3</sup>.</li> <li>Astuce : Posez vos questions comme \u00e0 une personne r\u00e9elle, et soyez pr\u00e9cis si possible<sup>3</sup>.</li> </ul> <p>![[resp.png]]</p>"},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part1/#comprendre-les-sources","title":"Comprendre les sources","text":"<ul> <li>Sous chaque r\u00e9ponse, vous verrez des num\u00e9ros (ex : <sup>2</sup>, <sup>3</sup>, <sup>4</sup>) : ce sont les sources utilis\u00e9es par l\u2019IA<sup>3</sup>.</li> <li>Cliquez sur un num\u00e9ro pour afficher la source compl\u00e8te dans une nouvelle fen\u00eatre<sup>3</sup>.</li> </ul>"},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part1/#3-retrouver-ses-anciennes-discussions","title":"3. Retrouver ses anciennes discussions","text":""},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part1/#ou-retrouver-vos-conversations-passees","title":"O\u00f9 retrouver vos conversations pass\u00e9es","text":"<ul> <li>Sur la page principale, cherchez l'icone Accueil sur la gauche \ud83d\udd0d.</li> <li>Cherchez le menu Biblioth\u00e8que ou Historique sur la gauche<sup>2</sup><sup>5</sup>. </li> <li>Cliquez dessus pour voir la liste de toutes vos discussions pr\u00e9c\u00e9dentes. Vous pouvez en rouvrir une en un clic<sup>2</sup><sup>5</sup>.</li> <li>Attention : Si vous supprimez une discussion, elle ne pourra pas \u00eatre r\u00e9cup\u00e9r\u00e9e<sup>6</sup><sup>5</sup>.</li> </ul>"},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part1/#4-faire-une-recherche-sur-le-web","title":"4. Faire une recherche sur le web","text":""},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part1/#comment-proceder","title":"Comment proc\u00e9der","text":"<ul> <li>Tapez votre question dans la barre de recherche<sup>2</sup><sup>3</sup>.</li> <li>Perplexity va chercher sur Internet et vous affiche une r\u00e9ponse avec des sources \u00e0 jour<sup>2</sup><sup>3</sup>.</li> <li>Pour approfondir, cliquez sur les num\u00e9ros de sources en bas de la r\u00e9ponse<sup>3</sup>.</li> </ul>"},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part1/#5-faire-une-recherche-academique","title":"5. Faire une recherche acad\u00e9mique","text":""},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part1/#etapes-a-suivre","title":"\u00c9tapes \u00e0 suivre","text":"<ul> <li>Creer une nouvelle discussion en cliquant sur l'icone \u00e0 gauche \u2795. Puis, vous serez sur une nouvelle page</li> <li>Dans la barre de recherche, cliquez sur le menu d\u00e9roulant (souvent nomm\u00e9 \u00ab Mode \u00bb ou \u00ab Focus \u00bb)<sup>3</sup><sup>7</sup>. </li> <li>S\u00e9lectionnez Academique pour cibler les articles scientifiques, th\u00e8ses, etc.<sup>3</sup><sup>7</sup>.</li> <li>Posez votre question : l\u2019IA vous donnera des r\u00e9ponses issues de publications reconnues, avec des r\u00e9f\u00e9rences claires<sup>7</sup><sup>8</sup><sup>9</sup>.</li> </ul>"},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part1/#6-ajouter-un-fichier-a-une-discussion","title":"6. Ajouter un fichier \u00e0 une discussion","text":"<p>Ic\u00f4ne \u00e0 utiliser : Capture d\u2019\u00e9cran sugg\u00e9r\u00e9e : Bouton \u00ab + Attacher \u00bb (alt=\"Bouton pour ajouter un fichier \u00e0 la discussion\")</p> <p></p>"},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part1/#comment-faire","title":"Comment faire","text":"<ul> <li>Lors d\u2019une nouvelle question, cliquez sur le bouton + Attacher (\ud83d\udcce) \u00e0 droite de la barre de recherche<sup>2</sup><sup>10</sup><sup>11</sup>.</li> <li>S\u00e9lectionnez votre fichier (PDF, image, texte, etc.)<sup>10</sup><sup>11</sup><sup>12</sup>. </li> <li>L\u2019IA analysera le contenu du fichier pour r\u00e9pondre \u00e0 vos questions sp\u00e9cifiques<sup>11</sup><sup>12</sup>.</li> <li>Limite : 25 Mo par fichier, jusqu\u2019\u00e0 4 fichiers \u00e0 la fois pour les utilisateurs standards<sup>11</sup>. </li> </ul>"},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part1/#7-exporter-une-discussion-ou-un-fichier","title":"7. Exporter une discussion ou un fichier","text":"<p>Ic\u00f4ne \u00e0 utiliser : Capture d\u2019\u00e9cran sugg\u00e9r\u00e9e : Menu d\u2019export (alt=\"Menu pour exporter ou enregistrer une discussion\")</p>"},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part1/#exporter-une-discussion","title":"Exporter une discussion","text":"<ul> <li>Dans une discussion, cherchez en haut \u00e0 droite, l'icone <code>\u22ef</code> l\u2019option Exporter ou utilisez une extension comme \u00ab Save my Chatbot \u00bb pour sauvegarder la conversation au format Markdown ou PDF<sup>13</sup><sup>14</sup><sup>15</sup>. </li> <li>Pratique pour garder une trace ou partager vos \u00e9changes avec d\u2019autres personnes<sup>16</sup><sup>14</sup>.</li> <li>Vous pouvez \u00e9galement g\u00e9n\u00e9rer un lien et partager avec d'autres personnes </li> </ul>"},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part1/#8-generer-les-fichiers-avec-la","title":"8. G\u00e9n\u00e9rer les fichiers avec l'A","text":"<p>Dans une question, vous pouvez demander \u00e0 l'AI de g\u00e9n\u00e9rer un fichier</p> <p></p> <p>Pour trouver les fichier, cliquer sur l'onglet \"Etapes\" </p> <p></p>"},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part1/#tutoriels-avances-perplexity-rapports-approfondis-personnalisation-collaboration","title":"Tutoriels Avanc\u00e9s Perplexity : Rapports approfondis, Personnalisation, Collaboration","text":""},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part1/#8-creer-des-rapports-de-recherche-approfondis-deep-research","title":"8. Cr\u00e9er des rapports de recherche approfondis (Deep Research)","text":""},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part1/#a-quoi-ca-sert","title":"\u00c0 quoi \u00e7a sert\u202f?","text":"<p>Le mode \u00ab\u202fRecherche approfondie\u202f\u00bb (Deep Research) permet d\u2019obtenir des rapports d\u00e9taill\u00e9s et structur\u00e9s sur des sujets complexes, en quelques minutes seulement. L\u2019IA analyse des dizaines de sources, synth\u00e9tise l\u2019information et vous livre un document clair, id\u00e9al pour des \u00e9tudes, des dossiers ou des analyses pouss\u00e9es<sup>20</sup><sup>21</sup><sup>22</sup>.</p>"},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part1/#etapes-simples_1","title":"\u00c9tapes simples","text":"<ul> <li>Acc\u00e9dez \u00e0 Perplexity et connectez-vous \u00e0 votre compte.</li> <li>Saisissez votre question dans la barre de recherche, puis s\u00e9lectionnez le mode \u00ab\u202fRecherche approfondie\u202f\u00bb ou \u00ab\u202fDeep Research\u202f\u00bb.</li> <li>Lancez la recherche : l\u2019IA va effectuer plusieurs recherches li\u00e9es, analyser les sources et r\u00e9diger un rapport complet. </li> <li>Consultez le rapport : vous pouvez voir le raisonnement \u00e9tape par \u00e9tape, les sources utilis\u00e9es et la synth\u00e8se finale. </li> <li>Exportez ou partagez le rapport\u202f: cliquez sur l\u2019option d\u2019export (PDF, Word, page partag\u00e9e) pour sauvegarder ou envoyer votre analyse \u00e0 d\u2019autres personnes<sup>20</sup><sup>21</sup><sup>22</sup>.</li> </ul> <p>Astuce\u202f: Ce mode est id\u00e9al pour les \u00e9tudes de march\u00e9, les analyses techniques ou la r\u00e9daction de m\u00e9moires universitaires. Par exemple, vous pouvez exporter les r\u00e9sultats sous la forme d'un blog </p> <p></p>"},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part1/#9-personnaliser-les-resultats-de-lia","title":"9. Personnaliser les r\u00e9sultats de l\u2019IA","text":""},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part1/#pourquoi-personnaliser","title":"Pourquoi personnaliser\u202f?","text":"<p>Adapter Perplexity \u00e0 vos besoins permet d\u2019obtenir des r\u00e9ponses plus pertinentes et adapt\u00e9es \u00e0 votre profil, vos centres d\u2019int\u00e9r\u00eat ou votre style de communication<sup>20</sup><sup>23</sup><sup>24</sup>.</p>"},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part1/#comment-faire_1","title":"Comment faire\u202f?","text":"<ul> <li>Acc\u00e9dez aux param\u00e8tres\u202f: cliquez sur votre photo de profil (g\u00e9n\u00e9ralement en bas \u00e0 droite de l\u2019interface).  </li> <li>Remplissez votre profil\u202f:</li> <li>Centres d\u2019int\u00e9r\u00eat\u202f: indiquez vos sujets favoris (sciences, cuisine, voyages\u2026).</li> <li>Style de r\u00e9ponse\u202f: choisissez entre un ton formel ou d\u00e9contract\u00e9, des r\u00e9ponses courtes ou d\u00e9taill\u00e9es.</li> <li>Langue et localisation\u202f: s\u00e9lectionnez la langue de pr\u00e9f\u00e9rence et, si besoin, votre r\u00e9gion.</li> <li>Objectifs\u202f: pr\u00e9cisez ce que vous attendez de Perplexity (aide aux devoirs, veille professionnelle, etc.).</li> <li>Enregistrez vos pr\u00e9f\u00e9rences\u202f: ces r\u00e9glages s\u2019appliqueront \u00e0 toutes vos futures recherches, rendant l\u2019IA plus efficace et personnalis\u00e9e<sup>20</sup><sup>23</sup><sup>24</sup>.</li> </ul> <p>Astuce\u202f: Les abonn\u00e9s Pro peuvent aussi choisir le mod\u00e8le d\u2019IA utilis\u00e9 pour chaque recherche (ex\u202f: GPT-4o, Claude 3), pour encore plus de personnalisation<sup>24</sup>.</p>"},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part1/#10-collaborer-dans-les-espaces-de-travail-spaces","title":"10. Collaborer dans les espaces de travail (Spaces)","text":""},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part1/#quest-ce-quun-espace-de-travail","title":"Qu\u2019est-ce qu\u2019un espace de travail\u202f?","text":"<p>Un \u00ab\u202fSpace\u202f\u00bb est un espace collaboratif o\u00f9 vous pouvez organiser vos recherches, partager des fichiers et travailler \u00e0 plusieurs sur des projets communs (\u00e9tudes, travaux de groupe, veille d\u2019\u00e9quipe\u2026)<sup>25</sup>.</p>"},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part1/#etapes-pour-collaborer","title":"\u00c9tapes pour collaborer","text":"<ul> <li>Dans le menu \u00e0 gauche, clickez sur le bouton espace </li> <li>Cr\u00e9ez un espace\u202f: cliquez sur \u00ab\u202fCr\u00e9er un espace\u202f\u00bb dans la barre lat\u00e9rale, donnez-lui un nom et une description.  </li> <li>Ajoutez des membres\u202f: invitez d\u2019autres utilisateurs par e-mail ou lien de partage.</li> <li>Centralisez vos documents\u202f: importez notes, fichiers, liens et ressources dans l\u2019espace.</li> <li>Posez des questions \u00e0 l\u2019IA\u202f: chaque membre peut interroger l\u2019IA sur les documents partag\u00e9s, g\u00e9n\u00e9rer des synth\u00e8ses, ou demander des plans de projet.</li> <li>Organisez la collaboration\u202f: cr\u00e9ez des guides d\u2019\u00e9tude, des calendriers partag\u00e9s, divisez les t\u00e2ches et suivez l\u2019avancement du groupe<sup>25</sup>.</li> </ul> <p>Exemples d\u2019usages\u202f:</p> <ul> <li>Pr\u00e9parer un expos\u00e9 ou un rapport \u00e0 plusieurs.</li> <li>Centraliser les ressources d\u2019une \u00e9quipe projet.</li> <li> <p>G\u00e9n\u00e9rer des outils d\u2019\u00e9tude personnalis\u00e9s pour une classe ou un groupe d\u2019apprentissage<sup>25</sup>.</p> </li> <li> <p>Cliquez sur Cr\u00e9er un espace dans la barre lat\u00e9rale<sup>17</sup><sup>19</sup>. </p> </li> <li> <p>Donnez un nom \u00e0 l\u2019espace, ajoutez une description et, si besoin, des instructions personnalis\u00e9es pour l\u2019IA<sup>17</sup><sup>19</sup>.</p> </li> <li> <p>Ajoutez vos fichiers et commencez \u00e0 poser des questions ou \u00e0 collaborer avec d\u2019autres utilisateurs<sup>18</sup><sup>19</sup>.</p> </li> </ul> \u2042"},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part1/#guide-pour-utiliser-notebooklm","title":"Guide pour utiliser NotebookLM","text":"<p>Guide pour utiliser NotebookLM : Simplifiez vos t\u00e2ches et organisez vos informations</p> <p>Bienvenue dans ce guide d\u00e9taill\u00e9 pour utiliser NotebookLM ! Que vous soyez un jeune \u00e9tudiant, un professionnel exp\u00e9riment\u00e9 ou simplement curieux, ce guide vous aidera \u00e0 ma\u00eetriser NotebookLM pour all\u00e9ger vos t\u00e2ches administratives, stocker et classer vos recherches, et bien plus encore. Chaque section propose des instructions simples, des ic\u00f4nes pour rep\u00e9rer les actions, et des indications pour ins\u00e9rer vos propres captures d'\u00e9cran (pensez \u00e0 ajouter des balises <code>alt</code> pour l'accessibilit\u00e9).</p>"},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part1/#1-creer-un-nouveau-carnet-de-notes-notebook","title":"1. Cr\u00e9er un nouveau carnet de notes (Notebook)","text":""},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part1/#icone-a-utiliser","title":"Ic\u00f4ne \u00e0 utiliser","text":""},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part1/#etapes-simples_2","title":"\u00c9tapes simples","text":"<ul> <li>Rendez-vous sur notebooklm.google depuis votre navigateur.</li> <li>Cliquez sur le bouton \"+ Nouveau carnet de notes\" (ou \"New notebook\")<sup>26</sup>.</li> <li>Donnez un nom \u00e0 votre carnet de notes (ex: \"Projet Alpha\", \"Recettes de cuisine\", \"Recherches historiques\").</li> <li>Cliquez sur \"Cr\u00e9er\" pour ouvrir votre nouveau carnet<sup>26</sup>.</li> </ul>"},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part1/#2-ajouter-des-sources-a-votre-carnet-de-notes","title":"2. Ajouter des sources \u00e0 votre carnet de notes","text":"<p>NotebookLM est puissant gr\u00e2ce aux sources que vous lui fournissez. Il peut analyser diff\u00e9rents types de documents.</p>"},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part1/#icone-a-utiliser_1","title":"Ic\u00f4ne \u00e0 utiliser","text":""},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part1/#comment-ajouter-des-sources","title":"Comment ajouter des sources","text":"<ul> <li> <p>Dans votre carnet de notes, cherchez le bouton \"Ajouter des sources\" (g\u00e9n\u00e9ralement \"+ Add sources\" ou une ic\u00f4ne similaire).</p> </li> <li> <p>Vous avez plusieurs options pour ajouter des sources<sup>27</sup>:</p> </li> <li> <p>Depuis votre ordinateur : Cliquez sur \"T\u00e9l\u00e9charger un fichier\" et s\u00e9lectionnez des documents PDF, Google Docs, ou d'autres fichiers texte<sup>27</sup>.</p> </li> <li>Depuis Google Drive : Connectez votre compte Google Drive pour importer des documents directement<sup>27</sup>.</li> <li>Copier-coller du texte : Collez du texte directement dans la zone pr\u00e9vue \u00e0 cet effet (par exemple, un article de blog, une transcription)<sup>27</sup>.</li> <li> <p>Coller une URL de site web : Ajoutez l'URL d'une page web pour que NotebookLM en tire des informations<sup>27</sup>.</p> </li> <li> <p>Une fois les sources ajout\u00e9es, NotebookLM les analyse. Cela peut prendre quelques instants selon la taille et le nombre de fichiers<sup>27</sup>.</p> </li> </ul>"},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part1/#3-discuter-avec-lia-et-obtenir-des-resumes","title":"3. Discuter avec l'IA et obtenir des r\u00e9sum\u00e9s","text":"<p>Une fois vos sources ajout\u00e9es, vous pouvez interroger NotebookLM pour obtenir des informations, des r\u00e9sum\u00e9s ou des id\u00e9es.</p>"},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part1/#icone-a-utiliser_2","title":"Ic\u00f4ne \u00e0 utiliser","text":""},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part1/#comment-discuter-avec-lia_1","title":"Comment discuter avec l'IA","text":"<ul> <li>Apr\u00e8s avoir ajout\u00e9 vos sources, vous verrez une barre de discussion ou une zone de texte en bas ou sur le c\u00f4t\u00e9 de l'interface<sup>28</sup>.</li> <li>Tapez votre question ou votre demande dans cette barre. Par exemple : \"R\u00e9sumez les points cl\u00e9s du document sur le projet X\", \"Quels sont les avantages de la solution Y mentionn\u00e9s dans ce fichier ?\", ou \"G\u00e9n\u00e9rez un plan d'action bas\u00e9 sur ces notes\"<sup>28</sup>.</li> <li>Appuyez sur Entr\u00e9e ou cliquez sur l'ic\u00f4ne d'envoi.</li> <li>L'IA g\u00e9n\u00e9rera une r\u00e9ponse bas\u00e9e uniquement sur les sources que vous avez fournies, ce qui garantit la pertinence et la fiabilit\u00e9 des informations<sup>28</sup>.</li> </ul>"},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part1/#comprendre-les-citations-sources","title":"Comprendre les citations (Sources)","text":"<ul> <li>Les r\u00e9ponses de NotebookLM incluent des citations (par exemple, <code>[1]</code>, <code>[2]</code>) qui renvoient aux sources sp\u00e9cifiques que vous avez ajout\u00e9es<sup>29</sup>.</li> <li>Cliquez sur ces citations pour voir quelle partie de votre document a \u00e9t\u00e9 utilis\u00e9e pour g\u00e9n\u00e9rer la r\u00e9ponse. Cela vous aide \u00e0 v\u00e9rifier l'information et \u00e0 trouver rapidement la section originale<sup>29</sup>.</li> </ul>"},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part1/#4-organiser-vos-notes-et-informations","title":"4. Organiser vos notes et informations","text":"<p>NotebookLM vous aide \u00e0 structurer vos pens\u00e9es et vos recherches.</p>"},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part1/#icone-a-utiliser_3","title":"Ic\u00f4ne \u00e0 utiliser","text":""},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part1/#comment-organiser-vos-informations","title":"Comment organiser vos informations","text":"<ul> <li>Ajouter des notes : Vous pouvez prendre des notes directement dans votre carnet, \u00e0 c\u00f4t\u00e9 de vos sources ou des discussions avec l'IA<sup>30</sup>.</li> <li>Cr\u00e9er des r\u00e9sum\u00e9s automatiques : Demandez \u00e0 l'IA de g\u00e9n\u00e9rer des r\u00e9sum\u00e9s pour des sections sp\u00e9cifiques de vos documents ou pour l'ensemble de votre carnet<sup>30</sup>.</li> <li>G\u00e9n\u00e9rer des plans et des id\u00e9es : Utilisez l'IA pour structurer des plans d'expos\u00e9, des \u00e9bauches d'articles, ou pour brainstormer des id\u00e9es en fonction de vos sources<sup>30</sup>.</li> <li>Mettre en \u00e9vidence les passages importants : S\u00e9lectionnez du texte dans vos sources et demandez \u00e0 NotebookLM de vous en extraire des points cl\u00e9s ou de les r\u00e9sumer<sup>31</sup>.</li> </ul>"},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part1/#5-utiliser-les-guides-generes-par-lia","title":"5. Utiliser les guides g\u00e9n\u00e9r\u00e9s par l'IA","text":"<p>NotebookLM peut cr\u00e9er des \"Guides\" bas\u00e9s sur vos sources pour vous aider \u00e0 explorer des sujets en profondeur.</p>"},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part1/#icone-a-utiliser_4","title":"Ic\u00f4ne \u00e0 utiliser","text":""},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part1/#comment-utiliser-les-guides","title":"Comment utiliser les guides","text":"<ul> <li>Dans certains cas, NotebookLM peut sugg\u00e9rer de cr\u00e9er un guide bas\u00e9 sur le contenu de votre carnet<sup>32</sup>.</li> <li>Cliquez sur \"Cr\u00e9er un guide\" ou une option similaire.</li> <li>L'IA organisera les informations de vos sources en sections th\u00e9matiques, facilitant la navigation et la compr\u00e9hension des sujets complexes<sup>32</sup>.</li> <li>Les guides sont particuli\u00e8rement utiles pour \u00e9tudier, pr\u00e9parer des pr\u00e9sentations ou explorer de nouvelles th\u00e9matiques.</li> </ul>"},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part1/#6-partager-vos-carnets-de-notes","title":"6. Partager vos carnets de notes","text":"<p>Vous pouvez partager vos carnets de notes avec d'autres personnes pour collaborer ou simplement diffuser vos recherches.</p>"},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part1/#icone-a-utiliser_5","title":"Ic\u00f4ne \u00e0 utiliser","text":""},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part1/#comment-partager-un-carnet","title":"Comment partager un carnet","text":"<ul> <li>Dans votre carnet de notes, cherchez l'ic\u00f4ne de partage (souvent un symbole de fl\u00e8che ou de trois points connect\u00e9s) ou le bouton \"Partager\"<sup>33</sup>.</li> <li>Vous pourrez alors g\u00e9n\u00e9rer un lien partageable.</li> <li>Attention : Assurez-vous de bien comprendre les options de partage (lecture seule, modification) avant d'envoyer le lien<sup>33</sup>.</li> </ul>"},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part1/#7-gerer-et-retrouver-vos-carnets","title":"7. G\u00e9rer et retrouver vos carnets","text":""},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part1/#ou-retrouver-vos-carnets-passes","title":"O\u00f9 retrouver vos carnets pass\u00e9s","text":"<ul> <li>Sur la page principale de NotebookLM, vous verrez la liste de tous vos carnets de notes.</li> <li>Cliquez sur un carnet pour l'ouvrir et reprendre votre travail l\u00e0 o\u00f9 vous l'avez laiss\u00e9.</li> <li>Vous pouvez rechercher des carnets par nom si vous en avez beaucoup.</li> </ul>"},{"location":"blog/guide-perplexity-ai-and-notebooklm-tutoriels-pour-tous-part1/#attention","title":"Attention","text":"<ul> <li>Si vous supprimez un carnet de notes, toutes les sources et discussions qu'il contient seront d\u00e9finitivement effac\u00e9es<sup>34</sup>.</li> </ul> <p>En utilisant NotebookLM, vous transformez vos documents en une base de connaissances interactive et personnalis\u00e9e, pr\u00eate \u00e0 r\u00e9pondre \u00e0 toutes vos questions et \u00e0 vous aider \u00e0 organiser vos id\u00e9es.</p> <p>\\&lt;div style=\"text-align: center\"&gt;\u2042\\&lt;/div&gt;</p> <ol> <li> <p>https://infopreneur.blog/perplexity-login/ \u21a9\u21a9\u21a9</p> </li> <li> <p>https://reglo.ai/comment-utiliser-perplexity-ai/ \u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>https://gitmind.com/fr/perplexity-ai-guide-utilisation.html \u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>https://www.blogdumoderateur.com/perplexity-guide-bien-utiliser-moteur-recherche-assiste-ia/ \u21a9</p> </li> <li> <p>https://www.perplexity.ai/help-center/fr/articles/10354769-qu-est-ce-qu-un-fil-de-discussion \u21a9\u21a9\u21a9</p> </li> <li> <p>https://www.reddit.com/r/perplexity_ai/comments/1dyx9uy/need_help_with_managing_threads/?tl=fr \u21a9</p> </li> <li> <p>https://www.perplexity.ai/fr/hub/getting-started \u21a9\u21a9\u21a9</p> </li> <li> <p>https://www.perplexity.ai/help-center/fr/articles/10354975-commencer-a-utiliser-perplexity \u21a9</p> </li> <li> <p>https://www.elephorm.com/formation/code-data/no-code/maitriser-perplexity-ai-optimisez-vos-recherches-avec-lia \u21a9</p> </li> <li> <p>https://www.elephorm.com/formation/code-data/perplexity/maitriser-perplexity-ai-optimisez-vos-recherches-avec-lia/integration-dimages-et-de-fichiers-dans-vos-recherches \u21a9\u21a9</p> </li> <li> <p>https://app.studyraid.com/fr/read/18469/680245/importer-des-fichiers-pdf \u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>https://www.perplexity.ai/hub/faq/how-does-file-upload-work \u21a9\u21a9</p> </li> <li> <p>https://www.reddit.com/r/perplexity_ai/comments/16n2g3d/i_made_an_extension_to_export_perplexity_threads/?tl=fr \u21a9</p> </li> <li> <p>https://chromewebstore.google.com/detail/save-my-chatbot-ai-conver/agklnagmfeooogcppjccdnoallkhgkod \u21a9\u21a9</p> </li> <li> <p>https://github.com/Hugo-COLLIN/SaveMyPhind-conversation-exporter \u21a9</p> </li> <li> <p>https://www.perplexity.ai/hub/blog/introducing-perplexity-deep-research \u21a9</p> </li> <li> <p>https://www.elephorm.com/formation/code-data/perplexity/maitriser-perplexity-ai-optimisez-vos-recherches-avec-lia/explorez-la-fonctionnalite-space-de-perplexity \u21a9\u21a9</p> </li> <li> <p>https://anthemcreation.com/intelligence-artificielle/perplexity-spaces-recherche-en-ligne-et-locale-premium/ \u21a9</p> </li> <li> <p>https://action-commerciale.com/espace-perplexity-hub-projets/ \u21a9\u21a9\u21a9</p> </li> <li> <p>https://reglo.ai/comment-utiliser-perplexity-ai/ \u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>https://www.elephorm.com/formation/code-data/perplexity/maitriser-perplexity-ai-optimisez-vos-recherches-avec-lia/exploration-de-la-recherche-approfondie-avec-perplexity \u21a9\u21a9</p> </li> <li> <p>https://cedric.fm/guide-perplexity-redaction/ \u21a9\u21a9</p> </li> <li> <p>https://www.perplexity.ai/help-center/fr/articles/10354948-comment-dois-je-remplir-la-section-profil-de-mes-parametres \u21a9\u21a9</p> </li> <li> <p>https://www.blogdumoderateur.com/perplexity-guide-bien-utiliser-moteur-recherche-assiste-ia/ \u21a9\u21a9\u21a9</p> </li> <li> <p>https://www.perplexity.ai/fr/hub/blog/a-student-s-guide-to-using-perplexity-spaces \u21a9\u21a9\u21a9</p> </li> <li> <p>Google Workspace Updates: Build your own AI assistant with NotebookLM \u21a9\u21a9</p> </li> <li> <p>Google Chrome : Utiliser NotebookLM \u21a9\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>Google Workspace Updates: Get to know NotebookLM, a Google AI experiment \u21a9\u21a9\u21a9</p> </li> <li> <p>NotebookLM - Your AI-powered research assistant \u21a9\u21a9</p> </li> <li> <p>Google Workspace Updates: NotebookLM makes your research easier with AI-generated notes, outlines, and summaries \u21a9\u21a9\u21a9</p> </li> <li> <p>How to use NotebookLM - YouTube \u21a9</p> </li> <li> <p>Google Blog: How NotebookLM helps you learn anything \u21a9\u21a9</p> </li> <li> <p>How to Share NotebookLM Projects - YouTube \u21a9\u21a9</p> </li> <li> <p>NotebookLM support page \u21a9</p> </li> </ol>"},{"location":"blog/numerical-analysis-and-plotting-equivalence-in-python-r-matlab-and-scilab/","title":"Numerical Analysis and Plotting: Equivalence in Python, R, MATLAB, and Scilab","text":""},{"location":"blog/numerical-analysis-and-plotting-equivalence-in-python-r-matlab-and-scilab/#introduction","title":"Introduction","text":"<p>Numerical analysis and data visualization are fundamental aspects of scientific computing across various programming languages.</p> <p>This guide explores syntax equivalences in Python, R, MATLAB, and Scilab, empowering you to transition seamlessly between these languages for numerical computations and plotting tasks.</p> <p>Understanding the corresponding syntaxes in each language facilitates code portability and collaboration among researchers and developers across different platforms.</p>"},{"location":"blog/numerical-analysis-and-plotting-equivalence-in-python-r-matlab-and-scilab/#key-considerations","title":"Key Considerations","text":""},{"location":"blog/numerical-analysis-and-plotting-equivalence-in-python-r-matlab-and-scilab/#choosing-the-right-language","title":"Choosing the Right Language","text":"<ul> <li>Syntax Familiarity: Consider your familiarity with the language syntax.</li> <li>Library Ecosystem: Evaluate the availability and maturity of numerical and plotting libraries.</li> <li>Community Support: Assess the community size and active development in relevant domains.</li> <li>Integration Requirements: Determine integration needs with existing systems or frameworks.</li> <li>Performance: Consider the performance requirements for your numerical computations.</li> </ul>"},{"location":"blog/numerical-analysis-and-plotting-equivalence-in-python-r-matlab-and-scilab/#languages-overview","title":"Languages Overview","text":"Python R MATLAB Scilab <ul> <li>Numerical Analysis Libraries: NumPy, SciPy</li> <li>Plotting Libraries: Matplotlib, sns</li> <li>Syntax Highlights: Vectorized operations, array slicing, and broadcasting.</li> </ul> <ul> <li>Numerical Analysis Libraries: Base R functions, <code>data.table</code>, <code>dplyr</code></li> <li>Plotting Libraries: ggplot2, plotly</li> <li>Syntax Highlights: Data frames, functional programming, pipe operator <code>%&gt;%</code>.</li> </ul> <ul> <li>Numerical Analysis Libraries: MATLAB Core Functions, MATLAB Toolboxes</li> <li>Plotting Libraries: MATLAB Plotting Functions</li> <li>Syntax Highlights: Matrix operations, built-in functions, plotting with <code>plot</code> and <code>imshow</code>.</li> </ul> <ul> <li>Numerical Analysis Libraries: Scilab Core Functions, Toolboxes</li> <li>Plotting Libraries: Scilab Plotting Functions</li> <li>Syntax Highlights: Matrix-based computations, built-in functions, plotting with <code>plot</code> and <code>imshow</code>.</li> </ul>"},{"location":"blog/numerical-analysis-and-plotting-equivalence-in-python-r-matlab-and-scilab/#comparison-tables","title":"Comparison Tables","text":""},{"location":"blog/numerical-analysis-and-plotting-equivalence-in-python-r-matlab-and-scilab/#array-creation","title":"Array Creation","text":"Task Python (NumPy) R (Base/Additional Packages) MATLAB Core Functions Scilab Core Functions Row Vector <code>np.array([1, 2, 3])</code> <code>c(1, 2, 3)</code> <code>row_vector = [1, 2, 3]</code> <code>row_vector = [1, 2, 3]</code> Column Vector <code>np.array([[1], [2], [3]])</code> <code>c(1, 2, 3)</code> <code>col_vector = [1; 2; 3]</code> <code>col_vector = [1; 2; 3]</code> Matrix <code>np.array([[1, 2], [3, 4]])</code> <code>matrix(c(1, 2, 3, 4), nrow=2)</code> <code>matrix = [1, 2; 3, 4]</code> <code>matrix = [1, 2; 3, 4]</code> j:k Row Vector <code>np.arange(j, k+1)</code> <code>j:k</code> <code>j:k</code> <code>j:k</code> j:i:k Row Vector <code>np.arange(j, k+i, i)</code> <code>seq(j, k, by=i)</code> <code>j:i:k</code> <code>j:i:k</code> Linearly Spaced Vector <code>np.linspace(a, b, n)</code> <code>seq(a, b, length.out=n)</code> <code>linspace(a, b, n)</code> <code>linspace(a, b, n)</code> Linearly spaced vector <code>np.linspace(1, 10, 100)</code> <code>seq(1, 10, length.out=100)</code> <code>linspace(1, 10, 100)</code> <code>linspace(1, 10, 100)</code> Logarithmically spaced vector <code>np.logspace(1, 3, 100)</code> <code>10^seq(1, 3, length.out=100)</code> <code>logspace(1, 3, 100)</code> <code>logspace(1, 3, 100)</code> Range Array <code>np.arange(start, stop, step)</code> <code>seq(from = start, to = end, by = step)</code> <code>start:step:end</code> <code>start:step:end</code> Empty Array <code>np.empty(shape=(m, n))</code> <code>vector(length = n)</code> <code>zeros(m, n)</code> <code>zeros(m, n)</code> NaN Matrix <code>np.full((a, b), np.nan)</code> <code>matrix(NaN, nrow=a, ncol=b)</code> <code>NaN(a, b)</code> <code>nan(a, b)</code> Zeros Array <code>np.zeros(shape=(m, n))</code> <code>rep(0, n)</code> <code>zeros(m, n)</code> <code>zeros(m, n)</code> Zeros Matrix <code>np.zeros((a, b))</code> <code>matrix(0, nrow=a, ncol=b)</code> <code>zeros(a, b)</code> <code>zeros(a, b)</code> Ones Array <code>np.ones(shape=(m, n))</code> <code>rep(1, n)</code> <code>ones(m, n)</code> <code>ones(m, n)</code> Ones Matrix <code>np.ones((a, b))</code> <code>matrix(1, nrow=a, ncol=b)</code> <code>ones(a, b)</code> <code>ones(a, b)</code> Identity Matrix <code>np.identity(n)</code> or <code>np.eye(n)</code> <code>diag(rep(1, n))</code> <code>eye(n)</code> <code>eye(n)</code> Meshgrid <code>np.meshgrid(x, y)</code> <code>expand.grid(x, y)</code> <code>[X, Y] = meshgrid(x, y)</code> <code>[X, Y] = meshgrid(x, y)</code> Random Array <code>np.random.rand(rows, cols)</code> <code>runif(n)</code> <code>rand(m, n)</code> <code>rand(m, n)</code>"},{"location":"blog/numerical-analysis-and-plotting-equivalence-in-python-r-matlab-and-scilab/#descriptive-methods","title":"Descriptive Methods","text":"Task Python R MATLAB Core Functions Scilab Core Functions Rows and Columns <code>x.shape</code> <code>dim(x)</code> <code>size(x)</code> <code>size(x)</code> Number of Array Elements <code>np.size(A)</code> <code>length(A)</code> <code>numel(A)</code> <code>length(A)</code>"},{"location":"blog/numerical-analysis-and-plotting-equivalence-in-python-r-matlab-and-scilab/#accessing-elements","title":"Accessing Elements","text":"Feature Python (NumPy/SciPy) R (Base/Additional Packages) MATLAB Core Functions Scilab Core Functions Change Index Value <code>x[1] = 4</code> <code>x[2] &lt;- 4</code> <code>x(2) = 4</code> <code>x(2) = 4</code> All Elements of x <code>x.flatten()</code> <code>as.vector(x)</code> <code>x(:)</code> <code>x(:)</code> Jth to Last Element <code>x[j-1:]</code> <code>x[j:length(x)]</code> <code>x(j:end)</code> <code>x(j:$)</code> 2<sup>nd</sup> to 5<sup>th</sup> Element <code>x[1:4]</code> <code>x[2:5]</code> <code>x(2:5)</code> <code>x(2:5)</code> All J Row Elements <code>x[j-1, :]</code> <code>x[j, ]</code> <code>x(j, :)</code> <code>x(j, :)</code> All J Column Elements <code>x[:, j-1]</code> <code>x[, j]</code> <code>x(:, j)</code> <code>x(:, j)</code> Sort Vector <code>np.sort(A)</code> <code>sort(A)</code> <code>sort(A)</code> <code>sort(A)</code> Change Elements &gt; 5 to 0 <code>x[x &gt; 5] = 0</code> <code>x[x &gt; 5] &lt;- 0</code> <code>x(x &gt; 5) = 0</code> <code>x(x &gt; 5) = 0</code> List Elements &gt; 5 <code>x[x &gt; 5]</code> <code>x[x &gt; 5]</code> <code>x(x &gt; 5)</code> <code>x(x &gt; 5)</code> Indices of Elements &gt; 5 <code>np.where(x &gt; 5)</code> <code>which(x &gt; 5)</code> <code>find(x &gt; 5)</code> <code>find(x &gt; 5)</code> Indices of NaN Elements <code>np.where(np.isnan(A))</code> <code>which(is.na(A))</code> <code>find(isnan(A))</code> <code>find(isnan(A))</code>"},{"location":"blog/numerical-analysis-and-plotting-equivalence-in-python-r-matlab-and-scilab/#mathematical-operations","title":"Mathematical Operations","text":"Task Python (NumPy) R (Base/Additional Packages) MATLAB Core Functions Scilab Core Functions Element-wise Multiply <code>np.multiply(x, y)</code> or <code>a * b</code> <code>x * y</code> <code>x .* y</code> <code>x .* y</code> Element-wise Divide <code>np.divide(x, y)</code> or <code>a / b</code> <code>x / y</code> <code>x ./ y</code> <code>x ./ y</code> Element-wise Add <code>np.add(x, y)</code> or <code>a + b</code> <code>x + y</code> <code>x + y</code> <code>x + y</code> Element-wise Subtract <code>np.subtract(x, y)</code> or <code>a - b</code> <code>x - y</code> <code>x - y</code> <code>x - y</code> Element-wise square <code>np.square(a)</code> <code>a^2</code> <code>a.^2</code> <code>a.^2</code> Elementwise Power <code>np.power(A, n)</code> <code>A^n</code> <code>A .^ n</code> <code>A .^ n</code> Normal/Matrix Power <code>np.linalg.matrix_power(A, n)</code> <code>%*%</code> <code>A^n</code> <code>A^n</code> Diagonal of a matrix <code>np.diag(a)</code> <code>diag(a)</code> <code>diag(a)</code> <code>diag(a)</code> Transpose <code>np.transpose(A)</code> <code>t(A)</code> <code>A'</code> <code>A'</code> Horizontally Concatenates <code>np.concatenate((A, B), axis=1)</code> <code>cbind(A, B)</code> <code>[A, B]</code> <code>[A, B]</code> Vertically Concatenates <code>np.concatenate((A, B), axis=0)</code> <code>rbind(A, B)</code> <code>[A; B]</code> <code>[A; B]</code>"},{"location":"blog/numerical-analysis-and-plotting-equivalence-in-python-r-matlab-and-scilab/#other-numrical-analysis-related-programming-tools","title":"Other Numrical Analysis Related Programming tools","text":"Task Python R MATLAB Core Functions Scilab Core Functions Variable Declaration <code>[a, b] = deal(np.full((5, 5), np.nan))</code> <code>list(a = matrix(NaN, nrow=5, ncol=5), b = matrix(NaN, nrow=5, ncol=5))</code> <code>[a, b] = deal(NaN(5, 5))</code> <code>[a, b] = deal(NaN(5, 5))</code>"},{"location":"blog/numerical-analysis-and-plotting-equivalence-in-python-r-matlab-and-scilab/#standard-functions","title":"Standard Functions","text":"Function Python (Base/NumPy) R (Base/Additional Packages) MATLAB Core Functions Scilab Core Functions Absolute Value <code>abs(x)</code> or <code>np.abs(x)</code> <code>abs(x)</code> <code>abs(x)</code> <code>abs(x)</code> Pi <code>math.pi</code> or <code>np.pi</code> <code>pi</code> <code>pi</code> <code>%pi</code> Infinity <code>math.inf</code> or <code>np.inf</code> <code>Inf</code> <code>Inf</code> <code>%inf</code> Floating Point Accuracy <code>np.finfo(float).eps</code> <code>double.eps</code> <code>eps</code> <code>eps</code> Large Number <code>1e6</code> <code>1e6</code> <code>1e6</code> <code>1d6</code> Sum <code>np.sum(x)</code> <code>sum(x)</code> <code>sum(x)</code> <code>sum(x)</code> Cumulative Sum <code>np.cumsum(x)</code> <code>cumsum(x)</code> <code>cumsum(x)</code> <code>cumsum(x)</code> Product <code>np.prod(x)</code> <code>prod(x)</code> <code>prod(x)</code> <code>prod(x)</code> Cumulative Product <code>np.cumprod(x)</code> <code>cumprod(x)</code> <code>cumprod(x)</code> <code>cumprod(x)</code> Difference <code>np.diff(x)</code> <code>diff(x)</code> <code>diff(x)</code> <code>diff(x)</code> Round <code>np.round(x)</code> <code>round(x)</code> <code>round(x)</code> <code>round(x)</code> Ceiling <code>np.ceil(x)</code> <code>ceiling(x)</code> <code>ceil(x)</code> <code>ceil(x)</code> Floor <code>np.floor(x)</code> <code>floor(x)</code> <code>floor(x)</code> <code>floor(x)</code> Bessel Function <code>scipy.special.jn(n, x)</code> <code>besselJ(x, n)</code> <code>besselj(n, x)</code> <code>besselj(n, x)</code> Factorial <code>np.math.factorial(x)</code> <code>factorial(x)</code> <code>factorial(x)</code> <code>factorial(x)</code> Exponential function <code>np.exp(a)</code> <code>exp(a)</code> <code>exp(a)</code> <code>exp(a)</code> Square root <code>np.sqrt(a)</code> <code>sqrt(a)</code> <code>sqrt(a)</code> <code>sqrt(a)</code> Trigonometric functions <code>np.sin(a)</code>, <code>np.cos(a)</code>, <code>np.tan(a)</code> <code>sin(a)</code>, <code>cos(a)</code>, <code>tan(a)</code> <code>sin(a)</code>, <code>cos(a)</code>, <code>tan(a)</code> <code>sin(a)</code>, <code>cos(a)</code>, <code>tan(a)</code>"},{"location":"blog/numerical-analysis-and-plotting-equivalence-in-python-r-matlab-and-scilab/#statistical-measures","title":"Statistical Measures","text":"Measure Python (NumPy/SciPy) R (Base/Additional Packages) MATLAB Core Functions Scilab Core Functions Mean <code>np.mean(x)</code> or <code>np.average(x)</code> <code>mean(x)</code> <code>mean(x)</code> or <code>mean(mean(x))</code> <code>mean(x)</code> or <code>mean(x(:))</code> Median <code>np.median(x)</code> <code>median(x)</code> <code>median(x)</code> <code>median(x)</code> Variance <code>np.var(x)</code> <code>var(x)</code> <code>var(x)</code> <code>variance(x)</code> Covariance <code>np.cov(x, y)</code> <code>cov(x, y)</code> <code>cov(x, y)</code> <code>covar(x, y)</code> Correlation <code>np.corrcoef(x, y)</code> <code>cor(x, y)</code> <code>corrcoef(x, y)</code> <code>correlation(x, y)</code> Quantile <code>np.percentile(x, p)</code> <code>quantile(x, p)</code> <code>quantile(x, p)</code> <code>quantile(x, p)</code> <p>Note: The <code>quantile(x, p)</code> function in MATLAB and Scilab uses interpolation for missing quantiles, which may differ from textbook definitions.</p>"},{"location":"blog/numerical-analysis-and-plotting-equivalence-in-python-r-matlab-and-scilab/#statistical-commands","title":"Statistical Commands","text":"Command Python (NumPy/SciPy) R (Base/Additional Packages) MATLAB Core Functions Scilab Core Functions Generate Random Numbers <code>np.random.rand(n)</code> or <code>np.random.randn(n)</code> <code>runif(n)</code> or <code>rnorm(n)</code> <code>rand(n, 1)</code> or <code>randn(n, 1)</code> <code>rand(n, 1)</code> or <code>randn(n, 1)</code> Probability Density Function <code>scipy.stats.norm.pdf(x, mean, std)</code> <code>dnorm(x, mean, sd)</code> <code>normpdf(x, mean, std)</code> <code>pdf('normal', x, mean, std)</code> Cumulative Distribution Function <code>scipy.stats.norm.cdf(x, mean, std)</code> <code>pnorm(x, mean, sd)</code> <code>normcdf(x, mean, std)</code> <code>cdf('normal', x, mean, std)</code> Histogram <code>plt.hist(x)</code> or <code>np.histogram(x)</code> <code>hist(x)</code> <code>hist(x)</code> <code>histogram(x)</code> Histogram with Fit <code>plt.hist(x, density=True)</code> and <code>scipy.stats.norm.fit(x)</code> <code>histfit(x)</code> <code>histfit(x)</code> <code>histfit(x)</code>"},{"location":"blog/numerical-analysis-and-plotting-equivalence-in-python-r-matlab-and-scilab/#standard-distributions-dist","title":"Standard Distributions (dist)","text":"<ul> <li>Normal Distribution (norm)</li> <li>Student's t-Distribution (t)</li> <li>F-Distribution (f)</li> <li>Gamma Distribution (gam)</li> <li>Chi-Square Distribution (chi2)</li> <li>Binomial Distribution (bino)</li> </ul>"},{"location":"blog/numerical-analysis-and-plotting-equivalence-in-python-r-matlab-and-scilab/#non-linear-numerical-methods","title":"Non Linear Numerical Methods","text":"Feature Python (NumPy/SciPy) R (Base/Additional Packages) MATLAB Core Functions Scilab Core Functions Inverse of Matrix <code>np.linalg.inv(A)</code> <code>solve(A)</code> <code>inv(A)</code> <code>inv(A)</code> Eigenvalues &amp; Eigenvectors <code>np.linalg.eig(a)</code> <code>eigen(a)</code> or (<code>eigen(A)$values</code>, <code>eigen(A)$vectors</code>) <code>eig(a)</code> <code>spec(a)</code> Singular Value Decomposition <code>np.linalg.svd(a)</code> <code>svd(a)</code> <code>svd(a)</code> <code>svd(a)</code> Interpolation <code>scipy.interpolate.interp1d(x, y)</code> <code>approx(x, y)</code> <code>interp1(x, y)</code> <code>splin(x, y)</code> Quad Integration <code>scipy.integrate.quad(fun, a, b)</code> <code>integrate.quad(fun, a, b)</code> <code>quad(fun, a, b)</code> <code>quad(fun, a, b)</code> Simpson Integration <code>scipy.integrate.simps(y, x)</code> <code>integrate.simps(y, x)</code> <code>simpson(y, x)</code> <code>simpson(y, x)</code> Minimization (Derivative-free) <code>scipy.optimize.fmin(fun, x0)</code> <code>optim(x0, fun)</code> <code>fminsearch(fun, x0)</code> <code>fminsearch(fun, x0)</code> Minimization (Constrained) <code>scipy.optimize.minimize(fun, x0, constraints=cons)</code> <code>optim(x0, fun, method = \"L-BFGS-B\")</code> <code>fmincon(fun, x0, A, b, Aeq, beq, lb, ub)</code> <code>fmincon(fun, x0, A, b, Aeq, beq, lb, ub)</code>"},{"location":"blog/numerical-analysis-and-plotting-equivalence-in-python-r-matlab-and-scilab/#plotting-methods","title":"Plotting Methods","text":"Task Python (Matplotlib/Seaborn) R (Base/ggplot2/plotly) MATLAB Core Functions Scilab Core Functions Plot line <code>plt.plot(x, y)</code> <code>plot(x, y, type='l')</code> or <code>ggplot(data, aes(x, y)) + geom_line()</code> <code>plot(x, y)</code> <code>plot(x, y)</code> or <code>plot2d(x, y)</code> Scatter plot <code>plt.scatter(x, y)</code> <code>plot(x, y)</code> or <code>ggplot(data, aes(x, y)) + geom_point()</code> <code>scatter(x, y)</code> <code>plot2d(x, y, style='o')</code> Title <code>plt.title('Title')</code> <code>title('Title')</code> <code>title('Title')</code> <code>xtitle('Title')</code> Label axes <code>plt.xlabel('x'), plt.ylabel('y')</code> <code>xlab('x'), ylab('y')</code> <code>xlabel('x'), ylabel('y')</code> <code>xlabel('x'), ylabel('y')</code> Histogram <code>plt.hist(data)</code> <code>ggplot(data, aes(x)) + geom_histogram()</code> <code>hist(data)</code> <code>histplot(data)</code> Heatmap <code>sns.heatmap(data)</code> <code>plot_ly(z = ~matrix_data, type = \"heatmap\")</code> <code>heatmap(data)</code> <code>heatmap(data)</code>"},{"location":"blog/numerical-analysis-and-plotting-equivalence-in-python-r-matlab-and-scilab/#strings-and-regular-expressions","title":"Strings and Regular Expressions","text":"Task Python (NumPy/SciPy) R (Base/Additional Packages) MATLAB Core Functions Scilab Core Functions Compare Strings <code>str1 == str2</code> <code>str1 == str2</code> <code>strcmp(str1, str2)</code> <code>strcmp(str1, str2)</code> Compare Strings (Case Insensitive) <code>str1.lower() == str2.lower()</code> <code>tolower(str1) == tolower(str2)</code> <code>strcmpi(str1, str2)</code> <code>strcmpi(str1, str2)</code> Compare First n Letters <code>str1[:n] == str2[:n]</code> <code>substr(str1, 1, n) == substr(str2, 1, n)</code> <code>strncmp(str1, str2, n)</code> <code>strncmp(str1, str2, n)</code> Find Substring <code>str1.find(substring)</code> <code>grep(substring, str1)</code> <code>strfind(str1, substring)</code> <code>strindex(str1, substring)</code> Regular Expression Search <code>re.search(pattern, str1)</code> <code>grep(pattern, str1)</code> <code>regexp(str1, pattern)</code> <code>regexp(str1, pattern)</code>"},{"location":"blog/numerical-analysis-and-plotting-equivalence-in-python-r-matlab-and-scilab/#logical-operators","title":"Logical Operators","text":"Operator Python (NumPy/SciPy) R (Base/Additional Packages) MATLAB Core Functions Scilab Core Functions Short-Circuit AND <code>a and b</code> or <code>np.logical_and(a, b)</code> <code>a &amp; b</code> <code>a &amp;&amp; b</code> <code>a &amp; b</code> Short-Circuit OR <code>a or b</code> or <code>np.logical_or(a, b)</code> <code>a | b</code> <code>a || b</code> <code>a | b</code> NOT <code>not a</code> or <code>np.logical_not(a)</code> <code>!a</code> <code>~a</code> <code>~a</code> Equality Comparison <code>a == b</code> or <code>np.equal(a, b)</code> <code>a == b</code> <code>a == b</code> <code>a == b</code> Not Equal <code>a != b</code> or <code>np.not_equal(a, b)</code> <code>a != b</code> <code>a ~= b</code> <code>a ~= b</code> Object in Class <code>isinstance(obj, class_name)</code> <code>class(obj) == \"class_name\"</code> <code>isa(obj, 'class_name')</code> <code>typeof(obj) == 'class_name'</code>"},{"location":"blog/numerical-analysis-and-plotting-equivalence-in-python-r-matlab-and-scilab/#installing-and-importing-libraries","title":"Installing and Importing Libraries","text":"Feature Python (NumPy/SciPy) R (Base/Additional Packages) MATLAB Core Functions Scilab Core Functions Install Library <code>!pip install library_name</code> <code>install.packages(\"package_name\")</code> Download from MathWorks website or use MATLAB's Add-On Explorer Download from Scilab website or use ATOMS Package Manager Import Library <code>import library_name</code> <code>library(package_name)</code> N/A <code>exec('load' + newline + 'load(\"path_to_library\")')</code>"},{"location":"blog/numerical-analysis-and-plotting-equivalence-in-python-r-matlab-and-scilab/#debugging-and-profiling","title":"Debugging and Profiling","text":"Feature Python (NumPy/SciPy) R (Base/Additional Packages) MATLAB Core Functions Scilab Core Functions Keyboard Pause Execution <code>KeyboardInterrupt</code> <code>Sys.sleep()</code> <code>keyboard</code> <code>pause</code> Return Resumes Execution N/A <code>return()</code> <code>return</code> N/A Start Timer <code>start = time.time()</code> <code>start_time &lt;- Sys.time()</code> <code>tic</code> <code>tic()</code> Stop Timer <code>end = time.time()</code> <code>end_time &lt;- Sys.time()</code> <code>toc</code> <code>toc()</code> Start Profiler N/A <code>Rprof()</code> <code>profile on</code> <code>profile on</code> View Profiler Output N/A <code>summaryRprof()</code> <code>profile viewer</code> <code>profile viewer</code> Try/Catch <code>try: ... except Exception as e:</code> <code>tryCatch({...}, error = function(e) {...})</code> <code>try ... catch ... end</code> <code>try ... catch ... end</code> Debugging Conditional N/A <code>browser()</code> <code>dbstop if error</code> <code>dbstop if error</code> Clear Breakpoints N/A N/A <code>dbclear</code> <code>dbclear</code> Resume Execution <code>continue</code> <code>next</code> <code>dbcont</code> <code>dbcont</code> Last Error Message <code>traceback.format_exc()</code> <code>last.error()</code> <code>lasterr</code> N/A Last Warning Message N/A <code>last.warning()</code> <code>lastwarn</code> N/A Break N/A <code>break()</code> <code>break</code> <code>break</code> Progress Indicator <code>tqdm</code> or <code>progressbar</code> <code>txtProgressBar()</code> <code>waitbar</code> <code>waitbar</code>"},{"location":"blog/numerical-analysis-and-plotting-equivalence-in-python-r-matlab-and-scilab/#data-importexport","title":"Data Import/Export","text":"Feature Python (NumPy/Pandas/Pillow) R (Base/Additional Packages) MATLAB Core Functions Scilab Core Functions Read Excel <code>pandas.read_excel()</code> <code>readxl::read_excel()</code> <code>xlsread()</code> <code>xls_read()</code> Write Excel <code>pandas.to_excel()</code> <code>writexl::write_xlsx()</code> <code>xlswrite()</code> <code>xls_write()</code> Read/Write Table <code>pandas.read_csv()</code> / <code>to_csv()</code> <code>readr::read_csv()</code> / <code>write_csv()</code> <code>readtable()</code> / <code>writetable()</code> <code>csvRead()</code> / <code>csvWrite()</code> Read/Write Text <code>numpy.loadtxt()</code> / <code>savetxt()</code> <code>readr::read_lines()</code> / <code>write_lines()</code> <code>dlmread()</code> / <code>dlmwrite()</code> <code>mgetl()</code> / <code>mputl()</code> Load/Save ASCII N/A <code>load()</code> / <code>save()</code> N/A <code>mget</code> / <code>mput</code> Load/Save MATLAB <code>scipy.io.loadmat()</code> / <code>savemat()</code> N/A <code>load()</code> / <code>save()</code> <code>loadmat()</code> / <code>savemat()</code> Read/Write Image <code>PIL.Image.open()</code> / <code>save()</code> <code>readbitmap::read.bmp()</code> / <code>write.bmp()</code> <code>imread()</code> / <code>imwrite()</code> <code>imread()</code> / <code>imwrite()</code>"},{"location":"blog/numerical-analysis-and-plotting-equivalence-in-python-r-matlab-and-scilab/#dataframe-handling","title":"DataFrame Handling","text":"Feature Python (pandas) R (tidyverse) MATLAB Core Functions Scilab Core Functions Import CSV File <code>import pandas as pd</code><code>fuel_data = pd.read_csv(\"fuel_data.csv\")</code> <code>library(readr)</code><code>fuel_data &lt;- read_csv(\"fuel_data.csv\")</code> N/A N/A View Leading Rows <code>fuel_data.head(10)</code> <code>head(fuel_data)</code> N/A N/A List Columns <code>fuel_data.columns</code> <code>names(fuel_data)</code> <code>fuel_data.Properties.VariableNames</code> <code>fieldnames(fuel_data)</code> View Metadata <code>fuel_data.info()</code> <code>summary(fuel_data)</code> N/A N/A Get Variable Class <code>fuel_data['Price'].dtype</code> <code>class(fuel_data$Price)</code> N/A N/A Add New Variable <code>fuel_data['NewVariable'] = some_calculation</code> <code>mutate(fuel_data, NewVariable = some_calculation)</code> N/A N/A Convert to Date <code>fuel_data['PriceUpdatedDate'] = pd.to_datetime(fuel_data['PriceUpdatedDate'],format=\"%d/%m/%y\")</code> <code>fuel_data$PriceUpdatedDate &lt;- as.Date(fuel_data$PriceUpdatedDate,\"%d/%m/%y\")</code> N/A N/A Filter Data by Condition <code>fuel_data[fuel_data['Suburb'] == 'Alexandria']</code> <code>filter(fuel_data, Suburb == \"Alexandria\")</code> N/A N/A Logical AND Condition <code>fuel_data[(fuel_data['Price'] &gt; 10) &amp; (fuel_data['Price'] &lt; 20)]</code> <code>filter(fuel_data, Price &gt; 10 &amp; Price &lt; 20)</code> N/A N/A Logical OR Condition <code>fuel_data[(fuel_data['Price'] &lt; 5) | (fuel_data['Price'] &gt; 15)]</code> <code>filter(fuel_data, Price &lt; 5 | Price &gt; 15)</code> N/A N/A <p>This table provides basic operations for handling data frames in Python (using pandas), R (using tidyverse), MATLAB, and Scilab. Make sure to replace <code>\"fuel_data.csv\"</code> with the actual name of your CSV file.</p>"},{"location":"blog/numerical-analysis-and-plotting-equivalence-in-python-r-matlab-and-scilab/#programming-commands","title":"Programming Commands","text":"Feature Python (Base/Additional Packages) R MATLAB Core Functions Scilab Core Functions Return <code>return</code> <code>return</code> <code>return</code> <code>return</code> Exist <code>os.path.exists()</code> <code>exists()</code> <code>exist()</code> <code>exist()</code> GPU Conversion <code>torch.Tensor.to()</code> <code>gpuR::gpuVector()</code> <code>gpuArray()</code> <code>gpuArray()</code> Function Declaration <code>def myfun(x1, x2): return x1+x2</code> <code>myfun &lt;- function(x1, x2) { return(x1 + x2) }</code> <code>function [y1,...,yN] = myfun(x1,...,xM)</code> <code>function [y1,...,yN] = myfun(x1,...,xM)</code> Anonymous Function <code>lambda x1, x2: x1 + x2</code> <code>function(x1, x2) { x1 + x2 }</code> <code>myfun = @(x1, x2) x1 + x2</code> <code>myfun = @(x1, x2) x1 + x2</code> Global Scope Declaration <code>global x</code> <code>x &lt;&lt;- value</code> <code>global x</code> <code>global x</code>"},{"location":"blog/numerical-analysis-and-plotting-equivalence-in-python-r-matlab-and-scilab/#system-commands","title":"System Commands","text":"Feature Python (NumPy/SciPy) R (Base/Additional Packages) MATLAB Core Functions Scilab Core Functions Add Path <code>sys.path.append(path)</code> <code>attach(path)</code> <code>addpath(path)</code> <code>path = path + \":\" + path_to_add</code> Get Subfolders N/A <code>list.dirs(path)</code> <code>genpath(path)</code> N/A Current Directory <code>os.getcwd()</code> <code>getwd()</code> <code>pwd</code> <code>pwd</code> Make Directory <code>os.mkdir(path)</code> <code>dir.create(path)</code> <code>mkdir(path)</code> <code>mkdir(path)</code> Temporary Directory <code>tempfile.gettempdir()</code> <code>tempdir()</code> <code>tempdir</code> <code>tempdir()</code> Functions in Memory N/A N/A <code>inmem</code> N/A Exit <code>exit()</code> <code>q()</code> <code>exit</code> <code>exit</code> List Folder Content <code>os.listdir(path)</code> <code>list.files(path)</code> <code>dir(path)</code> <code>files = dir(path)</code> List Toolboxes N/A <code>installed.packages()</code> <code>ver</code> <code>exec('getmodules' + newline + 'getmodules()')</code>"},{"location":"blog/numerical-analysis-and-plotting-equivalence-in-python-r-matlab-and-scilab/#conclusion","title":"Conclusion","text":"<p>Congratulations! You've now gained insights into the numerical analysis and plotting syntax equivalence across Python, R, MATLAB, and Scilab. Armed with this knowledge, you can seamlessly switch between these languages for scientific computing tasks, fostering collaboration and code portability.</p> <p>Stay proactive in exploring and utilizing the rich libraries and functionalities offered by each language to enhance your computational workflows!</p>"},{"location":"blog/numerical-analysis-and-plotting-equivalence-in-python-r-matlab-and-scilab/#related-posts","title":"Related Posts","text":"<ul> <li>Cheat on Python Package Managers</li> </ul>"},{"location":"blog/numerical-analysis-and-plotting-equivalence-in-python-r-matlab-and-scilab/#relevant-sources","title":"Relevant Sources","text":"<ul> <li>NumPy Documentation</li> <li>R Documentation</li> <li>MATLAB Documentation, including python pandas to matlab - mathworks</li> <li>Scilab Documentation, including struct and csvread</li> <li>Thor Nielsen, Matlab Programming Syntax Cheat</li> <li>Glen Bentley, R &amp; Python Cheat Sheet</li> </ul>"},{"location":"blog/integrating-requirementstxt-with-poetry/","title":"Integrating Requirements.txt with Poetry","text":"<p>Managing dependencies is a crucial aspect of any software project. Whether you're starting a new project or inheriting an existing one, handling dependencies effectively can greatly impact your workflow. Often, projects utilize a <code>requirements.txt</code> file to specify their dependencies, but when it comes to Python projects, integrating these dependencies seamlessly with a package manager like Poetry can streamline the process.</p> <p>So, when working with Poetry, you might need to integrate your existing <code>requirements.txt</code> file into your project so you can improve reusability and publishing. This document outlines how to achieve that efficiently.</p> <p>It's essential to ensure that the file contains only abstract requirements, akin to manual maintenance. Conversely, if the <code>requirements.txt</code> file is generated from a <code>pip freeze</code>, it includes all packages, not just high-level requirements.</p>"},{"location":"blog/integrating-requirementstxt-with-poetry/#removing-spaces-comments-and-empty-lines","title":"Removing Spaces, Comments, and Empty Lines","text":"<p>To streamline the process, we'll remove unnecessary elements from the <code>requirements.txt</code> file before adding packages to Poetry.</p> <pre><code># Add requirements.txt as dependencies for Poetry\ncat requirements.txt | sed 's/ //g' | sed 's/#.*$//g' | grep -v \"^$\" | xargs -n 1 poetry add\n</code></pre> <p>Explanation:</p> <ul> <li><code>sed 's/ //g'</code>: Removes all spaces.</li> <li><code>sed 's/#.*$//g'</code>: Removes everything after <code>#</code>, effectively removing comments.</li> <li><code>grep -v \"^$\"</code>: Excludes empty lines.</li> <li><code>xargs -n 1 poetry add</code>: Adds each package listed in <code>requirements.txt</code> to Poetry.</li> </ul> Example <p>Input: <pre><code>requests==2.26.0  # HTTP library\ndjango&gt;=3.2.0\nnumpy==1.21.0\n</code></pre></p> <p>Output: <pre><code>Adding requests (2.26.0)\nAdding django (3.2.0)\nAdding numpy (1.21.0)\n</code></pre></p>"},{"location":"blog/integrating-requirementstxt-with-poetry/#removing-package-versions","title":"Removing Package Versions","text":"<p>If you want to add packages without their versions, you can use the following command:</p> <pre><code>cat requirements.txt | sed 's/ //g' | sed 's/#.*$//g' | grep -v \"^$\" | cut -d= -f1 | xargs -n 1 poetry add\n</code></pre> <p>This command strips version information from the package names before adding them to Poetry.</p> Example <p>Input: <pre><code>requests==2.26.0  # HTTP library\ndjango&gt;=3.2.0\nnumpy==1.21.0\n</code></pre></p> <p>Output: <pre><code>Adding requests\nAdding django\nAdding numpy\n</code></pre></p> <p>Be Cautious!</p> <p>Ensure that your <code>requirements.txt</code> file contains abstract requirements. If it's generated from a <code>pip freeze</code>, it lists all packages, which may not align with high-level requirements.</p>"},{"location":"blog/integrating-requirementstxt-with-poetry/#conclusion","title":"Conclusion","text":"<p>Integrating an existing <code>requirements.txt</code> file into a Poetry project can be a straightforward process with the right approach. By ensuring that your requirements are abstract and by following the provided steps to preprocess your file, you can seamlessly manage dependencies in your Python projects using Poetry. Embracing modern tools like Poetry not only simplifies dependency management but also enhances the overall development experience, allowing you to focus more on building and refining your software.</p>"},{"location":"blog/integrating-requirementstxt-with-poetry/#related-posts","title":"Related Posts","text":"<ul> <li>Beginner's guide to poetry</li> <li>Cheat on Python Package Managers</li> <li>How to publish your python project (to pypi or testPypi) with Poetry then install as dependency in another project ?</li> </ul>"},{"location":"blog/integrating-requirementstxt-with-poetry/#relevant-sources","title":"Relevant Sources","text":"<ul> <li>How to import an existing requirements.txt into a Poetry project? - Stack Overflow</li> </ul>"},{"location":"blog/managing-python-dependencies-navigating-pip-pipenv-poetry-conda-and-more/","title":"Managing Python Dependencies: Navigating pip, pipenv, poetry, conda and more","text":""},{"location":"blog/managing-python-dependencies-navigating-pip-pipenv-poetry-conda-and-more/#introduction","title":"Introduction","text":"<p>In the realm of Python development, a crucial aspect is managing project dependencies effectively.</p> <p>This guide delves into four prominent tools\u2014pip, pipenv, poetry, conda and more\u2014each offering distinct approaches to dependency management. Grasping their strengths, weaknesses, and use cases empowers you to make informed decisions for your projects.</p>"},{"location":"blog/managing-python-dependencies-navigating-pip-pipenv-poetry-conda-and-more/#key-considerations","title":"Key Considerations","text":""},{"location":"blog/managing-python-dependencies-navigating-pip-pipenv-poetry-conda-and-more/#choosing-the-right-tool","title":"Choosing the Right Tool","text":"<ul> <li>Scope and Size: Consider project scale and complexity.</li> <li>Team Collaboration: Assess the extent of teamwork involved.</li> <li>Environment Isolation: Determine the need for isolated environments.</li> <li>Dependency Management: Evaluate desired features like dependency locking and version conflict resolution.</li> <li>Project Requirements: Factor in specific project needs or constraints.</li> </ul>"},{"location":"blog/managing-python-dependencies-navigating-pip-pipenv-poetry-conda-and-more/#tools-overview","title":"Tools Overview","text":"<code>pip</code> <code>pipenv</code> <code>poetry</code> <code>conda</code> <code>uv</code> <ul> <li>Core Package Installer: The foundation for Python package management.</li> <li>Global Installations: Installs packages system-wide by default.</li> <li>Virtual Environments: Can be used within virtual environments for isolation.</li> <li>Basic Dependency Management: Relies on <code>requirements.txt</code> for specifying dependencies.</li> </ul> <ul> <li>Virtual Environment Creation and Management: Automatically creates and manages virtual environments.</li> <li>Dependency Locking: Locks dependency versions for reproducibility.</li> <li>Integration with Pipfile: Uses Pipfile and Pipfile.lock for dependency management.</li> </ul> <ul> <li>Dependency Management and Packaging: Comprehensive dependency management and packaging tool.</li> <li>Virtual Environment Handling: Manages virtual environments effectively.</li> <li>Dependency Locking and Version Handling: Offers robust dependency locking and version conflict resolution.</li> <li>Declarative Syntax: Uses pyproject.toml for configuration.</li> </ul> <ul> <li>Cross-Platform Package and Environment Management: Manages packages and environments across multiple languages (Python, R, etc.).</li> <li>Large Ecosystem of Packages: Accesses a vast repository of packages through Anaconda repositories.</li> <li>Environment Isolation: Creates isolated environments for project-specific dependencies.</li> <li>Non-Python Dependencies: Handles non-Python dependencies as well.</li> </ul> <ul> <li>Extremely Fast Installation: Faster than traditional pip and pip-tools.</li> <li>Drop-in Replacement: Provides a familiar interface for common pip commands.</li> <li>Supports Advanced Features: Handles editable installs, Git dependencies, URL dependencies, etc.</li> <li>Dependency Management: Offers features like dependency overrides and conflict resolution.</li> <li>Platform-specific Requirements File Generation: Generates requirements files tailored to specific platforms.</li> <li>Resolution Strategy Customization: Allows customization of resolution strategies for dependencies.</li> </ul>"},{"location":"blog/managing-python-dependencies-navigating-pip-pipenv-poetry-conda-and-more/#comparison-table","title":"Comparison Table","text":"Feature pip pipenv poetry conda uv Installation Built-in <code>pip install pipenv</code> <code>pip install poetry</code> Download and install Anaconda or Miniconda <code>pip install uv</code> Environments Implicit Automatic Automatic Automatic Implicit Locking Manual Automatic Automatic Automatic Manual Configuration requirements.txt Pipfile, Pipfile.lock pyproject.toml Environment files N/A Official packages PyPI (Python Package Index) PyPI PyPI Conda Forge (conda-forge channel) PyPI (Python Package Index) Multi-python-version Resolution No No Yes No Yes"},{"location":"blog/managing-python-dependencies-navigating-pip-pipenv-poetry-conda-and-more/#common-commands","title":"Common Commands","text":"Feature pip pipenv poetry conda uv Create a project <code>python -m venv &lt;env-name&gt;</code> <code>pipenv --python &lt;python-version&gt;</code> <code>poetry init</code> <code>conda create -n &lt;env-name&gt; python=&lt;python-version&gt;</code> <code>uv init .</code> (creates virtualenv at: <code>.venv/</code>) Use a virtual environment <code>source &lt;env-name&gt;/bin/activate</code> (linux) or <code>&lt;env-name&gt;/Scripts/activate</code> (windows) <code>pipenv shell</code> <code>poetry shell</code> <code>conda activate &lt;env-name&gt;</code> Same as pip Install a package <code>pip install &lt;package-name&gt;</code> <code>pipenv install &lt;package-name&gt;</code> <code>poetry add &lt;package-name&gt;</code> <code>conda install &lt;package-name&gt;</code> <code>uv add &lt;package-name&gt;</code> Remove a package <code>pip uninstall &lt;package-name&gt;</code> <code>pipenv uninstall &lt;package-name&gt;</code> <code>poetry remove &lt;package-name&gt;</code> <code>conda uninstall &lt;package-name&gt;</code> <code>uv remove &lt;package-name&gt;</code> Install from requirements <code>pip install -r requirements.txt</code> <code>pipenv install</code> (reads from Pipfile) <code>poetry install</code> (reads from pyproject.toml) <code>conda install --file requirements.txt</code> <code>uv sync requirements.txt</code> or <code>uv pip install -r requirements.txt</code> Dev packages vs others No explicit distinction <code>--dev</code> flag for development dependencies <code>[tool.poetry.dev-dependencies]</code> section in pyproject.toml No explicit distinction <code>--dev</code> flag for development dependencies List requirements <code>pip freeze &gt; requirements.txt</code> or use <code>pipreqs</code> <code>pipenv lock -r &gt; requirements.txt</code> <code>poetry export -f requirements.txt &gt; requirements.txt</code> <code>conda list --export &gt; requirements.txt</code> Same as pip (e.g., <code>uv pip freeze &gt; requirements.txt</code>) Transport project Manually copy files or create a setup.py Copy Pipfile and Pipfile.lock Copy pyproject.toml and pyproject.toml.lock Export environment to .yml file (conda env export &gt; environment.yml) Not directly supported (requires rebuilding with UV) Install from lock file N/A <code>pipenv install --ignore-pipfile</code> <code>poetry install --no-dev</code> (reads from pyproject.toml.lock) <code>conda env update --file environment.yml</code> N/A (Dependency resolution handled by UV directly) Delete the venv Remove directory manually <code>pipenv --rm</code> <code>poetry env remove &lt;env-name&gt;</code> <code>conda remove --name &lt;env-name&gt; --all</code> N/A Update a package to its latest version <code>pip install &lt;package-name&gt; --upgrade</code> <code>pipenv update &lt;package-name&gt;</code> <code>poetry update &lt;package-name&gt;</code> <code>conda update &lt;package-name&gt;</code> <code>uv update &lt;package-name&gt;</code> Update all packages to their latest versions N/A <code>pipenv update</code> <code>poetry update</code> <code>conda update -n &lt;env-name&gt; --all</code> (be aware of potential conflicts) N/A <p>Remember !</p> <p>As you wrap up your exploration of Python's dependency management tools, remember:</p> <ul> <li>Consistency: Maintain a uniform approach within a project for easier maintenance.</li> <li>Descriptive Naming: Use clear and informative names for environments and dependencies.</li> <li>Thorough Testing: Ensure compatibility and functionality after dependency changes.</li> <li>Regular Updates: Keep tools and dependencies up-to-date for security and performance.</li> </ul> More on UV dependencies resolution <ul> <li>Generate Requirements from pyproject.toml: <pre><code>uv pip compile pyproject.toml -o requirements.txt\n</code></pre></li> <li> <p>Platform-specific Requirements Generation: <pre><code>uv pip compile requirements.in -o requirements.txt\n</code></pre></p> </li> <li> <p>Command Line Compatibility:   uv's <code>install</code> (<code>uv pip install</code>) and <code>compile</code> (<code>uv pip compile</code>) commands support many familiar command-line arguments, such as <code>-r requirements.txt</code>, <code>-c constraints.txt</code>, <code>-e .</code> (for editable installs), <code>--index-url</code>, and more.</p> </li> <li> <p>Resolution Strategy Customization:   uv allows customization of its resolution strategy. With <code>--resolution=lowest</code>, uv pip installs the lowest compatible versions for all dependencies, while <code>--resolution=lowest-direct</code> focuses on lowest compatible versions for direct dependencies and latest compatible versions for transitive dependencies.</p> </li> </ul> read this docu about how to add dependencies from a <code>requirements.txt</code> file How to use two conda environment at the same time without breaking anything ? <p>To simultaneously utilize two Conda environments without causing conflicts, you can leverage Conda's nested activation feature. Here's how you can achieve this:</p> <pre><code>conda activate old_project_env\nconda activate --stack new_project_env\n</code></pre> <p>This sequence of commands allows you to activate a parent environment (<code>old_project_env</code>) while retaining access to the command-line options from the old environment. Then, by using <code>conda activate --stack new_project_env</code>, you can activate a new environment (<code>new_project_env</code>) within the context of the parent environment.</p> <p>This approach ensures that you can work within the specified environment (<code>new_project_env</code>) without interfering with the configurations or dependencies of the parent environment (<code>old_project_env</code>).</p> <p>For more information on Conda environments and nested activation, you can refer to the official Conda documentation.</p>"},{"location":"blog/managing-python-dependencies-navigating-pip-pipenv-poetry-conda-and-more/#conclusion","title":"Conclusion","text":"<p>Congratulations! You've now equipped yourself with insights into managing Python dependencies effectively using pip, pipenv, poetry, conda and more. By understanding their strengths and use cases, you're better equipped to make informed decisions for your projects.</p> <p>Stay proactive in exploring and adapting these tools to optimize your Python development experience!</p>"},{"location":"blog/managing-python-dependencies-navigating-pip-pipenv-poetry-conda-and-more/#related-posts","title":"Related Posts","text":"<ul> <li>Beginner's guide to poetry</li> <li>Cheat on Python Package Managers</li> <li>How to publish your python project (to pypi or testPypi) with Poetry then install as dependency in another project ?</li> </ul>"},{"location":"blog/managing-python-dependencies-navigating-pip-pipenv-poetry-conda-and-more/#relevant-sources","title":"Relevant Sources","text":"<ul> <li>Is it safe to update all conda package at once ?</li> </ul>"},{"location":"blog/pathlib-tutorial-transitioning-to-simplified-file-and-directory-handling-in-python/","title":"Pathlib Tutorial: Transitioning to Simplified File and Directory Handling in Python","text":""},{"location":"blog/pathlib-tutorial-transitioning-to-simplified-file-and-directory-handling-in-python/#introduction","title":"Introduction","text":"<p>Are you still using <code>import os</code> for file handling after 2020 ? Use <code>pathlib</code> instead !</p> <p>If you're moving away from command line operations or 'os' module to Python's <code>pathlib</code>, you're at the right place.</p> <p>Well, in this tutorial, we'll dive into the powerful <code>pathlib</code> module in Python. It offers a clean transition for users accustomed to CLI or 'os' for file and directory handling, providing an elegant and intuitive approach.</p>"},{"location":"blog/pathlib-tutorial-transitioning-to-simplified-file-and-directory-handling-in-python/#key-considerations-for-choosing-os-or-pathlib","title":"Key Considerations for Choosing os or pathlib","text":"<p>While <code>os</code> and <code>pathlib</code> both handle file and directory operations, they differ significantly in their approach:</p> <code>os</code> <code>pathlib</code> <ul> <li>Procedural: Primarily functions for path operations.</li> <li>Built-in: A part of Python's standard library.</li> <li>String-based: Paths represented as strings.</li> </ul> <ul> <li>Object-oriented: Paths as Path objects.</li> <li>Introduced in Python 3.4: Not available in older Python versions.</li> <li>Enhanced functionality: Offers extensive methods for path manipulation.</li> <li>Cross-platform: Works well across different operating systems.</li> </ul>"},{"location":"blog/pathlib-tutorial-transitioning-to-simplified-file-and-directory-handling-in-python/#making-the-transition","title":"Making the Transition","text":"<ul> <li>Preference for Object-Oriented Approach: <code>pathlib</code> provides a more intuitive and readable experience.</li> <li>Compatibility and Legacy Code: <code>os</code> may be necessary for older Python versions or existing code.</li> <li>Specific Functionality: Some advanced operations might be easier with one module over the other.</li> <li>Project Style and Conventions: Consider the overall project style and best practices.</li> </ul>"},{"location":"blog/pathlib-tutorial-transitioning-to-simplified-file-and-directory-handling-in-python/#best-practices-for-smooth-transition","title":"Best Practices for Smooth Transition","text":"<ul> <li>Consistency: Maintain a uniform approach within a project for easier maintenance.</li> <li>Descriptive Naming: Use clear variable names for paths to enhance readability.</li> <li>Error Handling: Implement robust error handling for potential issues.</li> <li>Thorough Testing: Ensure correctness in file and directory operations.</li> </ul>"},{"location":"blog/pathlib-tutorial-transitioning-to-simplified-file-and-directory-handling-in-python/#why-transition-to-pathlib","title":"Why Transition to <code>pathlib</code>?","text":"<p>Traditionally, file handling in Python often relied on the command line or the 'os' module. However, <code>pathlib</code> introduces an object-oriented paradigm, offering an intuitive and platform-independent solution.</p> <p>The transition to <code>pathlib</code> allows for:</p> <ul> <li>Simplified Path Representation: Paths as Path objects offer enhanced readability and functionality.</li> <li>Streamlined Syntax: Code becomes concise and more understandable using <code>pathlib</code>'s methods.</li> <li>Expanded Methodology: <code>pathlib</code> covers common path operations comprehensively.</li> </ul> <p>Let's explore the functionalities of <code>pathlib</code> step by step.</p> <p>Usage Examples</p> <pre><code>from pathlib import Path\n\n# Get the current working directory\ncwd = Path.cwd()\n\n# Create a new directory\nnew_dir = Path(\"my_new_directory\")\nnew_dir.mkdir()\n\n# Create a nested directory structure\nnested_dir = Path(\"data/processed/results\")\nnested_dir.mkdir(parents=True, exist_ok=True)  # Create all parent directories if needed\n\n# Check if a file exists\nfile_path = Path(\"my_file.txt\")\nif filepath.exists():\n    print(\"The file exists!\")\n\n# Read the contents of a file\ntext = filepath.read_text()\n\n# Write content to a file\nfilepath.write_text(\"New content for the file\")\n\n# Remove an empty directory\nempty_dir = Path(\"empty_dir\")\nempty_dir.rmdir()\n\n# Remove a non-empty directory and its contents\nnon_empty_dir = Path(\"non_empty_dir\")\nshutil.rmtree(non_empty_dir)\n</code></pre>"},{"location":"blog/pathlib-tutorial-transitioning-to-simplified-file-and-directory-handling-in-python/#equivalence-os-vs-pathlib-vs-cli-for-common-operations","title":"Equivalence os vs pathlib vs cli for Common Operations","text":"<p>This table provides a comparison between Python's <code>os</code> module and the <code>pathlib</code> library operations alongside their Linux command equivalents for file and directory manipulation, information retrieval, traversal, file input/output (I/O), and path validation. It offers a comprehensive reference for developers familiar with Python who want to understand corresponding operations in Linux command line interfaces. The table is categorized by groups, making it easy to find specific functionalities and their corresponding commands in Python and Linux.</p> More on Table description <p>This table is a comprehensive guide detailing various file and directory operations along with path manipulation using Python's <code>os</code> and <code>pathlib</code> modules, alongside their Linux command equivalents.</p> <ul> <li> <p>Path Manipulation: Operations for obtaining the current working directory, joining paths, and checking path existence are provided.</p> </li> <li> <p>File and Directory Information: It includes functions to get file/directory size, list directory contents, and retrieve file-related timestamps like creation, modification, and access times.</p> </li> <li> <p>File and Directory Operations: This section covers creating and removing directories, renaming files/directories, copying files/directories, as well as commands for creating and deleting files directly.</p> </li> <li> <p>Path Information and Validation: Functions to check if a path points to a file or directory, checking if a path is absolute, and extracting file extensions are included.</p> </li> <li> <p>Path Traversal and Exploration: Operations to iterate through files matching a specific pattern, resolve absolute paths, and extract the parent directory or file/directory name are outlined.</p> </li> <li> <p>File I/O: Covers reading, writing, and appending contents to a file.</p> </li> <li> <p>Path Accessibility: How to check for path accessibility (read, write, execute permissions) using Python's <code>os</code> module alongside Linux command equivalents is explained.</p> </li> </ul> <p>Each operation is represented under its relevant group, detailing the equivalent Pythonic approach using <code>os</code> or <code>pathlib</code>, alongside their corresponding Linux command line alternatives. This table serves as a quick reference for performing common file system-related tasks using Python and Linux commands.</p> Group Operation os pathlib Linux Command Equivalent Path Manipulation Get current working directory <code>os.getcwd()</code> <code>Path.cwd()</code> <code>pwd</code> Join paths <code>os.path.join('/path', 'to', 'join')</code> <code>Path('/path') / 'to' / 'join'</code> <code>joinpath</code> Check path existence <code>os.path.exists('/path')</code> <code>Path('/path').exists()</code> <code>test -e /path</code> File and Directory Information Get file/directory size <code>os.path.getsize('/path')</code> <code>Path('/path').stat().st_size</code> <code>du -b /path</code> List directory contents <code>os.listdir('/path')</code> <code>[item.name for item in Path('/path').iterdir()]</code> <code>ls /path</code> Get file creation time <code>os.path.getctime('/path')</code> <code>Path('/path').stat().st_ctime</code> <code>stat -c %W /path</code> Get file last modification time <code>os.path.getmtime('/path')</code> <code>Path('/path').stat().st_mtime</code> <code>stat -c %Y /path</code> Get file last access time <code>os.path.getatime('/path')</code> <code>Path('/path').stat().st_atime</code> <code>stat -c %X /path</code> File and Directory Operations Create a directory <code>os.makedirs('/path')</code> <code>Path('/path').mkdir()</code> (for single directory), <code>Path('/path').mkdir(parents=True)</code> (for nested directories) <code>mkdir /path</code> Remove an empty directory <code>os.rmdir('/path')</code> <code>Path('/path').rmdir()</code> <code>rmdir /path</code> Remove a non-empty directory <code>shutil.rmtree('/path')</code> <code>shutil.rmtree(Path('/path'))</code> <code>rm -r /path</code> Rename a file/directory <code>os.rename('path/to/source', 'path/to/dest')</code> <code>Path('path/to/source').rename('path/to/dest')</code> <code>mv path/to/source path/to/dest</code> Copy a file <code>shutil.copy('/source/file', '/destination/file')</code> <code>Path('/source/file').replace('/destination/file')</code> <code>cp /source/file /destination/file</code> Copy a directory <code>shutil.copytree('/source/dir', '/destination/dir')</code> <code>shutil.copytree('/source/dir', '/destination/dir')</code> <code>cp -r /source/dir /destination/dir</code> Create a file <code>open('/file/path', 'w').close()</code> <code>Path('/file/path').touch()</code> <code>touch /file/path</code> Remove a file <code>os.remove('/file/path')</code> <code>Path('/file/path').unlink()</code> <code>rm /file/path</code> Path Information and Validation Check if the path is a file <code>os.path.isfile('/path')</code> <code>Path('/path').is_file()</code> <code>test -f /path</code> Check if the path is a directory <code>os.path.isdir('/path')</code> <code>Path('/path').is_dir()</code> <code>test -d /path</code> Check if the path is absolute <code>os.path.isabs('/path')</code> <code>Path('/path').is_absolute()</code> <code>readlink -f /path</code> Get the file extension <code>os.path.splitext('/path')[1]</code> <code>Path('/path').suffix</code> <code>echo /path | grep -o -P '\\.\\K.*'</code> Path Traversal and Exploration Iterate through files matching a pattern <code>glob.glob('/path')</code> <code>Path('/path').glob()</code> or <code>Path('/path').rglob()</code> for recursive search <code>find /path -name pattern</code> Resolve the absolute path <code>os.path.abspath('/path')</code> <code>Path('/path').resolve()</code> <code>realpath /path</code> Get the parent directory <code>os.path.dirname('/path')</code> <code>Path('/path').parent</code> <code>dirname /path</code> Get the file/directory name <code>os.path.basename('/path')</code> <code>Path('/path').name</code> <code>basename /path</code> File I/O Read the contents of a file <code>open('/file/path'', 'r').read()</code> <code>Path('/file/path'').read_text()</code> <code>cat /file/path</code> Write contents to a file <code>open('/file/path'', 'w').write(content)</code> <code>Path('/file/path'').write_text(content)</code> <code>echo content &gt; /file/path</code> Append contents to a file <code>open('/file/path', 'a').write(content)</code> <code>Path('/file/path'').open('a').write(content)</code> <code>echo content &gt;&gt; /file/path</code> Path Accessibility Check path accessibility <code>os.access()</code> Use <code>Path</code> methods in combination with <code>os.access()</code> or <code>os.stat()</code> <code>test -rwx /path</code> <p>Remember !</p> <ul> <li>Always handle potential errors with <code>try-except</code> blocks for file and directory operations.</li> <li>Test your code thoroughly to ensure correctness and handle edge cases.</li> <li>Embrace the object-oriented nature of pathlib for a more intuitive and readable approach to file handling in Python.</li> </ul>"},{"location":"blog/pathlib-tutorial-transitioning-to-simplified-file-and-directory-handling-in-python/#conclusion","title":"Conclusion","text":"<p>Congratulations! You've now gained a foundational understanding of <code>pathlib</code> and its capabilities for file and directory handling in Python. Experiment further by combining these methods and explore additional functionalities for your file manipulation needs.</p> <p>Stay curious and keep exploring to harness the full potential of <code>pathlib</code> in your Python projects!</p>"},{"location":"blog/pathlib-tutorial-transitioning-to-simplified-file-and-directory-handling-in-python/#related-posts","title":"Related Posts","text":"<ul> <li>Cheat on Python Package Managers</li> <li>Removing Directories in Python</li> </ul>"},{"location":"blog/managing-python-projects-with-poetry-a-comprehensive-guide/","title":"Managing Python Projects with Poetry","text":"<p>Poetry simplifies Python project management and dependency handling. It's beginner-friendly and offers advantages like streamlined dependency management, integrated virtual environments, and simplified workflow. Give it a try to experience efficient Python development.</p> <p>Follow these steps to initialize, add dependencies, and manage your project effortlessly:</p>"},{"location":"blog/managing-python-projects-with-poetry-a-comprehensive-guide/#1-install-poetry","title":"1. Install Poetry","text":"<p>If Poetry is not installed, use the following command:</p> <pre><code>pip install poetry\n</code></pre> <p></p>"},{"location":"blog/managing-python-projects-with-poetry-a-comprehensive-guide/#2-create-a-new-project","title":"2. Create a New Project","text":"<p>Navigate to your desired project directory and run:</p> <pre><code>poetry new project_name\n</code></pre> <p>Replace <code>project_name</code> with your desired project name.</p>"},{"location":"blog/managing-python-projects-with-poetry-a-comprehensive-guide/#3-add-dependencies","title":"3. Add Dependencies","text":"<p>Move into your project directory:</p> <pre><code>cd project_name\n</code></pre> <p>Add dependencies with:</p> <pre><code>poetry add package_name\n</code></pre> <p>Replace <code>package_name</code> with the desired package. Poetry manages dependencies in <code>pyproject.toml</code>.</p>"},{"location":"blog/managing-python-projects-with-poetry-a-comprehensive-guide/#4-install-dependencies","title":"4. Install Dependencies","text":"<p>Install dependencies or update existing ones:</p> <pre><code>poetry install\n</code></pre> <p>This creates a virtual environment for your project.</p>"},{"location":"blog/managing-python-projects-with-poetry-a-comprehensive-guide/#5-run-your-project","title":"5. Run Your Project","text":"<p>Execute Python scripts within the Poetry-managed environment:</p> <pre><code>poetry run python your_script.py\n</code></pre> <p>Replace <code>your_script.py</code> with your script's name.</p>"},{"location":"blog/managing-python-projects-with-poetry-a-comprehensive-guide/#6-other-commands","title":"6. Other Commands","text":"<ul> <li>Update a package:</li> </ul> <pre><code>poetry update package_name\n</code></pre> <ul> <li>Access the virtual environment shell:</li> </ul> <pre><code>poetry shell\n</code></pre>"},{"location":"blog/managing-python-projects-with-poetry-a-comprehensive-guide/#adding-a-script","title":"Adding a Script","text":"<p>Modify the <code>pyproject.toml</code> file to include a script:</p> <pre><code>...\n\n[tool.poetry.scripts]\nstart = \"uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload\"\n\n...\n</code></pre> <p>Run the development server with:</p> <pre><code>poetry run start\n</code></pre>"},{"location":"blog/managing-python-projects-with-poetry-a-comprehensive-guide/#additional-dependencies","title":"Additional Dependencies","text":"<p>Add Jupyter as a development dependency:</p> <pre><code>poetry add jupyter --group dev\n</code></pre> <p>Modify <code>pyproject.toml</code>:</p> <pre><code>...\n\njupyter = \"notebook\"\n\n...\n</code></pre>"},{"location":"blog/managing-python-projects-with-poetry-a-comprehensive-guide/#code-formatting-with-isort-and-black","title":"Code Formatting with isort and black","text":"<p>Install <code>isort</code> and <code>black</code>:</p> <pre><code>poetry add --dev isort black\n</code></pre> <p>Configure in <code>pyproject.toml</code>:</p> <pre><code>[tool.isort]\nprofile = \"black\"\n\n[tool.black]\nline-length = 88\n</code></pre> <p>Format code with:</p> <pre><code>poetry run format\n</code></pre>"},{"location":"blog/managing-python-projects-with-poetry-a-comprehensive-guide/#grouped-package-installation","title":"Grouped Package Installation","text":"<p>Install a package in a group:</p> <pre><code>poetry add \"laspy[lazrs,laszip]==2.4.1\" --group laspy\n</code></pre>"},{"location":"blog/managing-python-projects-with-poetry-a-comprehensive-guide/#modifying-python-version","title":"Modifying Python Version","text":"<p>Change Python version requirement in <code>pyproject.toml</code>:</p> <pre><code>[tool.poetry.dependencies]\npython = \"^3.9\"\nmatplotlib = \"3.7.1\"\n</code></pre> <p>Update dependencies:</p> <pre><code>poetry update\n</code></pre>"},{"location":"blog/managing-python-projects-with-poetry-a-comprehensive-guide/#running-the-project","title":"Running the Project","text":"<p>Execute your project with:</p> <pre><code>poetry run your_command_here\n</code></pre> <p>Replace <code>your_command_here</code> with your actual command.</p>"},{"location":"blog/managing-python-projects-with-poetry-a-comprehensive-guide/#generate-a-requirementstxt-file-using-poetry","title":"generate a <code>requirements.txt</code> file using Poetry","text":"<p>Poetry doesn't directly generate a <code>requirements.txt</code> file, but you can generate an equivalent file using the following command:</p> <pre><code>poetry export -f requirements.txt --output requirements.txt --without-hashes\n</code></pre> <p>This command exports your project's dependencies into a <code>requirements.txt</code> file. Here's what each part of the command does:</p> <ul> <li><code>-f requirements.txt</code>: Specifies the format of the output file.</li> <li><code>--output requirements.txt</code>: Sets the output file name.</li> <li><code>--without-hashes</code>: Exports the dependencies without including the hash information for each package.</li> </ul> <p>This command essentially creates a <code>requirements.txt</code> file that lists all the project dependencies along with their versions, making it similar to a standard <code>requirements.txt</code> file commonly used with <code>pip</code>.</p> <p>Remember that this file generated by Poetry won't include any packages managed exclusively by Poetry (like development dependencies or direct URLs to packages). It mainly generates a simplified version of requirements that can be used with <code>pip</code>.</p> <p>Run this command in the root directory of your Poetry-managed project, and it will generate the <code>requirements.txt</code> file based on your project's dependencies.</p>"},{"location":"blog/managing-python-projects-with-poetry-a-comprehensive-guide/#configuring-isort-and-black-with-poetry","title":"Configuring isort and black with Poetry","text":"<p>To maintain consistent code formatting and import organization in your Python projects managed with Poetry, you can configure tools like <code>isort</code> and <code>black</code>. Here's how you can integrate them into your Poetry workflow:</p> <ol> <li> <p>Create or open your <code>pyproject.toml</code> file in the root of your project directory.</p> </li> <li> <p>Add the <code>[tool.isort]</code> and <code>[tool.black]</code> sections along with the configuration settings:</p> </li> </ol> <pre><code>[tool.isort]\nprofile = \"black\"\n\n[tool.black]\nline-length = 100\n</code></pre> <ol> <li> <p>Save the <code>pyproject.toml</code> file.</p> </li> <li> <p>When you run <code>poetry install</code> or <code>poetry update</code> to manage your project dependencies, Poetry will consider the configuration provided for <code>isort</code> and <code>black</code>.</p> </li> <li> <p>Additionally, you can run <code>isort</code> and <code>black</code> directly with Poetry by using <code>poetry run</code> followed by the respective command. For example:</p> </li> </ol> <pre><code>poetry run isort .\npoetry run black .\n</code></pre> <p>This will run <code>isort</code> and <code>black</code> with the configurations specified in the <code>pyproject.toml</code> file on your project files.</p>"},{"location":"blog/managing-python-projects-with-poetry-a-comprehensive-guide/#adding-optional-dependencies","title":"Adding optional dependencies","text":"<p>You can add optional dependencies in poetry by adding them in the extra packages (more here)</p> <p>For example</p> <pre><code>[tool.poetry.dependencies]\npython = \"&gt;=3.8.1,&lt;3.12\"\nopencf-core = \"^0.2.0\"\ngeneralimport = \"^0.5.2\"\nnumpy = = \"^1.24.0\"\nopencv-python = {version = \"^4.8.1.78\", optional = true}\npillow = {version = \"^10.3.0\", optional = true}\npypdf2 = {version = \"^3.0.1\", optional = true}\nimageio = {version = \"^2.30.0\", optional = true}\n\n\n[tool.poetry.extras]\ndocument = [\"pdf2docx\", \"pypdf2\"]\nimage = [\"pillow\", \"imageio\"]\n</code></pre> <p>Here, 4 dependecncies are kept otional. The command <code>poetry install</code> would not install them. To add all <code>document</code> dependancies, use <code>poetry install --extras document</code> If you have make it a python module, use, <code>pip install mypackage[extras,...]</code></p> <p>For example</p> <pre><code>pip install mypackage[document]\npip install mypackage[document, image]\n</code></pre> <p>This is different from the dev dependancies, and groups in general</p>"},{"location":"blog/managing-python-projects-with-poetry-a-comprehensive-guide/#platform-specific-dependencies","title":"Platform specific dependencies","text":"<p>anwers from this stackoverflow thread: platform-specific dependency alternatives?</p> <p>Enjoy seamless Python project management with Poetry! Happy coding!</p>"},{"location":"blog/managing-python-projects-with-poetry-a-comprehensive-guide/#related-posts","title":"Related Posts","text":"<ul> <li>Beginner's guide to poetry</li> <li>Cheat on Python Package Managers</li> <li>How to publish your python project (to pypi or testPypi) with Poetry then install as dependency in another project ?</li> </ul>"},{"location":"blog/publishing-your-python-project-with-poetry/","title":"Publishing Your Python Project with Poetry","text":"<p>Are you ready to share your Python project with the world and make it accessible to users and developers alike?</p> <p>Publishing your work not only showcases your creation but also enables others to benefit from and contribute to your project. Discover the simplest way to publish Python projects using Poetry, a powerful dependency management and packaging tool.</p> <p>In this guide, we'll walk you through setting up your project with Poetry and publishing it on both test.pypi and production PyPI.</p> <p>Get ready to make your project accessible for testing and distribution. Let's get started!</p>"},{"location":"blog/publishing-your-python-project-with-poetry/#getting-started-with-poetry","title":"Getting Started with Poetry","text":"<p>Poetry is one of the simplest and most powerful tools for managing Python dependencies and packaging your projects. If you're new to Poetry, don't worry! We'll guide you through the process step by step.</p> <p>First, ensure you have Poetry installed on your system:</p> <pre><code>pip install poetry\n</code></pre> <p>Next, create your project using Poetry:</p> <pre><code>poetry new project-folder-name\n</code></pre> <p>Certainly! Here's the modified section with a visual representation of the folder structure:</p> <pre><code>Next, create your project using Poetry:\n\n```bash\npoetry new project-folder-name\n</code></pre> <p>When you run this command, Poetry will generate a project structure with the following folders and files:</p> <pre><code>project-folder-name/\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 your_main_python_file.py\n\u251c\u2500\u2500 tests/\n\u2502   \u2514\u2500\u2500 test_your_main_python_file.py\n\u251c\u2500\u2500 poetry.lock\n\u2514\u2500\u2500 .gitignore\n</code></pre> python project file architecture for packaging <ul> <li>project-folder-name/: This is the root directory of your project. It contains all the files and folders related to your project.</li> <li>pyproject.toml: This file is the Poetry configuration file where you define your project's dependencies, Python version, and other settings.</li> <li>README.rst: This file contains the documentation for your project. You can use reStructuredText format for writing documentation.</li> <li>src/: This folder is where your Python source code files reside. Placing your source files in a subfolder like this is important for proper project management. It helps keep your project organized and separate from other files, such as tests or configuration files.</li> <li>tests/: This folder is where you write tests for your project. Writing tests is crucial for ensuring your code works as expected and for maintaining code quality.</li> <li>poetry.lock: This file is automatically generated by Poetry and contains information about the exact versions of your project's dependencies.</li> <li>.gitignore: This file specifies which files and directories should be ignored by version control systems like Git. It helps keep irrelevant files out of your repository.</li> </ul> <p>Organizing your project files in this way helps maintain a clean and structured project layout, making it easier to manage, collaborate on, and maintain your codebase.</p> <p>For Python project management with Poetry, refer to this documentation. If you're not familiar with Poetry, you can check out this beginner's guide to get started or this this python dependency management cheat for usual commands.</p> Package name vs Module name <p>If you want to set your custom module name, you can update your pyproject.toml file to do so</p> <p>For example, if you want you package name on pypi to be <code>openconv-python</code>, but the name of the module <code>openconv</code> instead of <code>openconv_python</code>, you can do as below. </p> <pre><code>[tool.poetry]\nname = \"openconv-python\"\npackages = [\n    {include = \"openconv\"}\n]\n</code></pre> <p>Then, make sure put your module files inside the directory <code>openconv</code></p>"},{"location":"blog/publishing-your-python-project-with-poetry/#publishing-your-project-on-pypi-or-test-pypi","title":"Publishing Your Project on PyPi or Test PyPI","text":""},{"location":"blog/publishing-your-python-project-with-poetry/#account-setup-one-time-process","title":"Account Setup (One-Time Process)","text":"<p>Before publishing your project on pypi test.pypi, ensure your account setup is complete:</p> <ol> <li> <p>Create an account: Visit pypi.org or test.pypi.org and create an account using your email address.</p> </li> <li> <p>Verify your email: Check your inbox for a verification email from pypi.org or test.pypi.org and follow the instructions to verify your email address.</p> </li> <li> <p>Set up Two-Factor Authentication (2FA): Navigate to your account settings on pypi.org test.pypi.org and enable 2FA for added security. Follow the instructions to set up 2FA using an authenticator app or SMS.</p> </li> </ol> <p>Once your account setup is complete, proceed with configuring Poetry for publishing your project.</p>"},{"location":"blog/publishing-your-python-project-with-poetry/#configuring-poetry-one-time-setup","title":"Configuring Poetry (One-Time Setup)","text":"<p>Follow these steps to configure Poetry for publishing on test.pypi:</p> <code>for test-pypi</code> <code>for pypi</code> <ol> <li> <p>Add the test.pypi repository to your Poetry config:</p> <pre><code>poetry config repositories.test-pypi https://test.pypi.org/legacy/\n</code></pre> </li> <li> <p>Obtain a token from test.pypi.org and store it:</p> <pre><code>poetry config pypi-token.test-pypi pypi-YYYYYYYY\n</code></pre> </li> </ol> <ol> <li> <p>Add the pypi repository to your Poetry config:</p> <pre><code>poetry config repositories.pypi https://pypi.org/legacy/\n</code></pre> </li> <li> <p>Obtain a token from pypi.org and store it:</p> <pre><code>poetry config pypi-token.pypi pypi-YYYYYYYY\n</code></pre> </li> </ol>"},{"location":"blog/publishing-your-python-project-with-poetry/#publishing-to-pypi-or-test-pypi","title":"Publishing to Pypi or Test PyPI","text":"<p>When you're ready to publish your project for testing:</p> <ol> <li> <p>Bump the version using Poetry:</p> <pre><code>poetry version prerelease\n# or\npoetry version patch\n# or\npoetry version minor\n# or\npoetry version major\n</code></pre> </li> <li> <p>Build the new version</p> <pre><code>poetry build\n</code></pre> <p>At this point, you can even use the package on your own computer</p> </li> <li> <p>Publish your project:</p> <code>to test-pypi</code> <code>to pypi</code> <pre><code>poetry publish -r test-pypi\n</code></pre> <pre><code>poetry publish\n</code></pre> </li> </ol>"},{"location":"blog/publishing-your-python-project-with-poetry/#using-the-package-from-pypi-or-testpypi","title":"Using the Package from PyPI or TestPyPI","text":"<p>Once your Python project is packaged and published on PyPI or TestPyPI, other developers can easily use it in their projects. Here's how they can add your package as a dependency using both pip and Poetry:</p>"},{"location":"blog/publishing-your-python-project-with-poetry/#using-pip","title":"Using pip","text":"<code>from pypi</code> <code>from test-pypi</code> <p>To use your package from PyPI, other developers can simply add it to their project's dependencies using pip. They can do this by running the following command:</p> <pre><code>pip install your-package-name\n</code></pre> <p>Replace <code>your-package-name</code> with the name of your package as published on PyPI.</p> <p>If your package is published on TestPyPI for testing purposes, developers can also add it to their projects similarly using pip and Poetry, but they need to specify the TestPyPI repository URL. Here's how they can do it:</p> <pre><code>pip install --index-url https://test.pypi.org/simple/ your-package-name\n</code></pre> <p>Replace <code>your-package-name</code> with the name of your package as published on TestPyPI.</p>"},{"location":"blog/publishing-your-python-project-with-poetry/#using-poetry","title":"Using Poetry","text":"<code>from test-pypi</code> <code>from pypi</code> <p>For Poetry users, they can specify the TestPyPI repository URL in their <code>pyproject.toml</code> file before adding the package (more details here):</p> <pre><code>[[tool.poetry.source]]\nname = \"test-pypi\"\nurl = \"https://test.pypi.org/simple/\"\npriority = \"explicit\"\n</code></pre> <p>Then, they can add the package using Poetry as usual:</p> <pre><code>poetry add your-package-name --source test-pypi\n</code></pre> <p>Again, replace <code>your-package-name</code> with the name of your package as published on TestPyPI.</p> <p>For developers using Poetry, they can add your package to their project by specifying it in their <code>pyproject.toml</code> file. They can do this by running:</p> <pre><code>poetry add your-package-name\n</code></pre> <p>Again, replace <code>your-package-name</code> with the name of your package as published on PyPI.</p> <p>By following these steps, other developers can easily include your package as a dependency in their Python projects, whether it's from PyPI or TestPyPI.</p>"},{"location":"blog/publishing-your-python-project-with-poetry/#conclusion","title":"Conclusion","text":"<p>Congratulations! You've learned how to publish your Python project using Poetry. Whether you're testing your package on test.pypi or releasing it to the production PyPI, Poetry simplifies the process and ensures smooth distribution of your projects.</p> <p>For further reference, you can also check out this Stack Overflow answer demonstrating the publishing process.</p> <p>Now, go ahead and share your amazing Python projects with the world!</p>"},{"location":"blog/publishing-your-python-project-with-poetry/#related-posts","title":"Related Posts","text":"<ul> <li>Beginner's guide to poetry</li> <li>python dependency management cheat including poetry, conda and more</li> <li>Introducing Two New Packages for Streamlining File Conversions in Python</li> </ul>"},{"location":"blog/publishing-your-python-project-with-poetry/#relevant-sources","title":"Relevant Sources","text":"<ul> <li>Test PyPI, PyPI, Python Packaging</li> </ul>"},{"location":"blog/exploring-python-code-formatters-and-linters-black-vs-flake8-vs-isort-vs-autopep8-vs-yapf-vs-pylint-vs-ruff-and-more/","title":"Exploring Python Code Formatters and Linters: black vs. flake8 vs. isort vs. autopep8 vs. yapf vs. pylint vs. ruff and more","text":""},{"location":"blog/exploring-python-code-formatters-and-linters-black-vs-flake8-vs-isort-vs-autopep8-vs-yapf-vs-pylint-vs-ruff-and-more/#introduction","title":"Introduction","text":"<p>Are you struggling to maintain consistent formatting in your Python code? Do you find yourself spending too much time organizing imports or adjusting code style manually?</p> <p>Navigating the landscape of Python code formatters and linters can be overwhelming, especially for beginners.</p> <p>This guide serves as your roadmap to mastering Python code formatters and linters, simplifying the process and providing practical examples for effective code formatting, organization, and analysis.</p> <p>Whether you're a novice Python developer or an experienced programmer looking to streamline your workflow, this document is tailored to demystify Python code formatters and linters, offering insights into popular tools like Black, isort, YAPF, ruff, flake8, and pylint. By the end, you'll have a solid understanding of how to leverage these tools to enhance the readability and maintainability of your Python codebase.</p>"},{"location":"blog/exploring-python-code-formatters-and-linters-black-vs-flake8-vs-isort-vs-autopep8-vs-yapf-vs-pylint-vs-ruff-and-more/#overview","title":"Overview","text":"<p>Python code formatters and linters are tools designed to improve the readability and maintainability of Python code by enforcing consistent formatting and organization standards, while also detecting potential issues and code smells. These tools automatically format your code according to predefined rules and analyze it for potential problems, saving time and ensuring adherence to best practices. In this document, we'll explore popular Python code formatters and linters, including Black, isort, YAPF, ruff, flake8, pylint, and more.</p>"},{"location":"blog/exploring-python-code-formatters-and-linters-black-vs-flake8-vs-isort-vs-autopep8-vs-yapf-vs-pylint-vs-ruff-and-more/#key-considerations","title":"Key Considerations","text":""},{"location":"blog/exploring-python-code-formatters-and-linters-black-vs-flake8-vs-isort-vs-autopep8-vs-yapf-vs-pylint-vs-ruff-and-more/#choosing-the-right-tool","title":"Choosing the Right Tool","text":"<ul> <li>Code Formatting vs. Code Analysis: Distinguish between tools primarily focused on formatting (black, autopep8, yapf, isort) and those focused on analysis (flake8, pylint, ruff).</li> <li>Customizability: Assess the level of customization offered by each tool to align with project-specific style guidelines and requirements.</li> <li>Integration: Consider compatibility and integration with existing development workflows, IDEs, and automation tools.</li> <li>Performance: Evaluate the speed and resource consumption of each tool, especially for large codebases.</li> <li>Community and Support: Explore the size and activity of the user community, available documentation, and support resources.</li> </ul>"},{"location":"blog/exploring-python-code-formatters-and-linters-black-vs-flake8-vs-isort-vs-autopep8-vs-yapf-vs-pylint-vs-ruff-and-more/#tools-overview","title":"Tools Overview","text":"<code>isort</code> <code>black</code> <code>yapf</code> <code>autopep8</code> <code>ruff</code> <code>flake8</code> <code>pylint</code> <ul> <li>Import Sorter: Isort is a utility for sorting and organizing Python imports within code files.</li> <li>Automatic Import Sorting: Automatically organizes imports alphabetically and groups them by type (standard library, third-party, local imports).</li> <li>Configuration: Offers a variety of configuration options through a <code>pyproject.toml</code> or <code>setup.cfg</code> file to customize import sorting behavior.</li> <li>Integration: Easily integrates with other tools and formatters such as black to ensure consistent code styling.</li> <li>Performance: Known for its fast execution speed, making it suitable for large codebases.</li> </ul> <ul> <li>Code Formatter: Black is an opinionated code formatter that enforces a consistent code style throughout Python projects.</li> <li>Automatic Formatting: Requires no configuration and automatically reformats code to comply with PEP 8 standards.</li> <li>Uncompromising: Focuses on minimizing differences between code styles, favoring simplicity and readability over customization.</li> <li>Integration: Seamlessly integrates with most IDEs, text editors, and CI/CD pipelines.</li> <li>Community Support: Backed by a vibrant community and actively maintained.</li> </ul> <ul> <li>Yet Another Python Formatter: Yapf is a Python code formatter that aims for readability, consistency, and ease of use.</li> <li>Configurable Formatting: Offers various formatting options and presets to customize code styling according to project preferences.</li> <li>Integration: Supports integration with popular IDEs, text editors, and automation tools for seamless code formatting.</li> <li>Presets: Provides built-in presets for different Python styles, allowing users to quickly apply common formatting configurations.</li> <li>Community: Backed by an active community and ongoing development efforts.</li> </ul> <ul> <li>Automatic Code Formatter: Autopep8 automatically formats Python code to conform to the PEP 8 style guide.</li> <li>Prescriptive Formatting: Provides simple and prescriptive formatting rules, focusing on improving code consistency and readability.</li> <li>Command-Line Tool: Can be used as a command-line tool or integrated into text editors and IDEs for automatic code formatting.</li> <li>Configuration: Allows some degree of customization through command-line options and configuration files.</li> <li>Community: Supported by a community of users and actively maintained.</li> </ul> <ul> <li>Code Linter and Formatter: Ruff is a fast and highly customizable code linter and formatter for Python, combining the functionality of tools like flake8 and black.</li> <li>Automatic Formatting: Ruff can automatically format Python code to comply with PEP 8 standards.</li> <li>Customizable: Ruff offers a variety of configuration options through a <code>.ruff.toml</code> or <code>pyproject.toml</code> file to customize code analysis and formatting behavior.</li> <li>Integration: Ruff seamlessly integrates with most IDEs, text editors, and CI/CD pipelines.</li> <li>Community Support: Backed by a growing community and actively maintained.</li> </ul> <ul> <li>Code Linter: Flake8 is a code analysis tool that checks Python code against coding style (PEP 8) and detects various programming errors.</li> <li>Modular Architecture: Consists of several plugins for code style enforcement, syntax checking, and error detection (e.g., pep8, mccabe, pyflakes).</li> <li>Customizable: Allows configuration through a <code>.flake8</code> configuration file to adjust rules and behavior according to project requirements.</li> <li>Extensibility: Supports custom plugins and extensions to enhance functionality and add additional checks.</li> <li>Usage: Typically used as part of CI/CD pipelines or integrated into text editors for real-time feedback.</li> </ul> <ul> <li>Code Checker: Pylint is a static code analysis tool that checks Python code for errors, potential bugs, and adherence to coding standards.</li> <li>Extensive Checks: Performs a wide range of checks including code style, error detection, code complexity analysis, and more.</li> <li>Highly Configurable: Offers extensive configuration options through a <code>.pylintrc</code> file to adjust the level of strictness and enable/disable specific checks.</li> <li>Integration: Integrates with various IDEs, text editors, and CI/CD pipelines for automated code analysis and feedback.</li> <li>Learning Curve: May have a steeper learning curve due to its comprehensive feature set and configuration options.</li> </ul>"},{"location":"blog/exploring-python-code-formatters-and-linters-black-vs-flake8-vs-isort-vs-autopep8-vs-yapf-vs-pylint-vs-ruff-and-more/#comparison-table","title":"Comparison Table","text":"Feature Type Focus Configuration Automatic Formatting Integration Customization Speed Error Detection isort Import Sorter Import Sorting Configurable Yes Compatible Customizable (Configuration Files) Fast Limited to Import Errors black Formatter Code Formatting Minimal (No Configuration) Yes Seamless Integration Limited (Follows Strict Rules) Fast Limited to Formatting Errors yapf Formatter Code Formatting Highly Configurable Yes Integration with IDEs, Text Editors Extensive (Configuration Options) Moderate Limited to Formatting Errors autopep8 Formatter Code Formatting Configurable Yes Integration with IDEs, Text Editors Limited (Some Configuration Options) Moderate Limited to Formatting Errors ruff Linter and Formatter Code Analysis and Formatting Highly Configurable Yes Seamless Integration Moderate Fast Wide Range of Programming Errors and Formatting Errors flake8 Linter Code Analysis Highly Configurable No Integration via Plugins Extensive (Adjustable via .flake8 file) Moderate Wide Range of Programming Errors pylint Linter Code Analysis Highly Configurable No Integration with IDEs, Text Editors Extensive (Configuration via .pylintrc) Moderate Wide Range of Programming Errors"},{"location":"blog/exploring-python-code-formatters-and-linters-black-vs-flake8-vs-isort-vs-autopep8-vs-yapf-vs-pylint-vs-ruff-and-more/#common-commands","title":"Common Commands","text":"Feature Installation pyproject.toml Config Other Configs Format Code Check Code isort <code>pip install isort</code> <code>[tool.isort]</code> <code>setup.cfg, tox.ini, .pep8, .flake8</code> <code>isort &lt;files_or_dir&gt;</code> N/A black <code>pip install black</code> <code>[tool.black]</code> N/A <code>black &lt;files_or_dir&gt;</code> N/A yapf <code>pip install yapf</code> <code>[tool.yapf]</code> <code>.style.yapf, setup.cfg</code> <code>yapf &lt;files_or_dir&gt; -i -r</code> <code>yapf &lt;files_or_dir&gt; -r</code> autopep8 <code>pip install autopep8</code> <code>[tool.autopep8]</code> <code>setup.cfg, tox.ini, .pep8, .flake8</code> <code>autopep8 &lt;files_or_dir&gt; --in-place --recursive</code> <code>autopep8 &lt;files_or_dir&gt; --recursive</code> ruff <code>pip install ruff</code> <code>[tool.ruff]</code> <code>.ruff.toml</code> <code>ruff format &lt;files_or_dir&gt;</code> <code>ruff check &lt;files_or_dir&gt;</code> flake8 <code>pip install flake8</code> N/A <code>.flake8</code> N/A <code>flake8 &lt;files_or_dir&gt;</code> pylint <code>pip install pylint</code> <code>[tool.pylint]</code> <code>.pylintrc</code> N/A <code>pylint &lt;files_or_dir&gt;</code> More Options for isort <ul> <li> <p>Sorting Imports with isort Using the Black Profile</p> <p>To sort import statements using isort with the Black profile, run:</p> <pre><code>isort --profile black\n</code></pre> </li> </ul> More Options for YAPF <ul> <li> <p>Specify Line Length: You can specify the line length for formatting using the <code>-l</code> or <code>--style</code> option:</p> <pre><code>yapf -i --style=\"{based_on_style: google, column_limit: 120}\" your_file.py\n</code></pre> <p>This command formats <code>your_file.py</code> with a line length limit of 120 characters, based on the Google style guide.</p> </li> <li> <p>Preserve Existing Formatting: YAPF allows you to preserve the existing formatting to some extent using the <code>--style</code> option:</p> <pre><code>yapf -i --style=\"{based_on_style: pep8, indent_width: 4}\" your_file.py\n</code></pre> <p>This command formats <code>your_file.py</code> while preserving the existing indentation width of 4 spaces, based on the PEP 8 style guide.</p> </li> <li> <p>Dry Run Mode: You can use the <code>--diff</code> option to perform a dry run and see the proposed changes without actually modifying the files:</p> <pre><code>yapf --diff your_file.py\n</code></pre> <p>This command displays the proposed changes to <code>your_file.py</code> without modifying the file itself.</p> </li> <li> <p>Recursive Formatting: To format Python files in the current directory and all subdirectories, use the <code>--recursive</code> flag:</p> <pre><code>yapf --in-place --recursive .\n</code></pre> <p>This command recursively formats all <code>.py</code> files starting from the current directory and traversing through all subdirectories.</p> </li> </ul> More Options for Ruff <ul> <li> <p>Specific Rule Selection: You can enable or disable specific rules:</p> <pre><code>ruff check --select E4,E7,E9,F\n</code></pre> </li> <li> <p>Fix Issues Automatically: Ruff can automatically fix many issues:</p> <pre><code>ruff check --fix\n</code></pre> </li> <li> <p>Watch Mode: Run ruff in watch mode to continuously check files:</p> <pre><code>ruff check --watch\n</code></pre> </li> </ul> <p>Other Python Code Formatters and Linters</p> <p>In addition to the tools mentioned above, there are several other Python formatters and linters available:</p> <ul> <li>pyfmt: A code formatter that aims to balance flexibility and readability, offering customizable formatting options.</li> <li>pycodestyle: A tool that checks Python code against the PEP 8 style guide and provides suggestions for improvements.</li> <li>blue: A tool created for the sole reason of using single quotes wherever possible.</li> <li>bandit: A security linter that scans Python code for common security issues.</li> <li>vulture: Finds unused code in Python programs.</li> </ul>"},{"location":"blog/exploring-python-code-formatters-and-linters-black-vs-flake8-vs-isort-vs-autopep8-vs-yapf-vs-pylint-vs-ruff-and-more/#supercharge-your-ci-with-formatters-and-linters","title":"Supercharge Your CI with Formatters and Linters","text":"<p>Combining isort, Black, and Ruff</p> <p>You can combine multiple tools to format and analyze your Python files efficiently:</p> <pre><code>#!/bin/bash\n# Format and sort imports\nisort . --profile black\nblack . -l 100\n\n# Lint and check for issues\nruff check .\nflake8 .\n</code></pre> <p>This script first sorts imports with isort, formats code with Black, and then runs comprehensive linting with ruff and flake8.</p> Example with find + Multiple Tools <p>To recursively format and check Python files in a directory and its subdirectories, while excluding certain directories like 'env' and '.git':</p> <pre><code>#!/bin/bash\n# Format Python files\nfind . -type f -name \"*.py\" ! -path \"*env/*\" ! -path \"*.git/*\" -exec isort {} +\nfind . -type f -name \"*.py\" ! -path \"*env/*\" ! -path \"*.git/*\" -exec black -l 100 {} +\n\n# Check for issues\nruff check .\nflake8 .\npylint --recursive=y .\n</code></pre> <p>These commands will find all Python files in the current directory and its subdirectories, excluding specific directories like 'env' and '.git', and then apply formatting and comprehensive linting.</p> <p>GitHub Actions CI Configuration</p> <p>Here's a complete GitHub Actions workflow for code formatting and linting:</p> <pre><code>name: Code Quality\n\non: [push, pull_request]\n\njobs:\n  code-quality:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v3\n\n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: '3.9'\n\n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        pip install black isort ruff flake8 pylint\n\n    - name: Run formatters\n      run: |\n        isort --check-only --profile black .\n        black --check .\n\n    - name: Run linters\n      run: |\n        ruff check .\n        flake8 .\n        pylint --recursive=y .\n</code></pre> pyproject.toml Configuration <p>Comprehensive configuration for multiple tools:</p> <pre><code>[tool.black]\nline-length = 100\ntarget-version = ['py39']\ninclude = '\\.pyi?$'\nextend-exclude = '''\n/(\n  # directories\n  \\.eggs\n  | \\.git\n  | \\.hg\n  | \\.mypy_cache\n  | \\.tox\n  | \\.venv\n  | build\n  | dist\n)/\n'''\n\n[tool.isort]\nprofile = \"black\"\nline_length = 100\nmulti_line_output = 3\ninclude_trailing_comma = true\nforce_grid_wrap = 0\nuse_parentheses = true\nensure_newline_before_comments = true\n\n[tool.ruff]\nline-length = 100\ntarget-version = \"py39\"\nselect = [\n    \"E\",  # pycodestyle errors\n    \"W\",  # pycodestyle warnings\n    \"F\",  # pyflakes\n    \"I\",  # isort\n    \"N\",  # pep8-naming\n    \"UP\", # pyupgrade\n    \"YTT\", # flake8-2020\n    \"S\",  # bandit\n    \"BLE\", # flake8-blind-except\n    \"B\",  # flake8-bugbear\n    \"A\",  # flake8-builtins\n    \"C4\", # flake8-comprehensions\n    \"T10\", # flake8-debugger\n    \"ISC\", # flake8-implicit-str-concat\n    \"ICN\", # flake8-import-conventions\n    \"PT\",  # flake8-pytest-style\n    \"Q\",   # flake8-quotes\n    \"RET\", # flake8-return\n    \"SIM\", # flake8-simplify\n    \"TID\", # flake8-tidy-imports\n    \"ARG\", # flake8-unused-arguments\n    \"ERA\", # eradicate\n    \"PGH\", # pygrep-hooks\n    \"PLC\", # pylint conventions\n    \"PLE\", # pylint errors\n    \"PLR\", # pylint refactor\n    \"PLW\", # pylint warnings\n    \"RUF\", # ruff-specific rules\n]\nignore = [\n    \"E501\",  # Line too long (handled by black)\n    \"S101\",  # Use of assert\n    \"PLR0913\", # Too many arguments to function call\n]\n\n[tool.ruff.per-file-ignores]\n\"tests/*\" = [\"S101\", \"PLR2004\"]\n\n[tool.pylint.messages_control]\ndisable = [\n    \"line-too-long\",\n    \"too-many-arguments\",\n    \"too-many-locals\",\n    \"too-few-public-methods\",\n    \"missing-module-docstring\",\n    \"missing-class-docstring\",\n    \"missing-function-docstring\",\n]\n\n[tool.pylint.format]\nmax-line-length = 100\n</code></pre>"},{"location":"blog/exploring-python-code-formatters-and-linters-black-vs-flake8-vs-isort-vs-autopep8-vs-yapf-vs-pylint-vs-ruff-and-more/#best-practices","title":"Best Practices","text":""},{"location":"blog/exploring-python-code-formatters-and-linters-black-vs-flake8-vs-isort-vs-autopep8-vs-yapf-vs-pylint-vs-ruff-and-more/#tool-selection-strategy","title":"Tool Selection Strategy","text":"<ol> <li>Start Simple: Begin with <code>black</code> for formatting and <code>ruff</code> for linting</li> <li>Add Gradually: Introduce additional tools like <code>isort</code> for import sorting</li> <li>Team Consistency: Ensure all team members use the same configuration</li> <li>CI Integration: Run formatters and linters in your CI pipeline</li> </ol>"},{"location":"blog/exploring-python-code-formatters-and-linters-black-vs-flake8-vs-isort-vs-autopep8-vs-yapf-vs-pylint-vs-ruff-and-more/#performance-optimization","title":"Performance Optimization","text":"<ol> <li>Use Ruff: Consider <code>ruff</code> as a faster alternative to multiple tools</li> <li>Parallel Execution: Run different tools in parallel in CI</li> <li>Incremental Checking: Use tools that support checking only changed files</li> <li>Caching: Enable caching in CI to speed up repeated runs</li> </ol>"},{"location":"blog/exploring-python-code-formatters-and-linters-black-vs-flake8-vs-isort-vs-autopep8-vs-yapf-vs-pylint-vs-ruff-and-more/#configuration-management","title":"Configuration Management","text":"<ol> <li>Centralized Config: Use <code>pyproject.toml</code> for tool configuration</li> <li>Consistent Settings: Align settings across tools (e.g., line length)</li> <li>Version Control: Include configuration files in version control</li> <li>Documentation: Document tool choices and configurations for your team</li> </ol>"},{"location":"blog/exploring-python-code-formatters-and-linters-black-vs-flake8-vs-isort-vs-autopep8-vs-yapf-vs-pylint-vs-ruff-and-more/#conclusion","title":"Conclusion","text":"<p>Python code formatters and linters are essential tools for maintaining high-quality, consistent codebases. By combining formatters like Black and isort with linters like ruff, flake8, and pylint, developers can ensure their code adheres to established standards and best practices, leading to more efficient and collaborative development processes.</p> <p>The key is to start with essential tools and gradually build up your toolchain based on your project's needs. Modern tools like ruff are making it easier to get comprehensive code analysis with minimal configuration, while established tools like Black continue to provide reliable, opinionated formatting.</p>"},{"location":"blog/exploring-python-code-formatters-and-linters-black-vs-flake8-vs-isort-vs-autopep8-vs-yapf-vs-pylint-vs-ruff-and-more/#related-posts","title":"Related Posts","text":"<ul> <li>Read Python Type Checking Tools: mypy vs. pyright vs. pydantic vs. pandera vs. jaxtyping vs. check_shapes vs. typeguard</li> <li>Cheat on Python Package Managers</li> </ul>"},{"location":"blog/exploring-python-code-formatters-and-linters-black-vs-flake8-vs-isort-vs-autopep8-vs-yapf-vs-pylint-vs-ruff-and-more/#relevant-sources","title":"Relevant Sources","text":"<ul> <li>Black and Blues - lewoudar.medium.com</li> <li>Implement autoformat capabilities - ruff github issue</li> <li>Some notes on the right formatter to use for your project - github.com/orsinium</li> <li>On testing flake8 - blog - makeuseof.com</li> <li>Seems flake8 is better ?! - reddit</li> <li>Ruff Documentation - docs.astral.sh</li> <li>Black Documentation - black.readthedocs.io</li> <li>isort Documentation - pycqa.github.io/isort</li> <li>Flake8 Documentation - flake8.pycqa.org</li> <li>Pylint Documentation - pylint.pycqa.org</li> </ul>"},{"location":"blog/generating-api-documentation-for-your-python-package-with-sphinx-apidoc/","title":"Generating API Documentation for Your Python Package with Sphinx-apidoc","text":""},{"location":"blog/generating-api-documentation-for-your-python-package-with-sphinx-apidoc/#introduction","title":"Introduction","text":"<p>In this guide, we will explore how to generate API documentation for your Python package using Sphinx-apidoc. Effective documentation is crucial for understanding, maintaining, and collaborating on software projects. Sphinx-apidoc is a tool that automates the process of creating documentation from docstrings within your Python codebase. Let's dive into the steps required to set up and utilize this tool effectively.</p>"},{"location":"blog/generating-api-documentation-for-your-python-package-with-sphinx-apidoc/#motivation","title":"Motivation","text":"<p>In my quest to streamline the documentation process for my python projects as packages, I embarked on a journey to explore methods of leveraging docstrings to automatically generate documentation compatible with ReadTheDocs or MkDocs, platforms I'm familiar with and prefer due to their formatting styles. Despite my inclination towards MkDocs, I encountered a dilemma as it lacks built-in support for API documentation.</p> <p>Amidst my research, I stumbled upon a pivotal discovery. A question posed by a fellow developer seeking to automatically generate Sphinx documentation from their docstrings led me to a solution. Users had proposed a script, which eventually found its way into the Sphinx ecosystem as sphinx-apidoc, aligning perfectly with my requirements.</p> <p>Despite the prevalence of Sphinx in platforms like ReadTheDocs, I encountered discussions debating the merits of MkDocs versus Sphinx, particularly concerning module documentation. While MkDocs appealed to me for its simplicity, the absence of built-in support for API documentation posed a significant hurdle. This realization prompted me to explore alternatives and delve later on into lesser-known solutions like mkdocstrings or react based ones or whatever solution is used in pydandic api documentation, built using mkdocs with some automation.</p> Example of documentations with these tools: <ul> <li>MkDocs Documentation: Showcases simplicity in general documentation.</li> <li>Requests Documentation: An example of documentation built with Sphinx, although some may find the theme less appealing.</li> <li>Open3D v0.17 Documentation: Known for its visually appealing documentation despite using Sphinx.</li> <li>Open3D v0.18 Documentation: Built with Sphinx using another theme.</li> </ul> <p>With my decision made, I opted to utilize <code>sphinx-apidoc</code> for its efficiency and compatibility with Sphinx. My journey involved generating a foundational Sphinx documentation, employing <code>sphinx-apidoc</code> to automate the creation of documentation directives, integrating them into my project's index file, and finally, compiling the documentation. Alongside these steps, I undertook additional refinements, such as adopting the widely-used sphinx-rtd-theme for an enhanced visual experience and leveraging napoleon for greater flexibility in docstring formatting, eliminating the need for reStructuredText within the docstrings themselves.</p>"},{"location":"blog/generating-api-documentation-for-your-python-package-with-sphinx-apidoc/#installing-sphinx","title":"Installing Sphinx","text":"<p>Before we can generate documentation with Sphinx-apidoc, we need to install Sphinx. Sphinx is a documentation generation tool widely used in the Python community.</p> <code>using pip</code> <code>using apt</code> <p>You can install Sphinx using pip:</p> <pre><code>pip install -U sphinx\n</code></pre> <p>Alternatively, you can install it globally with:</p> <pre><code>sudo apt update\nsudo apt install python3-sphinx\n</code></pre> to uninstall <code>using pip</code> <pre><code>pip uninstall sphinx\n</code></pre> <code>using apt</code> <pre><code>sudo apt remove python3-sphinx\nsudo apt autoremove\n</code></pre>"},{"location":"blog/generating-api-documentation-for-your-python-package-with-sphinx-apidoc/#generating-documentation-base-files-for-a-basic-setup","title":"Generating Documentation Base Files for a Basic Setup","text":"<p>To start generating documentation with Sphinx-apidoc, we first need to create the base files for our documentation project. This can be achieved using the <code>sphinx-quickstart</code> command:</p> <pre><code>cd my/project/path\nsphinx-quickstart docs\n</code></pre> <p>Follow the prompts to set up your project. Once completed, you can generate the HTML documentation by running:</p> <pre><code>cd docs\nmake html\n</code></pre> <p>Your documentation will be located at <code>build/html/index.html</code>.</p>"},{"location":"blog/generating-api-documentation-for-your-python-package-with-sphinx-apidoc/#utilizing-docstrings","title":"Utilizing Docstrings","text":""},{"location":"blog/generating-api-documentation-for-your-python-package-with-sphinx-apidoc/#incorporating-directives","title":"Incorporating Directives","text":"<p>Now that we have our basic documentation structure set up, let's incorporate our Python docstrings into the documentation. We can achieve this with Sphinx-apidoc. For example, if our module is located at <code>./statanalysis</code>, we can generate documentation files for it using the following command:</p> <pre><code>sphinx-apidoc ./statanalysis/ -o ./docs/source/statanalysis/ -f -E\n</code></pre> <p>Make sure to update your index file like below to include the newly generated files.</p> <pre><code>...\n\n.. toctree::\n   :maxdepth: 2\n\n   statanalysis/statanalysis\n\n...\n</code></pre> <p>Here, i use <code>statanalysis/statanalysis</code> that will access the file <code>docs/source/statanalysis/statanalysis.rts</code>. This file list the other module files.</p> <p>Then run the command <code>make html</code> in the <code>docs</code> directory to update the build.</p> Troubleshooting Import Errors in Sphinx <p>If you encounter import errors while generating documentation with Sphinx, it's essential to diagnose and resolve the issue promptly. Here are some steps to troubleshoot and address import errors effectively:</p> <ol> <li> <p>Check Module Installation: Ensure that all required Python modules are installed in the environment where Sphinx is running. Use <code>pip list</code> or <code>pip show &lt;module_name&gt;</code> to verify module installation.</p> </li> <li> <p>Activate Virtual Environment: If you're using a virtual environment, activate it before running Sphinx. Use the appropriate activation command (<code>source &lt;venv&gt;/bin/activate</code> for Unix-like systems or <code>&lt;venv&gt;\\Scripts\\activate</code> for Windows).</p> </li> <li> <p>Inspect Sphinx Configuration: Review your Sphinx configuration file (<code>conf.py</code>) to ensure that it specifies the correct Python interpreter and environment. Uncomment and update the <code>sys.path</code> variable to include the path to your project. For example:</p> <pre><code>import sys\nfrom pathlib import Path\nsys.path.insert(0, str(Path(__file__).parent.parent.parent.absolute()))\n</code></pre> <p>This step ensures that Sphinx can locate and import your project modules correctly.</p> </li> <li> <p>Resolve Dependency Conflicts: Check for any conflicting dependencies between modules or packages in your project. Resolve conflicts by updating package versions or installing compatible versions of conflicting packages.</p> </li> <li> <p>Validate Module Paths: Double-check the import paths specified in your Python source files. Ensure that module paths are correctly defined and that all necessary <code>__init__.py</code> files are present in directories.</p> </li> <li> <p>Check <code>__init__.py</code>: Make sure that <code>__init__.py</code> files are present in all module folders within your project. These files are necessary for Python to recognize directories as packages.</p> </li> <li> <p>Update Sphinx Extensions: If you're using Sphinx extensions, ensure that they are up to date. Outdated extensions may cause compatibility issues or import errors. Use <code>pip install --upgrade &lt;extension_name&gt;</code> to update extensions.</p> </li> <li> <p>Rebuild Documentation: After making any changes or resolving issues, rebuild your Sphinx documentation to verify that the import errors have been resolved. Use the <code>make html</code> command to rebuild the documentation.</p> </li> </ol>"},{"location":"blog/generating-api-documentation-for-your-python-package-with-sphinx-apidoc/#enhancing-docstring-readability","title":"Enhancing Docstring Readability","text":"<p>We can improve the readability of our docstrings by using the <code>napoleon</code> extension.  <code>napoleon</code> allows us to write docstrings in a more concise and readable format, such as Google or Numpy style. To enable  <code>napoleon</code>, update your <code>conf.py</code> with the following:</p> <pre><code>...\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n# ones.\nextensions = [\n    \"sphinx.ext.napoleon\",\n]\n\n#  `napoleon` settings\n# from https://github.com/cimarieta/sphinx-autodoc-example\nnapoleon_google_docstring = True\nnapoleon_numpy_docstring = True\nnapoleon_include_init_with_doc = True\nnapoleon_include_private_with_doc = True\nnapoleon_include_special_with_doc = True\nnapoleon_use_admonition_for_examples = False\nnapoleon_use_admonition_for_notes = False\nnapoleon_use_admonition_for_references = False\nnapoleon_use_ivar = False\nnapoleon_use_param = True\nnapoleon_use_rtype = True\n...\n</code></pre> <p>Rerun the <code>make html</code>. Great! They may still be some warnings, but in general, the build will be successful.</p>"},{"location":"blog/generating-api-documentation-for-your-python-package-with-sphinx-apidoc/#additional-extensions-for-enhanced-functionality","title":"Additional Extensions for Enhanced Functionality","text":"<p>Sphinx offers a variety of extensions to enhance the functionality of your documentation. Some useful extensions include:</p> <ul> <li><code>sphinx.ext.autodoc</code>: Automatically generates documentation from docstrings.</li> <li><code>sphinx.ext.autosummary</code>: Generates summaries of modules, classes, and functions based on documentation.</li> <li><code>sphinx.ext.mathjax</code>: Renders mathematical equations in your documentation.</li> <li><code>sphinx.ext.todo</code>: Includes \"todo\" items in your documentation.</li> <li>and more.</li> </ul> <p>You can activate these extensions by adding them to the <code>extensions</code> list in your <code>conf.py</code> like below:</p> <pre><code>extensions = [\n    \"sphinx.ext.autodoc\", \n    \"sphinx.ext.autosummary\",\n    \"sphinx.ext.napoleon\",\n    \"sphinx.ext.mathjax\",\n    \"sphinx.ext.todo\",\n]\n</code></pre> more extensions <p>Here is an example of a list of extensions. You can activate those you find useful:</p> <pre><code>extensions = [\n   # like open3d:0.17\n   \"sphinx.ext.autodoc\", # generate documentation for Python modules, classes, and functions, from docstring\n   \"sphinx.ext.autosummary\", # generates summaries of modules, classes, and functions based on the documentation generated by sphinx.ext.autodoc\n   \"sphinx.ext.napoleon\", # the  `napoleon` style of docstrings for Python modules, classes, and functions, is easy to read and write\n   \"sphinx.ext.mathjax\", # display mathematical equations\n   \"sphinx.ext.todo\", # include \"todo\" items in your documentation\n   # \"nbsphinx\", #include Jupyter Notebooks in your documentation\n   # 'm2r2', #convert Markdown files to reStructuredText (RST) format,\n   # more from https://github.com/cimarieta/sphinx-autodoc-example\n   # \"nbsphinx\", #include Jupyter Notebooks in your documentation\n   # 'm2r2', #convert Markdown files to reStructuredText (RST) format,\n   # more from https://github.com/cimarieta/sphinx-autodoc-example\n   # 'sphinx.ext.doctest', # test code snippets in the documentation\n   # 'sphinx.ext.intersphinx', # link to external documentation\n   # 'sphinx.ext.coverage', # measure the coverage of the documentation\n   # 'sphinx.ext.ifconfig', # include content based on configuration options\n   # 'sphinx.ext.viewcode', # show the source code of modules and functions\n   # 'sphinx.ext.githubpages', # publish documentation on GitHub Pages\n]\n</code></pre>"},{"location":"blog/generating-api-documentation-for-your-python-package-with-sphinx-apidoc/#handy-tips-for-integration","title":"Handy Tips for Integration","text":"Link Integration <p>When adding hyperlinks to your documentation, use reStructuredText syntax to ensure proper rendering. For example:</p> <ul> <li> <p>We may do this in markdown <pre><code>Author: Susan Li source:[LogisticRegressionImplementation.ipynb](https://github.com/aihubprojects/Logistic-Regression-From-Scratch-Python/blob/master/LogisticRegressionImplementation.ipynb)\n</code></pre></p> </li> <li> <p>But in rst, we use</p> </li> </ul> <pre><code>Author: Susan Li source: `LogisticRegressionImplementation.ipynb &lt;https://github.com/aihubprojects/Logistic-Regression-From-Scratch-Python/blob/master/LogisticRegressionImplementation.ipynb&gt;`_\n</code></pre> <p>So, use the backtick (<code>) and angle brackets (&lt;&gt;) around the text you want to display as a hyperlink, along with the correct reStructuredText syntax (</code>link text `_), you can ensure that the text is rendered as a clickable link in the documentation. Utilizing Todo Extension <p>To mark sections as \"todo\" items within your Python docstrings, use the <code>.. todo::</code> directive. For example:</p> <pre><code>def my_function():\n \"\"\"\n This is a function that does something.\n\n .. todo:: Implement error handling.\n \"\"\"\n</code></pre> <p>Make sure to enable the <code>sphinx.ext.todo</code> extension in your <code>conf.py</code> and optionaly, set the <code>todo_include_todos</code> configuration value to <code>True</code> in your <code>conf.py</code> file. This configuration ensures that the <code>todo</code> directives in your docstrings will be included in the output.</p> <p>Generate a List of Todos: To generate a list of all <code>todo</code> directives in your documentation, you can use the <code>.. todolist::</code> directive in your documentation. This will create a list of all <code>todo</code> items throughout your documentation.</p> Adding Sphinx to Development Dependencies <p>Ensure Sphinx is included in your development dependencies. You can achieve this by adding it to your project using Poetry or pip:</p> <pre><code>poetry add sphinx --group buildthedocs\n</code></pre> <p>or</p> <pre><code>pip install sphinx\n</code></pre>"},{"location":"blog/generating-api-documentation-for-your-python-package-with-sphinx-apidoc/#customizing-the-theme","title":"Customizing the Theme","text":"<p>The default theme or even the built in themes (<code>alabaster</code>, <code>classic</code>, <code>sphinxdoc</code>, <code>scrools</code>, ...) provided by Sphinx may not suit your preferences.</p> <p></p> <p>You can customize the theme to better fit your project's style. Some popular themes include:</p> <ul> <li>solar-theme</li> <li>sphinx-rtd-theme</li> <li>furo</li> </ul> <p>Example of online project using these themes</p> <ul> <li><code>sphinx-rtd-theme</code> used for open3d v0.17 documentation</li> <li><code>furo</code> in open3d v0.18\"</li> </ul> <p>Install your chosen theme using pip and update the <code>html_theme</code> in your <code>conf.py</code>.</p> use sphinx-rtd-theme <p>Install in the environment with the method you use (I use Poetry):</p> <pre><code>poetry add sphinx-rtd-theme --group buildthedocs\n</code></pre> <p>Update the <code>html_theme</code> in the <code>conf.py</code> file:</p> <pre><code>...\nhtml_theme = 'sphinx_rtd_theme'\n...\n</code></pre> <p>Generate the build like mentioned before.</p> <p>You may have some warnings related to the syntax; you can fix them.</p> <p>Here was my output of my python package statanalysis with <code>sphinx-docapi</code> + <code>sphinx-rtd-theme</code></p> <p></p>"},{"location":"blog/generating-api-documentation-for-your-python-package-with-sphinx-apidoc/#integrating-readmemd-into-sphinx-documentation","title":"Integrating README.md into Sphinx Documentation","text":"<p>To include your <code>README.md</code> file as an introduction to your Sphinx documentation, you can use the <code>m2r2</code> extension as implicitely mentioned in this stackoverflow answer. Follow these steps to seamlessly integrate your <code>README.md</code> file into your Sphinx documentation:</p> <ol> <li> <p>Install m2r2 Extension: First, install the <code>m2r2</code> extension in your development environment using Poetry or pip:</p> <code>using poetry</code> <code>using pip</code> <pre><code>poetry add m2r2 --group buildthedocs\n</code></pre> <pre><code>pip install m2r2\n</code></pre> </li> <li> <p>Create <code>readme.rst</code> File: Create a <code>readme.rst</code> file in your Sphinx documentation source directory (<code>./docs/source/readme.rst</code>).</p> <code>readme.rst</code> <code>using poetry</code> <p>In your <code>readme.rst</code> file, use the <code>mdinclude</code> directive to include the content of your <code>README.md</code> file:</p> <pre><code>-----------\nIntroduction\n-----------\n\n.. mdinclude:: ../../README.md\n</code></pre> <p>Update your <code>index.rst</code> file to include the <code>readme.rst</code> file in the table of contents (<code>toctree</code> directive):</p> <pre><code>...\n\n.. toctree::\n    :maxdepth: 2\n    :caption: Contents:\n\n    readme  # Include readme.rst here\n    reference\n\n...\n</code></pre> </li> </ol> <p>By following these steps, you can seamlessly integrate your <code>README.md</code> file into your Sphinx documentation, providing an informative introduction to your project.</p>"},{"location":"blog/generating-api-documentation-for-your-python-package-with-sphinx-apidoc/#deploying-documentation-to-readthedocs","title":"Deploying Documentation to ReadTheDocs","text":"<p>Once you've generated your Sphinx documentation, deploying it to ReadTheDocs allows you to host and share your documentation with ease.</p> <p>For example I've deployed the statanalysis api reference or the lissajou api reference</p> <p>To deploy your documentation to ReadTheDocs, you need to prepare your documentation and configuration files, then you can import your project to ReadTheDocs and initiate the build process. Follow this step-by-step guide:</p> <ol> <li> <p>Export Dependencies:</p> <p>Before deploying your documentation, you need to export your project dependencies to ensure that ReadTheDocs builds your documentation with the correct dependencies.</p> <p>Depending on your package manager (e.g., Poetry, pip), export your dependencies to a <code>requirements.txt</code> file.</p> <p>Depending on your package manager, to get the right syntax, refer to this cheat on package managers</p> Example Using Poetry <p><pre><code>poetry export -f requirements.txt --output ./docs/requirements.txt --without-hashes --with buildthedocs\n</code></pre>  This command exports the dependencies specified in your Poetry project, excluding development dependencies, and includes the dependencies grouped under <code>buildthedocs</code>.</p> </li> <li> <p>Create Configuration File: Next, you need to create a <code>.readthedocs.yaml</code> file in the root directory of your project. This file specifies the build configuration for ReadTheDocs.</p> Example of <code>.readthedocs.yaml</code> <pre><code># Required\nversion: 2\n\n# Set the OS, Python version, and other tools you might need\nbuild:\n  os: ubuntu-22.04\n  tools:\n    python: \"3.8\"\n\n# Build documentation in the \"docs/\" directory with Sphinx\nsphinx:\n  configuration: docs/source/conf.py\n\n# Optionally build your docs in additional formats such as PDF and ePub\nformats:\n  - pdf\n  - epub\n\npython:\n  install:\n    - requirements: docs/requirements.txt\n</code></pre> <p>This configuration file specifies the Python version, operating system, and build tools required for building your documentation. Additionally, it defines the location of your Sphinx configuration file and specifies the dependencies needed for the build.</p> </li> <li> <p>Create ReadTheDocs Account: If you haven't already, create an account on the ReadTheDocs website.</p> </li> <li> <p>Import Project: Import your project to ReadTheDocs, linking it to your GitHub repository or other version control system. You can either use the GitHub integration or import your project manually.</p> <p>Manual Import Example</p> <ul> <li> <p>Navigate to the manual import page.</p> <p></p> </li> <li> <p>Enter the URL of your repository (e.g., <code>https://github.com/hermann-web/lissajou</code>) </p> </li> <li> <p>Specify the desired project name. </p> <p>That name will appear in <code>https://&lt;name&gt;.readthedocs.io/en/latest</code></p> </li> <li> <p>Follow the prompts to complete the import process.</p> </li> </ul> </li> <li> <p>Build Documentation: Once your project is imported, initiate the build process on ReadTheDocs. The platform will automatically detect changes in your repository and trigger builds accordingly.</p> </li> <li> <p>Monitor Builds on ReadTheDocs Dashboard: After initiating the build process, you can monitor the progress and status of your documentation builds on the ReadTheDocs dashboard. This allows you to track build successes, view logs, and troubleshoot any errors that may occur during the build process.</p> <p></p> </li> </ol> <p>By following these steps, you can successfully deploy your Sphinx documentation to ReadTheDocs and make it accessible to your audience.</p> recap example <p>This was the steps for one python package i've built</p> <pre><code># 1. add deps\npoetry add sphinx==^7.0.0 --group buildthedocs\npoetry add sphinx-rtd-theme --group buildthedocs\npoetry add m2r2 --group buildthedocs\n# 2. create the docu template\nsphinx-quickstart docs\n# 2. generate buildthedocs requirements, generate api doc and build docu\nsource scripts/run-build-docu.sh\n# 3. add readme.rst\n# 4. update index.rst to integrate readme.rst and openconv/modules.rst in \n# 5. update conf.py to specify main path, add extensions, modify the html_theme\n# 6. rebuild docu to check\nsource scripts/run-build-docu.sh\n# 7. add a .readthedocs file\n# 8. push to readthedocs.org\n</code></pre>"},{"location":"blog/generating-api-documentation-for-your-python-package-with-sphinx-apidoc/#related-posts","title":"Related Posts","text":"<ul> <li>How to publish your python project (to pypi or testPypi) with Poetry then install as dependency in another project ?</li> <li>Cheat on Python Package Managers</li> </ul>"},{"location":"blog/generating-api-documentation-for-your-python-package-with-sphinx-apidoc/#additional-resources","title":"Additional Resources","text":"<p>For more information on Sphinx and its capabilities, refer to the official Sphinx documentation. Additionally, you can explore the available themes and extensions to further enhance your documentation.</p> <ul> <li>Sphinx Documentation - PDF</li> <li>sphinx-autodoc-example - github</li> <li>Sphinx Quickstart</li> <li><code>lissajou</code> Module Documentation</li> <li><code>statanalysis</code> Module Documentation</li> <li>Readthedocs Configuration File Reference</li> </ul>"},{"location":"blog/python-type-checking-tools-mypy-vs-pyright-vs-pydantic-vs-pandera-vs-jaxtyping-vs-check_shapes-vs-typeguard/","title":"Python Type Checking Tools: mypy vs. pyright vs. pydantic vs. pandera vs. jaxtyping vs. check_shapes vs. typeguard","text":""},{"location":"blog/python-type-checking-tools-mypy-vs-pyright-vs-pydantic-vs-pandera-vs-jaxtyping-vs-check_shapes-vs-typeguard/#introduction","title":"Introduction","text":"<p>Are you tired of runtime type errors that could have been caught earlier? Do you work with numerical computing, data science, or ML workflows where shape mismatches cause mysterious bugs?</p> <p>The Python ecosystem offers a rich variety of type checking tools, from traditional static type checkers to modern runtime validation libraries and specialized shape checkers for scientific computing.</p> <p>This comprehensive guide explores the landscape of Python type checking tools, helping you choose the right combination for your specific needs.</p> <p>Whether you're building web applications, data pipelines, machine learning models, or scientific computing applications, understanding the strengths and use cases of different type checking approaches will help you write more robust, maintainable code. We'll cover static type checkers like mypy and pyright, runtime validation libraries like pydantic and typeguard, data validation tools like pandera, and specialized shape checkers like jaxtyping and check_shapes.</p>"},{"location":"blog/python-type-checking-tools-mypy-vs-pyright-vs-pydantic-vs-pandera-vs-jaxtyping-vs-check_shapes-vs-typeguard/#overview","title":"Overview","text":"<p>Python type checking tools fall into several categories, each addressing different aspects of type safety and validation:</p> <ul> <li>Static Type Checkers: Analyze code without running it (mypy, pyright)</li> <li>Runtime Type Checkers: Validate types during execution (typeguard, beartype)</li> <li>Data Validation: Validate and parse data structures (pydantic, pandera)</li> <li>Shape Checkers: Validate array shapes and dtypes (jaxtyping, check_shapes)</li> </ul>"},{"location":"blog/python-type-checking-tools-mypy-vs-pyright-vs-pydantic-vs-pandera-vs-jaxtyping-vs-check_shapes-vs-typeguard/#key-considerations","title":"Key Considerations","text":""},{"location":"blog/python-type-checking-tools-mypy-vs-pyright-vs-pydantic-vs-pandera-vs-jaxtyping-vs-check_shapes-vs-typeguard/#choosing-the-right-approach","title":"Choosing the Right Approach","text":"<ul> <li>Static vs Runtime: Static checking catches errors before deployment, while runtime checking provides guarantees during execution</li> <li>Performance Impact: Runtime checking adds overhead, static checking has no runtime cost</li> <li>Coverage: Static checking might miss dynamic code patterns, runtime checking validates actual execution</li> <li>Integration: Consider how tools integrate with your existing workflow and dependencies</li> <li>Domain-Specific Needs: Scientific computing, web development, and data processing have different requirements</li> </ul>"},{"location":"blog/python-type-checking-tools-mypy-vs-pyright-vs-pydantic-vs-pandera-vs-jaxtyping-vs-check_shapes-vs-typeguard/#tools-overview","title":"Tools Overview","text":"<code>mypy</code> <code>pyright</code> <code>typeguard</code> <code>pydantic</code> <code>pandera</code> <code>jaxtyping</code> <code>check_shapes</code> <ul> <li>Static Type Checker: The original static type checker for Python, providing comprehensive type analysis</li> <li>Gradual Typing: Allows incremental adoption of type hints in existing codebases</li> <li>Extensive Plugin System: Supports plugins for frameworks like Django, SQLAlchemy, and more</li> <li>Configuration: Highly configurable through <code>mypy.ini</code> or <code>pyproject.toml</code></li> <li>Community: Large ecosystem with extensive documentation and community support</li> </ul> <ul> <li>Fast Static Type Checker: Microsoft's static type checker with TypeScript-style type inference</li> <li>Advanced Type System: Supports complex type constructs and provides excellent type inference</li> <li>IDE Integration: Powers the Python extension for VS Code</li> <li>Performance: Exceptionally fast type checking, suitable for large codebases</li> <li>Configuration: Configurable through <code>pyproject.toml</code> or <code>pyrightconfig.json</code></li> </ul> <ul> <li>Runtime Type Checker: Provides runtime type validation for Python functions</li> <li>Decorator-Based: Uses decorators to add type checking to functions</li> <li>Type Annotation Support: Works with standard Python type annotations</li> <li>Integration: Easy to integrate into existing codebases incrementally</li> <li>Performance: Moderate runtime overhead for comprehensive type validation</li> </ul> <ul> <li>Data Validation: Comprehensive data validation and parsing library</li> <li>Automatic Parsing: Automatically converts and validates input data</li> <li>JSON Schema: Generates JSON schemas from models</li> <li>Integration: Widely used in web frameworks like FastAPI</li> <li>Performance: Optimized for data validation and parsing tasks</li> </ul> <ul> <li>DataFrame Validation: Specialized for validating pandas DataFrames and Series</li> <li>Schema-Based: Uses schema definitions to validate data structures</li> <li>Statistical Validation: Supports statistical checks and data quality validation</li> <li>Integration: Seamlessly integrates with pandas workflows</li> <li>Reporting: Provides detailed validation reports and error messages</li> </ul> <ul> <li>Shape and Type Checker: Provides both static and runtime shape/dtype checking for numerical computing</li> <li>ML-Focused: Specifically designed for JAX, NumPy, and PyTorch workflows</li> <li>Python-Native Syntax: Uses Python-native type hints with shape specifications</li> <li>Static + Runtime: Supports both static checking (with mypy/pyright) and runtime checking (with beartype)</li> <li>Status: Rapidly evolving, not yet production-ready but promising</li> </ul> <ul> <li>Lightweight Shape Checker: Provides runtime shape checking for numerical arrays</li> <li>Decorator-Based: Uses decorators with string specifications for shape validation</li> <li>Backend Agnostic: Works with any object that has a <code>.shape</code> attribute</li> <li>Low Overhead: Minimal performance impact and easy integration</li> <li>Debugging Focus: Primarily designed for debugging and safety in numerical computing</li> </ul>"},{"location":"blog/python-type-checking-tools-mypy-vs-pyright-vs-pydantic-vs-pandera-vs-jaxtyping-vs-check_shapes-vs-typeguard/#comprehensive-comparison-table","title":"Comprehensive Comparison Table","text":"Feature / Tool <code>mypy</code> <code>pyright</code> <code>typeguard</code> <code>pydantic</code> <code>pandera</code> <code>jaxtyping</code> <code>check_shapes</code> Primary Purpose Static type checking Static type checking Runtime type checking Data validation &amp; parsing DataFrame validation Static + runtime shape checking Runtime shape checking Type of Checking Static Static Runtime Runtime Runtime Static + Runtime Runtime Performance Impact None (static) None (static) Medium Low-Medium Low-Medium Medium (with beartype) Low Shape Validation Limited Limited No No Yes (DataFrame schemas) Yes (full support) Yes (arrays only) Data Validation No No Basic type validation Comprehensive DataFrame-focused No No Configuration <code>mypy.ini</code>, <code>pyproject.toml</code> <code>pyproject.toml</code>, <code>pyrightconfig.json</code> Minimal Model-based Schema-based Type hints Decorator parameters Integration Effort Medium Low-Medium Low Low Low (for pandas) Medium Very Low Learning Curve Medium Medium Low Low-Medium Low-Medium Medium Very Low IDE Support Excellent Excellent (VS Code) Limited Good Good Growing Limited Ecosystem Large, mature Growing rapidly Small but stable Large, widely adopted Growing Early stage Small, specialized Best For General-purpose static checking Fast static checking, large codebases Runtime validation in tests API validation, web development Data science, pandas workflows ML/scientific computing Debugging array shapes"},{"location":"blog/python-type-checking-tools-mypy-vs-pyright-vs-pydantic-vs-pandera-vs-jaxtyping-vs-check_shapes-vs-typeguard/#installation-and-basic-usage","title":"Installation and Basic Usage","text":"Tool Installation Basic Usage <code>mypy</code> <code>pip install mypy</code> <code>mypy your_file.py</code> <code>pyright</code> <code>pip install pyright</code> <code>pyright your_file.py</code> <code>typeguard</code> <code>pip install typeguard</code> <code>@typechecked</code> decorator <code>pydantic</code> <code>pip install pydantic</code> Create models with <code>BaseModel</code> <code>pandera</code> <code>pip install pandera</code> Define schemas with <code>DataFrameSchema</code> <code>jaxtyping</code> <code>pip install jaxtyping beartype</code> Use shape annotations with <code>@beartype</code> <code>check_shapes</code> <code>pip install check_shapes</code> <code>@check_shapes</code> decorator"},{"location":"blog/python-type-checking-tools-mypy-vs-pyright-vs-pydantic-vs-pandera-vs-jaxtyping-vs-check_shapes-vs-typeguard/#practical-examples","title":"Practical Examples","text":""},{"location":"blog/python-type-checking-tools-mypy-vs-pyright-vs-pydantic-vs-pandera-vs-jaxtyping-vs-check_shapes-vs-typeguard/#static-type-checking-with-mypy-and-pyright","title":"Static Type Checking with mypy and pyright","text":"<pre><code># example.py\nfrom typing import List, Optional\n\ndef process_data(items: List[int], threshold: Optional[int] = None) -&gt; List[int]:\n    if threshold is None:\n        threshold = 0\n    return [item for item in items if item &gt; threshold]\n\n# Run: mypy example.py\n# Run: pyright example.py\n</code></pre>"},{"location":"blog/python-type-checking-tools-mypy-vs-pyright-vs-pydantic-vs-pandera-vs-jaxtyping-vs-check_shapes-vs-typeguard/#runtime-type-checking-with-typeguard","title":"Runtime Type Checking with typeguard","text":"<pre><code>from typeguard import typechecked\nfrom typing import List\n\n@typechecked\ndef calculate_average(numbers: List[float]) -&gt; float:\n    return sum(numbers) / len(numbers)\n\n# This will raise a TypeError at runtime if called with wrong types\nresult = calculate_average([1.0, 2.0, 3.0])  # OK\nresult = calculate_average([1, 2, 3])         # TypeError\n</code></pre>"},{"location":"blog/python-type-checking-tools-mypy-vs-pyright-vs-pydantic-vs-pandera-vs-jaxtyping-vs-check_shapes-vs-typeguard/#data-validation-with-pydantic","title":"Data Validation with pydantic","text":"<pre><code>from pydantic import BaseModel, validator\nfrom typing import List\n\nclass User(BaseModel):\n    name: str\n    age: int\n    email: str\n    tags: List[str] = []\n\n    @validator('age')\n    def validate_age(cls, v):\n        if v &lt; 0:\n            raise ValueError('Age must be positive')\n        return v\n\n# Automatic validation and parsing\nuser = User(name=\"John\", age=30, email=\"john@example.com\")\n</code></pre>"},{"location":"blog/python-type-checking-tools-mypy-vs-pyright-vs-pydantic-vs-pandera-vs-jaxtyping-vs-check_shapes-vs-typeguard/#dataframe-validation-with-pandera","title":"DataFrame Validation with pandera","text":"<pre><code>import pandas as pd\nimport pandera as pa\nfrom pandera import Column, DataFrameSchema, Check\n\nschema = DataFrameSchema({\n    \"name\": Column(str),\n    \"age\": Column(int, Check.greater_than(0)),\n    \"salary\": Column(float, Check.greater_than(0))\n})\n\n@pa.check_types\ndef process_employees(df: pa.DataFrame[schema]) -&gt; pa.DataFrame[schema]:\n    return df[df['age'] &gt; 18]\n\n# This will validate the DataFrame structure and data types\ndf = pd.DataFrame({\n    \"name\": [\"Alice\", \"Bob\"],\n    \"age\": [25, 30],\n    \"salary\": [50000.0, 60000.0]\n})\n</code></pre>"},{"location":"blog/python-type-checking-tools-mypy-vs-pyright-vs-pydantic-vs-pandera-vs-jaxtyping-vs-check_shapes-vs-typeguard/#shape-checking-with-jaxtyping","title":"Shape Checking with jaxtyping","text":"<pre><code>from jaxtyping import Float, Integer\nfrom beartype import beartype\nimport jax.numpy as jnp\n\n@beartype\ndef matrix_multiply(\n    a: Float[jnp.ndarray, \"batch dim_in\"],\n    b: Float[jnp.ndarray, \"dim_in dim_out\"]\n) -&gt; Float[jnp.ndarray, \"batch dim_out\"]:\n    return a @ b\n\n# This will check shapes at runtime\na = jnp.array([[1.0, 2.0], [3.0, 4.0]])  # Shape: (2, 2)\nb = jnp.array([[1.0], [2.0]])             # Shape: (2, 1)\nresult = matrix_multiply(a, b)            # Shape: (2, 1)\n</code></pre>"},{"location":"blog/python-type-checking-tools-mypy-vs-pyright-vs-pydantic-vs-pandera-vs-jaxtyping-vs-check_shapes-vs-typeguard/#lightweight-shape-checking-with-check_shapes","title":"Lightweight Shape Checking with check_shapes","text":"<pre><code>from check_shapes import check_shapes\nimport numpy as np\n\n@check_shapes(\n    \"features: [batch, n_features]\",\n    \"weights: [n_features, n_outputs]\",\n    \"return: [batch, n_outputs]\"\n)\ndef linear_layer(features, weights):\n    return features @ weights\n\n# This will validate shapes at runtime\nfeatures = np.random.randn(32, 128)  # batch=32, n_features=128\nweights = np.random.randn(128, 10)   # n_features=128, n_outputs=10\noutput = linear_layer(features, weights)  # batch=32, n_outputs=10\n</code></pre>"},{"location":"blog/python-type-checking-tools-mypy-vs-pyright-vs-pydantic-vs-pandera-vs-jaxtyping-vs-check_shapes-vs-typeguard/#when-to-use-what-decision-matrix","title":"When to Use What: Decision Matrix","text":""},{"location":"blog/python-type-checking-tools-mypy-vs-pyright-vs-pydantic-vs-pandera-vs-jaxtyping-vs-check_shapes-vs-typeguard/#choose-based-on-your-project-type","title":"Choose Based on Your Project Type","text":"Project Type Recommended Tools Web APIs and Services <code>pydantic</code> + <code>mypy</code>/<code>pyright</code> Data Science and Analytics <code>pandera</code> + <code>mypy</code>/<code>pyright</code> Machine Learning and Scientific Computing <code>jaxtyping</code> + <code>beartype</code> + <code>mypy</code>/<code>pyright</code> General Python Applications <code>mypy</code>/<code>pyright</code> + <code>typeguard</code> (for tests) Legacy Codebases Start with <code>mypy</code>/<code>pyright</code>, add others gradually High-Performance Computing <code>check_shapes</code> + <code>mypy</code>/<code>pyright</code>"},{"location":"blog/python-type-checking-tools-mypy-vs-pyright-vs-pydantic-vs-pandera-vs-jaxtyping-vs-check_shapes-vs-typeguard/#choose-based-on-your-needs","title":"Choose Based on Your Needs","text":"You want... Use \u2705 Catch type errors before deployment <code>mypy</code> or <code>pyright</code> \u2705 Fast static type checking <code>pyright</code> \u2705 Comprehensive static analysis <code>mypy</code> with plugins \u2705 Runtime type validation <code>typeguard</code> or <code>beartype</code> \u2705 Data validation and parsing <code>pydantic</code> \u2705 DataFrame validation <code>pandera</code> \u2705 Shape and dtype checking for ML <code>jaxtyping</code> + <code>beartype</code> \u2705 Lightweight shape validation <code>check_shapes</code> \u2705 Gradual typing adoption <code>mypy</code> with <code>--ignore-missing-imports</code>"},{"location":"blog/python-type-checking-tools-mypy-vs-pyright-vs-pydantic-vs-pandera-vs-jaxtyping-vs-check_shapes-vs-typeguard/#configuration-examples","title":"Configuration Examples","text":""},{"location":"blog/python-type-checking-tools-mypy-vs-pyright-vs-pydantic-vs-pandera-vs-jaxtyping-vs-check_shapes-vs-typeguard/#pyprojecttoml-configuration","title":"pyproject.toml Configuration","text":"<pre><code>[tool.mypy]\npython_version = \"3.9\"\nwarn_return_any = true\nwarn_unused_configs = true\ndisallow_untyped_defs = true\nno_implicit_optional = true\n\n[tool.pyright]\ninclude = [\"src\"]\nexclude = [\"**/node_modules\", \"**/__pycache__\"]\nvenv = \"venv\"\nreportMissingImports = true\nreportMissingTypeStubs = false\npythonVersion = \"3.9\"\npythonPlatform = \"Linux\"\n\n[tool.pydantic]\n# Pydantic v2 configuration\nvalidate_assignment = true\nstr_strip_whitespace = true\n</code></pre>"},{"location":"blog/python-type-checking-tools-mypy-vs-pyright-vs-pydantic-vs-pandera-vs-jaxtyping-vs-check_shapes-vs-typeguard/#cicd-integration","title":"CI/CD Integration","text":"<pre><code># .github/workflows/type-check.yml\nname: Type Checking\n\non: [push, pull_request]\n\njobs:\n  type-check:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v2\n    - name: Set up Python\n      uses: actions/setup-python@v2\n      with:\n        python-version: 3.9\n\n    - name: Install dependencies\n      run: |\n        pip install mypy pyright pydantic pandera jaxtyping beartype check_shapes\n\n    - name: Static type checking\n      run: |\n        mypy src/\n        pyright src/\n\n    - name: Run tests with runtime type checking\n      run: |\n        python -m pytest tests/ --typeguard-packages=mypackage\n</code></pre>"},{"location":"blog/python-type-checking-tools-mypy-vs-pyright-vs-pydantic-vs-pandera-vs-jaxtyping-vs-check_shapes-vs-typeguard/#advanced-usage-patterns","title":"Advanced Usage Patterns","text":""},{"location":"blog/python-type-checking-tools-mypy-vs-pyright-vs-pydantic-vs-pandera-vs-jaxtyping-vs-check_shapes-vs-typeguard/#combining-multiple-tools","title":"Combining Multiple Tools","text":"<pre><code># advanced_example.py\nfrom typing import List, Optional\nfrom pydantic import BaseModel, validator\nfrom jaxtyping import Float\nfrom beartype import beartype\nimport jax.numpy as jnp\nimport pandera as pa\n\n# Data validation with pydantic\nclass TrainingConfig(BaseModel):\n    batch_size: int\n    learning_rate: float\n    epochs: int\n\n    @validator('batch_size')\n    def validate_batch_size(cls, v):\n        if v &lt;= 0:\n            raise ValueError('Batch size must be positive')\n        return v\n\n# Shape validation with jaxtyping\n@beartype\ndef train_model(\n    features: Float[jnp.ndarray, \"batch features\"],\n    labels: Float[jnp.ndarray, \"batch\"],\n    config: TrainingConfig\n) -&gt; Float[jnp.ndarray, \"features\"]:\n    # Training logic here\n    return jnp.ones(features.shape[1])\n\n# DataFrame validation with pandera\nschema = pa.DataFrameSchema({\n    \"feature_1\": pa.Column(float),\n    \"feature_2\": pa.Column(float),\n    \"label\": pa.Column(float)\n})\n\n@pa.check_types\ndef preprocess_data(df: pa.DataFrame[schema]) -&gt; pa.DataFrame[schema]:\n    return df.dropna()\n</code></pre>"},{"location":"blog/python-type-checking-tools-mypy-vs-pyright-vs-pydantic-vs-pandera-vs-jaxtyping-vs-check_shapes-vs-typeguard/#best-practices","title":"Best Practices","text":""},{"location":"blog/python-type-checking-tools-mypy-vs-pyright-vs-pydantic-vs-pandera-vs-jaxtyping-vs-check_shapes-vs-typeguard/#1-start-with-static-type-checking","title":"1. Start with Static Type Checking","text":"<p>Begin with <code>mypy</code> or <code>pyright</code> for static type checking as it provides the most value with minimal runtime overhead.</p>"},{"location":"blog/python-type-checking-tools-mypy-vs-pyright-vs-pydantic-vs-pandera-vs-jaxtyping-vs-check_shapes-vs-typeguard/#2-use-runtime-checking-strategically","title":"2. Use Runtime Checking Strategically","text":"<p>Apply runtime type checking (<code>typeguard</code>, <code>beartype</code>) primarily in tests and critical code paths.</p>"},{"location":"blog/python-type-checking-tools-mypy-vs-pyright-vs-pydantic-vs-pandera-vs-jaxtyping-vs-check_shapes-vs-typeguard/#3-choose-domain-specific-tools","title":"3. Choose Domain-Specific Tools","text":"<p>Use specialized tools for your domain:</p> <ul> <li>Web APIs: <code>pydantic</code></li> <li>Data science: <code>pandera</code></li> <li>ML/Scientific computing: <code>jaxtyping</code> + <code>beartype</code></li> </ul>"},{"location":"blog/python-type-checking-tools-mypy-vs-pyright-vs-pydantic-vs-pandera-vs-jaxtyping-vs-check_shapes-vs-typeguard/#4-gradual-adoption","title":"4. Gradual Adoption","text":"<p>Implement type checking gradually:</p> <ol> <li>Start with static type checking</li> <li>Add type hints incrementally</li> <li>Introduce runtime checking in tests</li> <li>Add specialized validation as needed</li> </ol>"},{"location":"blog/python-type-checking-tools-mypy-vs-pyright-vs-pydantic-vs-pandera-vs-jaxtyping-vs-check_shapes-vs-typeguard/#5-configuration-management","title":"5. Configuration Management","text":"<p>Maintain consistent configuration across your project using <code>pyproject.toml</code> for all tools.</p>"},{"location":"blog/python-type-checking-tools-mypy-vs-pyright-vs-pydantic-vs-pandera-vs-jaxtyping-vs-check_shapes-vs-typeguard/#common-pitfalls-and-solutions","title":"Common Pitfalls and Solutions","text":""},{"location":"blog/python-type-checking-tools-mypy-vs-pyright-vs-pydantic-vs-pandera-vs-jaxtyping-vs-check_shapes-vs-typeguard/#1-performance-impact","title":"1. Performance Impact","text":"<p>Problem: Runtime type checking slows down code Solution: Use runtime checking only in development and testing, not in production</p>"},{"location":"blog/python-type-checking-tools-mypy-vs-pyright-vs-pydantic-vs-pandera-vs-jaxtyping-vs-check_shapes-vs-typeguard/#2-type-hint-complexity","title":"2. Type Hint Complexity","text":"<p>Problem: Complex type hints become hard to maintain Solution: Use type aliases and gradually introduce complexity</p>"},{"location":"blog/python-type-checking-tools-mypy-vs-pyright-vs-pydantic-vs-pandera-vs-jaxtyping-vs-check_shapes-vs-typeguard/#3-tool-conflicts","title":"3. Tool Conflicts","text":"<p>Problem: Different tools have conflicting requirements Solution: Use compatible tool combinations and maintain consistent configuration</p>"},{"location":"blog/python-type-checking-tools-mypy-vs-pyright-vs-pydantic-vs-pandera-vs-jaxtyping-vs-check_shapes-vs-typeguard/#4-learning-curve","title":"4. Learning Curve","text":"<p>Problem: Too many tools to learn at once Solution: Start with one tool (mypy/pyright) and add others gradually</p>"},{"location":"blog/python-type-checking-tools-mypy-vs-pyright-vs-pydantic-vs-pandera-vs-jaxtyping-vs-check_shapes-vs-typeguard/#conclusion","title":"Conclusion","text":"<p>The Python type checking ecosystem offers powerful tools for different aspects of type safety and validation. By understanding the strengths and use cases of each tool, you can build a robust type checking strategy that fits your project's needs.</p> <p>Key takeaways:</p> <ul> <li>Use static type checkers (<code>mypy</code>/<code>pyright</code>) as your foundation</li> <li>Add runtime validation strategically with tools like <code>typeguard</code> and <code>pydantic</code></li> <li>Choose specialized tools for your domain (ML, data science, web development)</li> <li>Adopt tools gradually and maintain consistent configuration</li> <li>Consider performance implications when using runtime checking</li> </ul> <p>The combination of these tools can significantly improve code quality, catch bugs early, and make your Python codebase more maintainable and robust.</p>"},{"location":"blog/python-type-checking-tools-mypy-vs-pyright-vs-pydantic-vs-pandera-vs-jaxtyping-vs-check_shapes-vs-typeguard/#related-posts","title":"Related Posts","text":"<ul> <li>Explore Python Code Formatters and Linters: black vs. flake8 vs. isort vs. autopep8 vs. yapf vs. pylint vs. ruff and more</li> </ul>"},{"location":"blog/python-type-checking-tools-mypy-vs-pyright-vs-pydantic-vs-pandera-vs-jaxtyping-vs-check_shapes-vs-typeguard/#relevant-sources","title":"Relevant Sources","text":"<ul> <li>Python Type Checking Documentation</li> <li>mypy Documentation</li> <li>pyright Documentation</li> <li>pydantic Documentation</li> <li>pandera Documentation</li> <li>jaxtyping Documentation</li> <li>beartype Documentation</li> <li>Type Hints Cheat Sheet</li> </ul>"},{"location":"blog/removing-directories-in-python/","title":"Removing Directories in Python","text":""},{"location":"blog/removing-directories-in-python/#remove-empty-directories-in-python","title":"Remove Empty Directories in Python","text":"<p>To remove an empty directory, you can use <code>os.rmdir()</code> with os and <code>Path.rmdir()</code> with pathlib.</p> <code>index.py</code> <code>index.py</code> <pre><code>import os\n\ndirectory_path = '/path/to/empty_directory'\n\ntry:\n    os.rmdir(directory_path)\n    print(f\"The directory '{directory_path}' has been successfully removed.\")\nexcept OSError as e:\n    print(f\"Error: {directory_path} : {e.strerror}\")\n</code></pre> <pre><code>from pathlib import Path\n\ndirectory_path = Path('/path/to/empty_directory')\n\ntry:\n    directory_path.rmdir()\n    print(f\"The directory '{directory_path}' has been successfully removed.\")\nexcept OSError as e:\n    print(f\"Error: {directory_path} : {e.strerror}\")\n</code></pre>"},{"location":"blog/removing-directories-in-python/#remove-non-empty-directories-in-python","title":"Remove Non-Empty Directories in Python","text":"<p>For directories that contain files or other directories, use <code>shutil.rmtree()</code> to remove them along with their contents.</p> <code>index.py</code> <code>index.py</code> <pre><code>import shutil\n\ndirectory_path = '/path/to/non_empty_directory'\n\ntry:\n    shutil.rmtree(directory_path)\n    print(f\"The directory '{directory_path}' and its contents have been successfully removed.\")\nexcept OSError as e:\n    print(f\"Error: {directory_path} : {e.strerror}\")\n</code></pre> <pre><code>from pathlib import Path\nimport shutil\n\ndirectory_path = Path('/path/to/non_empty_directory')\n\ntry:\n    shutil.rmtree(directory_path)\n    print(f\"The directory '{directory_path}' and its contents have been successfully removed.\")\nexcept OSError as e:\n    print(f\"Error: {directory_path} : {e.strerror}\")\n</code></pre> <p>Ensure to confirm the directory paths before removal operations to prevent accidental deletion of important data. When using shutil.rmtree(), exercise caution as it permanently deletes directories and their contents. The choice between os and pathlib can depend on your preference for object-oriented or procedural-style programming when handling paths and directories in Python.</p>"},{"location":"blog/removing-directories-in-python/#relating-links","title":"relating links","text":"<ul> <li>Pathlib Tutorial: Transitioning to Simplified File and Directory Handling in Python</li> </ul>"},{"location":"blog/logging-for-deployment-in-python-a-practical-guide-to-effective-debugging-and-monitoring/","title":"Logging for Deployment in Python: A Practical Guide to Effective Debugging and Monitoring","text":""},{"location":"blog/logging-for-deployment-in-python-a-practical-guide-to-effective-debugging-and-monitoring/#introduction","title":"Introduction","text":"<p>Are you still using print() statements for debugging in Python? Upgrade your logging game with Python's built-in logging module or the Loguru library!</p> <p>If you're tired of scattered print statements cluttering your codebase, it's time to embrace the power of logging in Python. Whether you're a beginner or an experienced developer, mastering logging techniques is essential for effective debugging, monitoring, and troubleshooting of your Python applications.</p> <p>Logging in Python allows you to set different levels of logging, such as DEBUG, INFO, WARNING, ERROR, and CRITICAL. With these levels, you can control the verbosity of log messages and focus on the information relevant to your current task.</p> <p>In this comprehensive guide, we'll explore both the built-in logging module and the Loguru library, offering practical examples and best practices for seamless integration into your projects. Say goodbye to ad-hoc debugging and hello to structured and manageable logs!</p>"},{"location":"blog/logging-for-deployment-in-python-a-practical-guide-to-effective-debugging-and-monitoring/#advantages-of-logging-in-python","title":"Advantages of Logging in Python","text":"<p>Logging in Python offers several advantages over ad-hoc debugging using print statements. Here are some key benefits:</p>"},{"location":"blog/logging-for-deployment-in-python-a-practical-guide-to-effective-debugging-and-monitoring/#structured-output","title":"Structured Output","text":"<p>Unlike print statements, which often result in unstructured output scattered throughout the codebase, logging allows developers to generate structured logs with predefined formats. This structured output makes it easier to parse and analyze log data, leading to improved debugging and troubleshooting.</p>"},{"location":"blog/logging-for-deployment-in-python-a-practical-guide-to-effective-debugging-and-monitoring/#granular-control","title":"Granular Control","text":"<p>With logging, developers can control the verbosity of log messages by setting different logging levels (e.g., DEBUG, INFO, WARNING, ERROR, CRITICAL). This granular control enables developers to filter log messages based on their severity, allowing them to focus on relevant information and ignore less critical messages.</p>"},{"location":"blog/logging-for-deployment-in-python-a-practical-guide-to-effective-debugging-and-monitoring/#modularization","title":"Modularization","text":"<p>Logging encourages modularization of code by promoting the separation of concerns. By logging messages within specific modules or components, developers can track the flow of execution and identify potential issues more effectively. Additionally, modular logging facilitates collaboration among team members by providing insight into each component's behavior.</p>"},{"location":"blog/logging-for-deployment-in-python-a-practical-guide-to-effective-debugging-and-monitoring/#runtime-configuration","title":"Runtime Configuration","text":"<p>Logging in Python allows for runtime configuration, meaning developers can adjust logging settings without modifying the source code. This flexibility is particularly useful in scenarios where different logging configurations are required for development, testing, and production environments.</p>"},{"location":"blog/logging-for-deployment-in-python-a-practical-guide-to-effective-debugging-and-monitoring/#performance-monitoring","title":"Performance Monitoring","text":"<p>Logging is essential for performance monitoring and profiling of Python applications. By logging performance-related metrics such as execution times, memory usage, and resource consumption, developers can identify bottlenecks and optimize the performance of their applications.</p>"},{"location":"blog/logging-for-deployment-in-python-a-practical-guide-to-effective-debugging-and-monitoring/#error-handling","title":"Error Handling","text":"<p>Effective error handling is crucial for robust Python applications. Logging provides a centralized mechanism for capturing and reporting errors, allowing developers to track the occurrence of exceptions and trace their origins. This helps in identifying and addressing potential issues before they impact the application's functionality.</p>"},{"location":"blog/logging-for-deployment-in-python-a-practical-guide-to-effective-debugging-and-monitoring/#integration-with-monitoring-tools","title":"Integration with Monitoring Tools","text":"<p>Logging seamlessly integrates with monitoring and alerting tools, enabling developers to monitor the health and performance of their applications in real-time. By integrating logging with tools like Prometheus, Grafana, or ELK Stack, developers can gain valuable insights into their application's behavior and take proactive measures to maintain its reliability.</p> <p>In the following sections, we'll delve deeper into the features and usage of both the built-in logging module and the Loguru library, showcasing practical examples and best practices for effective logging in Python.</p>"},{"location":"blog/logging-for-deployment-in-python-a-practical-guide-to-effective-debugging-and-monitoring/#built-in-logging","title":"Built-in Logging","text":""},{"location":"blog/logging-for-deployment-in-python-a-practical-guide-to-effective-debugging-and-monitoring/#basic-setup","title":"Basic Setup","text":"<p>Basic Setup with Built-in Logging</p> <pre><code>import logging\n\n# Creating a logger\nlogger = logging.getLogger()\n\n# Setting up a namespace (e.g., grpc)\nlogging.getLogger(\"grpc\")\n</code></pre>"},{"location":"blog/logging-for-deployment-in-python-a-practical-guide-to-effective-debugging-and-monitoring/#using-handlers","title":"Using Handlers","text":"<p>Handlers in logging are responsible for taking the log records (created by loggers) and outputting them to the desired destination, such as the console, files, or network sockets. Using handlers allows developers to control where log messages are sent and how they are formatted. For example, a console handler might be used for debugging during development, while a file handler could be used to store logs for later analysis.</p> <p>Using Handlers with Built-in Logging</p> <pre><code># Adding a console handler\nconsole_handler = logging.StreamHandler()\nformatter = logging.Formatter(fmt=\"%(asctime)s: %(levelname)-8s %(message)s\")\nconsole_handler.setFormatter(formatter)\n\n# Clearing any existing handlers and adding the console handler\nlogger.handlers = []\nlogger.addHandler(console_handler)\n\n# Setting the logging level\nlogger.setLevel(logging.WARNING)  # Adjust the level as needed\n</code></pre>"},{"location":"blog/logging-for-deployment-in-python-a-practical-guide-to-effective-debugging-and-monitoring/#logging-levels","title":"Logging Levels","text":"<p>Logging levels provide a way to categorize log messages based on their severity. By setting the logging level, developers can control which messages are emitted by the logger. This is essential for managing the volume of log output and focusing on relevant information. For instance, during development, setting the level to DEBUG allows developers to see detailed debugging information, while in production, it might be set to WARNING or higher to only capture critical issues.</p> <p>Logging Levels with Built-in Logging</p> <pre><code>logger.debug(\"Debug message: Detailed diagnostic output\")\nlogger.info(\"Info message: General system information\")\nlogger.warning(\"Warning message: Something to take note of but not critical\")\nlogger.error(\"Error message: A major problem that needs attention\")\nlogger.critical(\"Critical message: A severe error indicating a major failure in the application\")\n</code></pre> <p>logging example using formatted string</p> <pre><code>category = \"module\"\nname = \"example_function\"\nparameters = {\"arg1\": \"value1\", \"arg2\": \"value2\"}\nlogger.info(\"%s %s: %r\", category, name, parameters)\n</code></pre>"},{"location":"blog/logging-for-deployment-in-python-a-practical-guide-to-effective-debugging-and-monitoring/#suppressing-logging","title":"Suppressing Logging","text":"<p>Sometimes during testing or specific scenarios, it's beneficial to suppress logging within certain namespaces or contexts to keep the output clean and focused on relevant information. This is especially useful when running automated tests or building processes where excessive logging could clutter the output or interfere with the test results.</p> <p>To achieve this, use the <code>suppress_logging</code> context manager, which temporarily disables logging</p> <p>within the specified namespace.</p> <p>Suppressing Logging with Built-in Logging</p> <pre><code>from contextlib import contextmanager\n\n@contextmanager\ndef suppress_logging(namespace):\n    \"\"\"\n    Suppress logging within a specific namespace to keep output clean during testing or build processes.\n    \"\"\"\n    logger = logging.getLogger(namespace)\n    old_value = logger.disabled\n    logger.disabled = True\n    try:\n        yield\n    finally:\n        logger.disabled = old_value\n\n# Usage example:\nwith suppress_logging(\"your_namespace\"):\n    # Your code using logging goes here\n    pass\n</code></pre>"},{"location":"blog/logging-for-deployment-in-python-a-practical-guide-to-effective-debugging-and-monitoring/#loguru","title":"Loguru","text":"<p>Loguru is a powerful logging library for Python that simplifies logging configuration and provides advanced features such as automatic inclusion of file, function, and line information in log messages. It offers a more intuitive and flexible logging experience compared to the built-in logging module, making it a popular choice among developers.</p> <p></p>"},{"location":"blog/logging-for-deployment-in-python-a-practical-guide-to-effective-debugging-and-monitoring/#installation","title":"Installation","text":"<pre><code>pip install loguru\n</code></pre>"},{"location":"blog/logging-for-deployment-in-python-a-practical-guide-to-effective-debugging-and-monitoring/#usage","title":"Usage","text":"<p>Usage with Loguru</p> <pre><code>from loguru import logger\n\n# Logging messages with Loguru\nlogger.debug(\"A debug message\")\nlogger.info(\"An info message\")\nlogger.warning(\"A warning message\")\nlogger.error(\"An error message\")\nlogger.critical(\"A critical message\")\n\n# Loguru output automatically includes by default module name, function name as well as line information\n</code></pre>"},{"location":"blog/logging-for-deployment-in-python-a-practical-guide-to-effective-debugging-and-monitoring/#conclusion","title":"Conclusion","text":"<p>Logging plays a crucial role in Python development by providing a structured and manageable approach to debugging, monitoring, and troubleshooting applications. Whether using the built-in logging module or the Loguru library, developers can leverage logging to improve code quality, streamline development workflows, and enhance application reliability.</p> <p>By mastering logging techniques and best practices outlined in this guide, developers can effectively manage log output, control verbosity, modularize code, monitor performance, handle errors, and integrate with monitoring tools. With structured and informative logs, Python developers can gain valuable insights into their applications, leading to more efficient development cycles and improved overall software quality.</p>"},{"location":"blog/logging-for-deployment-in-python-a-practical-guide-to-effective-debugging-and-monitoring/#related-posts","title":"Related Posts","text":"<ul> <li>Cheat on Python Package Managers</li> </ul>"},{"location":"blog/how-to-deploy-a-streamlit-application-on-hugging-face/","title":"How to deploy a Streamlit Application on Hugging Face","text":"<p>In this guide, we'll walk through the steps to deploy a Streamlit app using the Hugging Face platform. For demonstration purposes, we'll create an app that utilizes the Python module Pix2Tex. Users will be able to upload an image and get the corresponding LaTeX formula along with a rendered version.</p>"},{"location":"blog/how-to-deploy-a-streamlit-application-on-hugging-face/#prerequisites","title":"Prerequisites","text":"<ol> <li>Create a virtual environment:</li> </ol> <code>For Linux &amp; Mac</code> <code>For Windows</code> <pre><code>python3 -m venv myenv\nsource myenv/bin/activate\n</code></pre> <pre><code>python -m venv myenv\n./myenv/Scripts/activate\n</code></pre> <ol> <li> <p>Install dependencies:</p> <pre><code>pip install streamlit pix2tex pillow\n</code></pre> </li> </ol>"},{"location":"blog/how-to-deploy-a-streamlit-application-on-hugging-face/#creating-the-streamlit-app","title":"Creating the Streamlit App","text":"<ol> <li> <p>Create the <code>app.py</code> file with your Streamlit app code.</p> <pre><code># app.py\nimport streamlit as st\nfrom PIL import Image\nfrom pix2tex.cli import LatexOCR\n\ndef main():\n    st.title(\"Image to LaTeX Formula Parser\")\n\n    # Upload image through Streamlit\n    uploaded_image = st.file_uploader(\"Upload an image\", type=[\"jpg\", \"jpeg\", \"png\"])\n\n    if uploaded_image is not None:\n        # Display the uploaded image\n        st.image(uploaded_image, caption=\"Uploaded Image\", use_column_width=True)\n\n        # Perform LaTeX OCR on the uploaded image\n        latex_formula = process_image(uploaded_image)\n\n        # Display LaTeX formula\n        st.subheader(\"LaTeX Formula:\")\n        st.text(latex_formula)\n\n        # Display parsed Markdown\n        parsed_md = parse_to_md(latex_formula)\n        st.subheader(\"Parsed Markdown:\")\n        st.latex(f\"\\n{latex_formula}\\n\")\n\ndef process_image(image):\n    # Perform LaTeX OCR on the image\n    img = Image.open(image)\n    model = LatexOCR()\n    latex_formula = model(img)\n    return latex_formula\n\ndef parse_to_md(latex_formula):\n    # You can implement your own logic to parse LaTeX to Markdown\n    # Here's a simple example for demonstration purposes\n    parsed_md = f\"**Parsed Formula:** *{latex_formula}*\"\n    return parsed_md\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> </li> <li> <p>Test the app locally:</p> <pre><code>streamlit run app.py\n</code></pre> </li> </ol>"},{"location":"blog/how-to-deploy-a-streamlit-application-on-hugging-face/#deploying-on-hugging-face","title":"Deploying on Hugging Face","text":"<ol> <li> <p>Create a new Hugging Face space:</p> <ul> <li>Visit Hugging Face Spaces.</li> <li>Name your repository and choose the Streamlit option.</li> </ul> <p></p> </li> <li> <p>Create a Hugging Face token:</p> <ul> <li>Visit Hugging Face Token Settings.</li> <li>Create a new token. the format is <code>hf_****</code></li> </ul> <p> </p> </li> <li> <p>Clone the repository and push your app:</p> <ul> <li> <p>clone the repo</p> <pre><code>git clone https://&lt;user_name&gt;:&lt;token&gt;@huggingface.co/&lt;user_name&gt;/&lt;repo_name&gt;\ncd &lt;repo_name&gt;\n</code></pre> </li> </ul> <p>the token is from step 6 - add <code>app.py</code> and <code>requirements.txt</code> into the folder - push the changes to huggingface</p> <pre><code>```bash\ngit add .\ngit commit -m \"Add application\"\ngit push\n```\n</code></pre> <ul> <li> <p>example</p> <pre><code>git clone https://hermann-web:hf_****@huggingface.co/spaces/hermann-web/pix2tex \ncd pix2tex\nmv ../app.py app.py\npip freeze &gt; requirements.txt\ngit add .\ngit commit -m \"Add application\"\ngit push \n</code></pre> </li> </ul> </li> <li> <p>Check your deployed application online:</p> <ul> <li>Visit Hugging Face Repository <code>https://huggingface.co/&lt;user_name&gt;/&lt;repo_name&gt;</code>.</li> </ul> <p></p> </li> </ol> <p>Congratulations! Your Streamlit app is now deployed on Hugging Face. You can find a live example on https://huggingface.co/spaces/hermann-web/pix2tex.</p>"},{"location":"blog/how-to-deploy-a-streamlit-application-on-hugging-face/#related-posts","title":"Related Posts","text":"<ul> <li>Deploying any Web application with Nginx: Example of Flask</li> </ul>"},{"location":"blog/simple-guide-to-using-docker-on-windows-10-and-access-from-wsl-2/","title":"Simple guide to using Docker on Windows 10 and access from WSL 2","text":""},{"location":"blog/simple-guide-to-using-docker-on-windows-10-and-access-from-wsl-2/#introduction","title":"Introduction","text":"<p>For a heavy linux user like me, using windows also mean find a door to work with a linux distro. There are several options to gauge from the situation. I usually need both docker and wsl2 on the computer. And i've installed this many times.</p> <p>So, this is a very straight forward tutorial on docker and wsl2 installation and configuration on windows10 or 11.</p>"},{"location":"blog/simple-guide-to-using-docker-on-windows-10-and-access-from-wsl-2/#prerequisites","title":"Prerequisites","text":"<ul> <li>Windows 10 Pro, Enterprise, or Education edition.</li> <li>WSL 2 enabled on your Windows machine.</li> <li>Docker Desktop for Windows installed.</li> </ul>"},{"location":"blog/simple-guide-to-using-docker-on-windows-10-and-access-from-wsl-2/#step-by-step-guide","title":"Step-by-Step Guide","text":"<ol> <li> <p>Install Docker Desktop for Windows:</p> <ul> <li>Download Docker Desktop for Windows from the official Docker website: https://www.docker.com/products/docker-desktop</li> <li>Run the installer and follow the instructions to complete the installation.</li> </ul> </li> <li> <p>Enable WSL 2:</p> Enable docker for wsl2 <p></p> </li> <li> <p>Set wsl default version to 2:</p> <ul> <li>Open PowerShell as an administrator and run the following command:</li> </ul> <pre><code>wsl --set-default-version 2\n</code></pre> </li> </ol> <ol> <li> <p>Install a Linux Distribution:</p> <ul> <li>Open the Microsoft Store app on your Windows machine.</li> <li>Search for a Linux distribution (e.g., Ubuntu, Debian, or Alpine) and install it.</li> </ul> wsl installer <p></p> </li> <li> <p>Configure WSL 2 as the Default WSL Version:</p> <ul> <li>see the list of distro you have</li> </ul> <pre><code>wsl.exe -l -v\n</code></pre> <ul> <li>Open PowerShell as an administrator and run the following command:</li> </ul> <pre><code>wsl --set-version &lt;distribution_name&gt; 2\n</code></pre> <p>Replace <code>&lt;distribution_name&gt;</code> with the name of the Linux distribution you installed.</p> </li> <li> <p>Start the Linux Distribution:</p> <ul> <li>Launch the Linux distribution you installed from the Start menu or by running its executable.</li> <li>Follow the initial setup instructions to create a user account and set a password.</li> </ul> </li> <li> <p>Add the Docker host configuration:</p> <ul> <li>Scroll to the end of the file using the arrow keys.</li> <li>Add the following line:</li> </ul> <pre><code>echo \"export DOCKER_HOST=unix:///var/run/docker.sock\" &gt;&gt; ~/.bashrc\n</code></pre> <ul> <li>I didn't need this one but who knows:</li> </ul> <pre><code># open the bashrc\nnano ~/.bashrc\n# add this line in the file\nexport DOCKER_HOST=tcp://localhost:2375\n</code></pre> </li> <li> <p>Reload the updated <code>.bashrc</code> file:</p> <ul> <li>Run the following command to apply the changes:</li> </ul> <pre><code>source ~/.bashrc\n</code></pre> </li> <li> <p>Verify Docker connectivity:</p> <ul> <li>Run the following command to check if the Docker client in WSL can connect to Docker Desktop:</li> </ul> <pre><code>docker version\n</code></pre> </li> <li> <p>Enable WSL Integration in Docker Desktop:</p> <ul> <li>Open Docker Desktop on your Windows machine.</li> <li>Right-click on the Docker Desktop icon in the system tray (notification area) and select \"Settings\".</li> <li>In the \"Settings\" window, navigate to the \"Resources\" section.</li> <li>Click on \"WSL Integration\" in the left sidebar.</li> <li>Toggle the switch next to your WSL distribution (e.g., Ubuntu) to enable integration.</li> <li>Click \"Apply &amp; Restart\" to save the changes and restart Docker Desktop.</li> </ul> </li> <li> <p>Verify Docker Connectivity in WSL:</p> <ul> <li>Open the WSL terminal (Ubuntu).</li> <li>Run the following command to check Docker connectivity:</li> </ul> <pre><code>docker version\n</code></pre> </li> </ol> <p>That's it! You should now have Docker successfully configured and running on Windows 10 with WSL 2. You can now use Docker commands within your Ubuntu WSL terminal to manage containers, images, and other Docker resources.</p> <p>If you encounter any issues or have further questions, feel free to ask. Happy Dockerizing!</p>"},{"location":"blog/simple-guide-to-using-docker-on-windows-10-and-access-from-wsl-2/#basic-commands-for-working-with-wsl","title":"basic commands for working with wsl <sup>2</sup>","text":"<ul> <li>install wsl: <code>wsl --install</code></li> <li>check list of distro (name and version): <code>wsl.exe -l -v</code></li> <li>change from v1 to v2: <code>wsl.exe --set-version Ubuntu 2</code> <sup>1</sup></li> <li>unsinstall the wsl: <code>wsl.exe --unregister Ubuntu-22.04</code> where <code>Ubuntu-22.04</code> is my distro name i got</li> <li>see images <code>docker images</code></li> </ul>"},{"location":"blog/simple-guide-to-using-docker-on-windows-10-and-access-from-wsl-2/#related-links","title":"Related links","text":"<ul> <li>Mastering Docker: A Comprehensive Guide to Efficient Container Management</li> </ul> <ol> <li> <p>Running Docker on WSL2 without Docker Desktop (the right way), Felipe Santos, 11 oct. 2022 \u21a9</p> </li> <li> <p>basic commands for working with wsl \u21a9</p> </li> </ol>"},{"location":"blog/mastering-docker-a-comprehensive-guide-to-efficient-container-management/","title":"Mastering Docker: A Comprehensive Guide to Efficient Container Management","text":"<p>Tired of complicated installation processes, inconsistent environments, and tedious deployment procedures in your development workflow?</p> <p>Whether you're an experienced developer or just starting out, Docker offers a game-changing approach when it comes to application deployment, scalability, and consistency.</p> <p>Dive into the world of Docker and utilise its full potential to simplify your container management and increase your productivity!</p> <p>In this comprehensive guide, we will introduce you to Docker's essential commands, best practices, and advanced techniques to help you harness the power of containerisation effectively.</p> <p>Ready to embark on a journey towards transparent development, easy deployment, and unrivalled efficiency? Let's dive in and master Docker from A to Z!</p>"},{"location":"blog/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#understanding-docker-a-primer-on-containerization","title":"Understanding Docker: A Primer on Containerization","text":"<p>Before diving into Docker's intricacies, let's establish a foundational understanding of containerization:</p>"},{"location":"blog/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#what-is-docker","title":"What is Docker?","text":"<p>Docker is a platform designed to make it easier to create, deploy, and run applications by using containers. Containers allow developers to package an application with all of its dependencies into a standardized unit for software development.</p>"},{"location":"blog/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#key-concepts","title":"Key Concepts","text":"<ul> <li>Images: Lightweight, standalone, and executable packages that contain everything needed to run a piece of software, including the code, runtime, libraries, and dependencies.</li> <li>Containers: Runnable instances of Docker images, encapsulating the software and its dependencies, ensuring consistency across different environments.</li> <li>Dockerfile: A text document that contains all the commands a user could call on the command line to assemble an image.</li> <li>Docker Hub: A cloud-based repository provided by Docker for finding, sharing, and distributing container images.</li> </ul> <p>With this foundation in place, let's delve deeper into Docker's functionalities and explore .</p>"},{"location":"blog/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#essential-docker-commands-navigating-the-docker-universe","title":"Essential Docker Commands: Navigating the Docker Universe","text":"<p>Creating a container from an image or using a specific distribution like Ubuntu involves a few steps. Here are the basic commands to create a Docker container:</p>"},{"location":"blog/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#create-a-container-from-an-image","title":"Create a Container from an Image","text":""},{"location":"blog/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#pull-an-image-from-docker-hub","title":"Pull an Image from Docker Hub","text":"<p>If you haven't already downloaded the image you want to use, you can pull it from Docker Hub using <code>docker pull</code>:</p> <pre><code>docker pull &lt;image_name&gt;\n</code></pre> <p>Replace <code>&lt;image_name&gt;</code> with the name of the image you want to pull. For example, to pull the Ubuntu image:</p> <pre><code>docker pull ubuntu\n</code></pre>"},{"location":"blog/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#run-a-container-from-an-image","title":"Run a Container from an Image","text":"<p>Once you have the image, you can create a container by running it with <code>docker run</code>:</p> <pre><code>docker run -it --name &lt;container_name&gt; &lt;image_name&gt; /bin/bash\n</code></pre> <p>Explanation of the flags used:</p> <ul> <li><code>-it</code>: Starts the container in interactive mode with a terminal.</li> <li><code>--name &lt;container_name&gt;</code>: Assigns a specific name to your container.</li> <li><code>&lt;image_name&gt;</code>: Specifies the image to use for creating the container.</li> <li><code>/bin/bash</code> (or another command): Starts a specific command or shell in the container. For Ubuntu, using <code>/bin/bash</code> opens a Bash shell.</li> </ul> <p>For instance, to create a container named <code>my-ubuntu-container</code> from the Ubuntu image:</p> <pre><code>docker run -it --name my-ubuntu-container ubuntu /bin/bash\n</code></pre> <p>This will start a new container based on the Ubuntu image and give you access to the Bash shell within that container.</p>"},{"location":"blog/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#install-software-or-customize-the-container","title":"Install Software or Customize the Container","text":"<p>Once inside the container, you can install software, make changes, or configure it as needed using standard commands as you would on a regular Linux system. For example, within the container:</p> <pre><code>apt update\napt install &lt;package_name&gt;\n</code></pre> <p>Replace <code>&lt;package_name&gt;</code> with the name of the package you want to install.</p>"},{"location":"blog/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#commit-changes-to-a-new-image-optional","title":"Commit Changes to a New Image (Optional)","text":"<p>If you make changes within the container and want to save them as a new image, you can commit the container's current state:</p> <pre><code>docker commit &lt;container_id&gt; &lt;new_image_name&gt;\n</code></pre> <p>Replace <code>&lt;container_id&gt;</code> with the ID of your container, and <code>&lt;new_image_name&gt;</code> with the name you want to give to the new image.</p> <p>For example:</p> <pre><code>docker commit my-ubuntu-container my-custom-ubuntu\n</code></pre> <p>This will create a new image named <code>my-custom-ubuntu</code> based on the changes made in the <code>my-ubuntu-container</code> container.</p> <p>Remember, changes made inside a container will be lost if you remove the container without committing those changes to a new image.</p> <p>These steps should help you create, customize, and manage containers based on Docker images, allowing you to work with specific distributions like Ubuntu or any other image available on Docker Hub.</p>"},{"location":"blog/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#get-all-container-along-with-their-id-and-status","title":"Get all container along with their ID and status","text":"<p>To access an existing container from another terminal, you need its container ID or name. You can find this information looking for all containers.</p> <p>To get the container ID, you can use the following command in the terminal:</p> <pre><code>$ docker ps\n# to see all containers, running and stopped\n$ docker ps -a\n</code></pre> <p>This command lists all the running containers along with their IDs, names, and other details.</p> <p>Example output (nodejs-docker):</p> <pre><code>ID            IMAGE                                COMMAND    ...   PORTS\necce33b30ebf  &lt;your username&gt;/node-web-app:latest  npm start  ...   49160-&gt;8080\n</code></pre>"},{"location":"blog/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#print-app-output","title":"Print app output","text":"<p>To print the application output, you can use the following command:</p> <pre><code>docker logs &lt;container id&gt;\n</code></pre> <p>Example output:</p> <pre><code>Running on http://localhost:8080\n</code></pre>"},{"location":"blog/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#access-the-application","title":"Access the application","text":"<p>To access the application, you can use the <code>curl</code> command:</p> <pre><code>curl -i localhost:49160\n</code></pre> <p>Example output:</p> <pre><code>HTTP/1.1 200 OK\nX-Powered-By: Express\nContent-Type: text/html; charset=utf-8\nContent-Length: 12\nETag: W/\"c-M6tWOb/Y57lesdjQuHeB1P/qTV0\"\nDate: Mon, 13 Nov 2017 20:53:59 GMT\nConnection: keep-alive\n\nHello world\n</code></pre>"},{"location":"blog/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#access-cmd-line-of-the-running-container","title":"Access cmd line of the running container","text":"<p>Once you have the container ID or name, you can use the <code>docker exec</code> command to access it from another terminal. The command syntax is:</p> <pre><code>docker exec -it &lt;container_id_or_name_&gt; /bin/bash\n</code></pre> <p>Replace <code>&lt;container_id_or_name&gt;</code> with the actual ID or name of your container.</p> <p>For example, if your container's name is <code>my-ubuntu-container</code>, you'd run:</p> <pre><code>docker exec -it my-ubuntu-container /bin/bash\n</code></pre> <p>This command will open a new terminal session inside the running container, allowing you to interact with it just like you did from the initial terminal.</p> <p>Remember, you can access a running container from multiple terminals simultaneously using <code>docker exec</code>. Each terminal session will have its own instance of a shell within the same container, enabling parallel interactions and commands execution.</p>"},{"location":"blog/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#kill-the-running-container","title":"Kill the running container","text":"<p>To stop the running container, you can use the following command:</p> <pre><code>docker kill &lt;container id&gt;\n</code></pre> <p>Example output:</p> <pre><code>&lt;container id&gt;\n</code></pre>"},{"location":"blog/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#restart-a-stoppedkilled-container","title":"Restart a stopped/killed container","text":"<p>To run the killed container, you can use the following command:</p> <pre><code># see all the containers\n$ docker ps -a \n# restart the one\n$ docker start &lt;container id&gt;\n</code></pre> <p>Example output:</p> <pre><code>&lt;container id&gt;\n</code></pre>"},{"location":"blog/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#confirm-that-the-app-has-stopped","title":"Confirm that the app has stopped","text":"<p>To confirm that the application has stopped, you can use the <code>curl</code> command again:</p> <pre><code>curl -i localhost:49160\n</code></pre> <p>Example output:</p> <pre><code>curl: (7) Failed to connect to localhost port 49160: Connection refused\n</code></pre>"},{"location":"blog/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#delete-a-container","title":"delete a container","text":"<p>To delete a Docker container by its ID, you can use the <code>docker rm</code> command followed by the container ID. Here's the syntax:</p> <pre><code>docker rm &lt;container_id&gt;\n</code></pre> <p>Replace <code>&lt;container_id&gt;</code> with the actual ID of the container you want to delete.</p> <p>For example, if your container ID is <code>a460131e5352</code>, you'd run:</p> <pre><code>docker rm a460131e5352\n</code></pre> <p>This command will remove the specified container. Make sure the container is stopped before attempting to remove it. If the container is running, you can stop it using <code>docker stop &lt;container_id&gt;</code> before removing it.</p> <p>Please note that deleting a container is a permanent action, and its associated data (unless stored in a separate volume) will be lost.</p>"},{"location":"blog/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#share-files-between-your-pc-and-a-docker-container","title":"Share files between your PC and a Docker container","text":"<p>To share files between your PC and a Docker container using the container ID, you can use Docker's <code>docker cp</code> command. This command allows you to copy files or directories between the host system and a container. Here's how you can use it:</p>"},{"location":"blog/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#copying-files-from-host-to-container","title":"Copying Files from Host to Container","text":"<pre><code>docker cp /path/on/host/file_or_directory &lt;container_id&gt;:/path/in/container\n</code></pre> <p>Replace <code>/path/on/host/file_or_directory</code> with the path to the file or directory on your host system that you want to copy into the container.</p> <p>Replace <code>&lt;container_id&gt;</code> with the ID of the container where you want to copy the files.</p> <p>Replace <code>/path/in/container</code> with the path inside the container where you want to place the files.</p> <p>For instance, if you have a file <code>/home/user/file.txt</code> on your host system and you want to copy it into a container with ID <code>a460131e5352</code> into the container's <code>/app/data</code> directory, you'd use:</p> <pre><code>docker cp /home/user/file.txt a460131e5352:/app/data\n</code></pre>"},{"location":"blog/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#copying-files-from-container-to-host","title":"Copying Files from Container to Host","text":"<p>Similarly, you can copy files from a container back to your host system:</p> <pre><code>docker cp &lt;container_id&gt;:/path/in/container/file_or_directory /path/on/host\n</code></pre> <p>Replace <code>&lt;container_id&gt;</code> with the ID of the container from which you want to copy files.</p> <p>Replace <code>/path/in/container/file_or_directory</code> with the path inside the container of the file or directory you want to copy.</p> <p>Replace <code>/path/on/host</code> with the path on your host system where you want to place the copied files.</p> <p>For example, if you want to copy a file named <code>output.log</code> from a container with ID <code>a460131e5352</code> located at <code>/var/logs</code> to your host's <code>/home/user/logs</code> directory, you'd run:</p> <pre><code>docker cp a460131e5352:/var/logs/output.log /home/user/logs\n</code></pre> <p>This way, you can share files between your PC and a Docker container using the <code>docker cp</code> command.</p>"},{"location":"blog/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#vscode-tools","title":"VSCode Tools","text":"<p>open a container folder in vscode</p> <p>consult this link</p>"},{"location":"blog/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#create-an-image-from-an-application","title":"Create an Image from an Application","text":"<p>Managing Docker images and their containers involves a structured process. Here's an organized version detailing the steps:</p>"},{"location":"blog/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#organize-files","title":"Organize Files:","text":"<p>Organizing files is pivotal for smooth Docker image creation. Ensure your application code, including the Dockerfile, <code>.dockerignore</code>, and necessary configuration files, resides within the same directory. For instance, a typical Node.js app might have the following structure:</p> <pre><code>your-app-directory/\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 .dockerignore\n\u251c\u2500\u2500 package.json\n\u251c\u2500\u2500 index.js (or main application file)\n\u251c\u2500\u2500 ... (other application files and directories)\n</code></pre> <p>Keeping all pertinent files together simplifies referencing in the Dockerfile and ensures that only essential application files are included in the image.</p>"},{"location":"blog/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#example-dockerfile-for-an-express-app","title":"Example Dockerfile for an Express App:","text":"<p>Here's a simple Dockerfile suitable for an Express application running on Node.js 14:</p> <pre><code># Use Node.js 14 as the base image\nFROM node:14\n\n# Create and set the working directory in the container\nWORKDIR /usr/src/app\n\n# Copy package.json and package-lock.json to the working directory\nCOPY package*.json ./\n\n# Install application dependencies\nRUN npm install\n\n# Copy the entire application directory into the container\nCOPY . .\n\n# Expose port 3001 to the outside world\nEXPOSE 3001\n\n# Define the command to start the application\nCMD [\"npm\", \"start\"]\n</code></pre>"},{"location":"blog/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#create-dockerignore","title":"Create .dockerignore:","text":"<p>A sample <code>.dockerignore</code> might contain exclusions like:</p> <pre><code>node_modules\nnpm-debug.log\nimages\ndoc-api\nlogs\n.github\n.git\n.env\n.prod.env\ntests\n</code></pre> <p>The <code>.dockerignore</code> file excludes unnecessary files and directories from being copied into the Docker image during the build process. Including <code>.git</code> is generally advised to prevent large, unneeded directories from being included.</p> <p>Additionally, consider excluding:</p> <ul> <li>Sensitive Data: Files containing sensitive information.</li> <li>Development Configs: Configuration specific to local development.</li> <li>Build Artifacts: Generated files not needed for the running application.</li> <li>IDE/Editor Files: Editor-specific files not crucial for the app's runtime.</li> <li>Documentation/Images: Non-essential assets irrelevant to the app's functionality.</li> </ul>"},{"location":"blog/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#quick-trick-using-a-modified-gitignore","title":"Quick trick: Using a modified .gitignore","text":"<p>Consider using a modified <code>.gitignore</code> as <code>.dockerignore</code>. However, ensure it's not overly restrictive, as some files ignored in version control might be necessary for the app to run correctly within a Docker container.</p> <p>In my example, <code>.test.env</code> was in .gitignore but important. However, <code>tests</code> is not in .gitignore but useless in production</p>"},{"location":"blog/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#build-the-image","title":"Build the Image:","text":"<p>Navigate to the directory containing your Dockerfile and execute:</p> <pre><code>docker build -t your-image-name .\n</code></pre> <p>This command constructs a Docker image from the Dockerfile and tags it with the specified name.</p>"},{"location":"blog/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#run-a-container-from-the-new-image","title":"Run a Container from the New Image:","text":"<p>Start a container using the newly built image:</p> <pre><code>docker run -p 49160:3001 your-image-name\n</code></pre> <p>Replace <code>your-image-name</code> with the image's name. If your app runs on port 3001 in the container, access it at <code>localhost:49160</code> on your machine.</p>"},{"location":"blog/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#managing-images-and-associated-containers","title":"Managing Images and Associated Containers","text":""},{"location":"blog/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#list-all-images","title":"List All Images","text":"<p>To view all Docker images on your system, execute:</p> <pre><code>docker images\n</code></pre> <p>Use <code>-q</code> for a compact list displaying only image IDs:</p> <pre><code>docker images -q\n</code></pre>"},{"location":"blog/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#list-containers-using-a-specific-image","title":"List Containers Using a Specific Image","text":"<p>To list containers using a particular image:</p> <pre><code>docker ps -a --filter ancestor=your-image-name\n</code></pre> <p>Use <code>--format</code> to customize output (e.g., <code>{{.ID}}</code> for IDs):</p> <pre><code>docker ps -a --filter ancestor=your-image-name --format \"{{.ID}}\"\n</code></pre>"},{"location":"blog/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#stop-containers-using-an-image","title":"Stop Containers Using an Image","text":"<p>Stop all containers associated with an image:</p> <pre><code>docker stop $(docker ps -a -q --filter ancestor=your-image-name)\n</code></pre>"},{"location":"blog/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#remove-an-image-and-associated-containers","title":"Remove an Image and Associated Containers","text":"<p>To rebuild an image, follow these steps:</p> <ol> <li>Stop and Remove Containers:</li> </ol> <pre><code>docker stop $(docker ps -a -q --filter ancestor=your-image-name)\ndocker rm $(docker ps -a -q --filter ancestor=your-image-name)\n</code></pre> <p>Replace <code>your-image-name</code> with the specific image name or ID.</p> <ol> <li>Remove the Image:</li> </ol> <pre><code>docker rmi your-image-name\n</code></pre> <p>Replace <code>your-image-name</code> with the name of your Docker image.</p> <ol> <li>Rebuild the Image:</li> </ol> <p>After removing the container and image, rebuild the image:</p> <pre><code>docker build -t your-image-name .\n</code></pre> <p>This rebuilds the Docker image using the Dockerfile in the directory and tags it with the specified name.</p> <ol> <li>Run a New Container:</li> </ol> <p>Start a new container from the rebuilt image:</p> <pre><code>docker run -p 49160:3001 your-image-name\n</code></pre> <p>Replace <code>your-image-name</code> with the name of your newly built Docker image.</p> <p>By following these steps, you'll manage existing containers, remove associated images, and rebuild a fresh image for running a new container with updated changes.</p>"},{"location":"blog/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#clean-env","title":"clean env","text":""},{"location":"blog/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#remove-all-images-that-have-none-as-img-name","title":"remove all images that have none as img name","text":"<pre><code>#!/bin/bash\n\n# Get a list of image IDs for images with the tag \"none\"\nimages=$(docker images | grep \"none\" | awk '{print $3}')\n\n# Loop through each image ID and execute the commands\nfor image_id in $images\ndo\n # Stop containers based on the image\n docker stop $(docker ps -a -q --filter ancestor=$image_id)\n\n # Remove containers based on the image\n docker rm $(docker ps -a -q --filter ancestor=$image_id)\n\n # Remove the image itself\n docker rmi $image_id\ndone\n</code></pre>"},{"location":"blog/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#remove-all-images-that-dont-have-a-container","title":"remove all images that dont have a container","text":"<pre><code># Get a list of image IDs for all images\nimages=$(docker images | awk '{print $3}')\n\n# Loop through each image ID and execute the commands\nfor image_id in $images\ndo\n # Remove the image itself if possible, which mean there's no associated container\n docker rmi $image_id\ndone\n</code></pre>"},{"location":"blog/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#a-great-removal","title":"a great removal","text":"<pre><code>docker system prune -a\n</code></pre> <p>This command will remove all unused containers, networks, images (both dangling and unreferenced), and optionally, volumes.</p>"},{"location":"blog/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#syncing-changes-between-local-directory-and-docker-container","title":"Syncing Changes between Local Directory and Docker Container","text":"<p>When working with projects like FastAPI, Django, ReactJS, MkDocs, NestJs, Flask or similar applications running inside Docker containers, syncing changes between your local development environment and the container becomes crucial for an efficient workflow. This synchronization ensures that any modifications made locally reflect inside the running Docker container, allowing real-time updates without the need for manual file transfers or container restarts.</p> <p>For instance, when using MkDocs to build documentation or working with FastAPI and Django for web development, syncing changes enables immediate feedback on updates, edits, or additions made to the project files.</p> <p>You can use the <code>--mount</code> option with <code>type=bind</code> in Docker accomplishes this synchronization:</p> <pre><code>docker run -p 49160:8000 --mount type=bind,source=$DOCS_ABS_PATH,target=/app/docs project_docu\n</code></pre> <p>This command binds the local <code>docs</code> directory (specified by <code>$DOCS_ABS_PATH</code>) to the <code>/app/docs</code> directory within the running <code>project_docu</code> Docker container. Any changes made within the local <code>docs</code> directory will be instantly reflected in the container, and vice versa.</p> <p>This synchronization mechanism ensures that as you modify files within your local development environment\u2014be it Markdown files for MkDocs, source code for FastAPI, or Django\u2014the changes are immediately accessible and reflected within the Docker container. This allows for seamless development, testing, and previewing of changes without interruptions caused by manual file transfers or container restarts.</p> <p>Integrating this sync approach into your development workflow significantly streamlines the process, enhancing productivity and enabling swift iterations during project development and testing phases.</p>"},{"location":"blog/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#optimizing-container-performance","title":"Optimizing Container Performance","text":"<p>To maximize the performance and efficiency of your containers, consider the following tips:</p> <ul> <li>Keep Images Lean: Minimize the size of your Docker images by removing unnecessary dependencies and files.</li> <li>Use Docker Compose: Streamline multi-container applications by defining them in a single file with Docker Compose.</li> <li>Monitor Resource Usage: Monitor CPU, memory, and disk usage of your containers using Docker stats to identify performance bottlenecks.</li> <li>Implement Container Orchestration: Explore container orchestration tools like Kubernetes or Docker Swarm for managing large-scale container deployments.</li> </ul> <p>By incorporating these advanced techniques into your Docker workflow, you can elevate your container management to new heights and unlock the full potential of Docker.</p>"},{"location":"blog/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#conclusion-embracing-the-future-of-containerization","title":"Conclusion: Embracing the Future of Containerization","text":"<p>Congratulations! You've embarked on a transformative journey through the world of Docker, mastering essential commands, best practices, and advanced techniques along the way. Armed with this knowledge, you're well-equipped to revolutionize your development workflow, streamline deployment processes, and unleash unparalleled efficiency with Docker.</p> <p>As you continue to explore and experiment with Docker, remember that containerization is not just a technology\u2014it's a mindset. Embrace the principles of consistency, scalability, and efficiency, and let Docker empower you to build, deploy, and scale applications like never before.</p>"},{"location":"blog/mastering-docker-a-comprehensive-guide-to-efficient-container-management/#related-links","title":"Related links","text":"<ul> <li>Simple guide to using Docker on Windows 10 and access from WSL 2</li> </ul>"},{"location":"blog/step-by-step-guide-to-identifying-and-terminating-processes-on-specific-ports/","title":"Step-by-Step Guide to Identifying and Terminating Processes on Specific Ports","text":""},{"location":"blog/step-by-step-guide-to-identifying-and-terminating-processes-on-specific-ports/#introduction","title":"Introduction","text":"<p>This markdown provides a step-by-step guide to identify and terminate processes running on a specific port, catering to both Unix-based and Windows systems.</p>"},{"location":"blog/step-by-step-guide-to-identifying-and-terminating-processes-on-specific-ports/#handling-processes-on-a-port","title":"Handling Processes on a Port","text":"<p>Suppose you encounter an <code>OSError: [Errno 98] Address already in use</code> error while trying to run an application that requires port 8000. This commonly happens when another process is already using the same port.</p>"},{"location":"blog/step-by-step-guide-to-identifying-and-terminating-processes-on-specific-ports/#method-1-using-curl-to-test-the-port","title":"Method 1: Using <code>curl</code> to Test the Port","text":"<p>One way to check if a process is using port 8000 is by attempting to access it:</p> <pre><code>curl 127.0.0.1:8000\n</code></pre> <p>If you encounter an error or a response different from what you expect, it may indicate a running application using that port.</p>"},{"location":"blog/step-by-step-guide-to-identifying-and-terminating-processes-on-specific-ports/#method-2-using-ps-and-grep-command","title":"Method 2: Using <code>ps</code> and <code>grep</code> Command","text":"<p>The <code>ps</code> command, in conjunction with <code>grep</code>, can display processes associated with a specific port. However, this method might not precisely show processes bound to port 8000; rather, it lists processes containing \"8000\" in their information.</p> <pre><code>ps aux | grep 8000\n</code></pre>"},{"location":"blog/step-by-step-guide-to-identifying-and-terminating-processes-on-specific-ports/#method-3-using-lsof-to-identify-processes-by-port","title":"Method 3: Using <code>lsof</code> to Identify Processes by Port","text":"<p>The <code>lsof</code> command is specifically designed to list processes using a particular port. Execute the following command to identify processes running on port 8000:</p> <pre><code>lsof -i :8000\n</code></pre> <p>This command displays detailed information about processes using port 8000, including their Process ID (PID) and associated program.</p>"},{"location":"blog/step-by-step-guide-to-identifying-and-terminating-processes-on-specific-ports/#method-4-windows-equivalent-netstat","title":"Method 4: Windows Equivalent (<code>netstat</code>)","text":"<p>For Windows users, the <code>netstat</code> command helps identify active connections and associated processes using port 8000:</p> <pre><code>netstat -ano | findstr :8000\n</code></pre>"},{"location":"blog/step-by-step-guide-to-identifying-and-terminating-processes-on-specific-ports/#additional-methods","title":"Additional Methods","text":""},{"location":"blog/step-by-step-guide-to-identifying-and-terminating-processes-on-specific-ports/#using-ps-l-for-detailed-process-information","title":"Using <code>ps -l</code> for Detailed Process Information","text":"<p>The <code>ps -l</code> command provides detailed information about processes, including the process state, start time, and more. Use it in combination with <code>grep</code> to filter processes for port 8000:</p> <pre><code>ps -l | grep 8000\n</code></pre>"},{"location":"blog/step-by-step-guide-to-identifying-and-terminating-processes-on-specific-ports/#forcefully-terminating-a-process-with-kill-9","title":"Forcefully Terminating a Process with <code>kill -9</code>","text":"<p>In some cases, a process may not respond to a regular <code>kill</code> command. The <code>kill -9</code> command forcefully terminates a process. Use it with caution, as it does not give the process a chance to clean up resources:</p> <pre><code>kill -9 &lt;PID&gt;\n</code></pre>"},{"location":"blog/step-by-step-guide-to-identifying-and-terminating-processes-on-specific-ports/#terminating-the-identified-process","title":"Terminating the Identified Process","text":"<p>Once you've identified the Process ID (PID) of the process using port 8000, you can terminate it using the <code>kill</code> or <code>kill -9</code> command.</p> <ol> <li> <p>Identify the PID: Use <code>lsof</code> or <code>netstat</code> to find the PID associated with port 8000.</p> <p>Example with <code>lsof</code>:</p> <pre><code>lsof -i :8000\n</code></pre> </li> <li> <p>Kill the Process: Once you have the PID, use the <code>kill</code> command followed by the PID to terminate the process.</p> <p>Example, if the PID is 1234:</p> <pre><code>kill 1234\n</code></pre> <p>If needed, and the process is unresponsive to a regular <code>kill</code>, you can use <code>kill -9</code>:</p> <pre><code>kill -9 1234\n</code></pre> </li> </ol> <p>Always exercise caution when terminating processes, especially with <code>kill -9</code>, as it may impact running applications or services. Ensure proper permissions and confirm that you're terminating the correct process to avoid unintended consequences.</p>"},{"location":"blog/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/","title":"Mastering Essential Linux Commands: Your Path to File and Directory Mastery","text":""},{"location":"blog/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#introduction","title":"Introduction","text":"<p>This documentation aims to offer a comprehensive understanding of essential commands and techniques for file and directory management in a Linux environment. Mastering these commands is crucial for efficient navigation, manipulation, and analysis of files and directories.</p> <p>We'll embark on a journey by delving into the foundational usage of key commands like <code>wc</code>, <code>du</code>, <code>grep</code>, <code>awk</code>, and <code>find</code>, uncovering their individual functionalities. Additionally, we'll explore how these commands can be combined using powerful methods such as pipes (<code>|</code>), <code>-exec {} \\;</code>, or <code>-exec {} +</code>, unlocking their synergistic potential.</p> <p>Moreover, to solidify your understanding, real-life examples showcasing practical applications will be demonstrated.</p> <p>The hands-on experience gained through testing and implementing these commands will be pivotal in comprehending their nuanced usage and unleashing their practical utility.</p> <p>Let the learning begin !</p>"},{"location":"blog/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#1-basic-commands-overview","title":"1. Basic Commands Overview","text":""},{"location":"blog/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#wc-word-count","title":"wc (Word Count)","text":"<p>The <code>wc</code> command is used to count lines, words, and characters in files.</p> <ul> <li>Counting lines in a file:</li> </ul> <pre><code>wc -l file.txt\n</code></pre> <p>This command displays the number of lines in <code>file.txt</code>.</p>"},{"location":"blog/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#du-disk-usage","title":"du (Disk Usage)","text":"<p>The <code>du</code> command estimates file and directory space usage.</p> <ul> <li>Getting the size of a directory:</li> </ul> <pre><code>du -h /path/to/directory\n</code></pre> <p>This command provides the disk usage of the specified directory (<code>/path/to/directory</code>) in a human-readable format (<code>-h</code>).</p>"},{"location":"blog/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#grep-global-regular-expression-print","title":"grep (Global Regular Expression Print)","text":"<p>The <code>grep</code> command searches for patterns in files.</p> <ul> <li>Searching for lines containing a pattern in a file:</li> </ul> <pre><code>grep \"pattern\" file.txt\n</code></pre> <p>This command displays lines in <code>file.txt</code> that contain the specified <code>pattern</code>.</p> <ul> <li>Searching for lines containing multiple patterns in a file:</li> </ul> <pre><code>grep -e \"pattern1\" -e \"pattern2\" file.txt\n</code></pre> <p>This command displays lines in <code>file.txt</code> that contain either \"pattern1\" or \"pattern2\".</p> Additional <code>grep</code> options: <ul> <li> <p><code>-H</code>: Print the filename for each match when searching in multiple files.   <pre><code>grep -H \"pattern\" file1.txt file2.txt\n</code></pre></p> </li> <li> <p><code>-l</code>: Display only the names of files that contain at least one match, instead of showing the matching lines.   <pre><code>grep -l \"pattern\" file1.txt file2.txt\n</code></pre></p> </li> <li> <p><code>-n</code>: Display the line numbers along with the matching lines.   <pre><code>grep -n \"pattern\" file.txt\n</code></pre></p> </li> <li> <p><code>-w</code>: Match the whole word, ensuring that only entire words are considered.   <pre><code>grep -w \"word\" file.txt\n</code></pre></p> </li> <li> <p><code>-i</code>: Perform case-insensitive matching, ignoring differences in case when searching for the pattern.   <pre><code>grep -i \"pattern\" file.txt\n</code></pre></p> </li> <li> <p><code>-B N</code>: Display N lines before the matching line.   <pre><code>grep -B 2 \"pattern\" file.txt\n</code></pre></p> </li> <li> <p><code>-A N</code>: Display N lines after the matching line.   <pre><code>grep -A 2 \"pattern\" file.txt\n</code></pre></p> </li> </ul> <p>These options enhance the functionality of <code>grep</code> by providing more context, line numbers, and filename information when searching for patterns in files.</p>"},{"location":"blog/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#awk-aho-weinberger-and-kernighan","title":"awk (Aho, Weinberger, and Kernighan)","text":"<ul> <li>Basic Syntax:</li> </ul> <pre><code>awk 'pattern { action }' file.txt\n</code></pre> <ul> <li><code>pattern</code>: The condition that a line must meet to trigger the action.</li> <li> <p><code>action</code>: The set of commands to be executed when the pattern is matched.</p> <p>Example</p> <p><pre><code>awk '/pattern/ { print $1 }' file.txt\n</code></pre> This command prints the first field of each line in <code>file.txt</code> where the pattern is found.</p> Common Use Cases: <ol> <li> <p>Printing Specific Columns: <pre><code>awk '{ print $2, $4 }' file.txt\n</code></pre>   This prints the second and fourth columns of each line in <code>file.txt</code>.</p> </li> <li> <p>Pattern Matching: <pre><code>awk '/error/ { print $0 }' log.txt\n</code></pre>   Prints lines containing the word \"error\" from the <code>log.txt</code> file.</p> </li> <li> <p>Calculations: <pre><code>awk '{ total += $1 } END { print total }' numbers.txt\n</code></pre>   Calculates and prints the sum of the values in the first column of <code>numbers.txt</code>.</p> </li> <li> <p>Custom Field and Record Separators: <pre><code>awk -F',' '{ print $1 }' data.csv\n</code></pre>   Specifies ',' as the field separator in a CSV file.</p> </li> </ol> Advanced Features: <ul> <li> <p>Variables: <pre><code>awk '{ total += $1 } END { print \"Sum:\", total }' numbers.txt\n</code></pre>   Uses the variable <code>total</code> to accumulate values.</p> </li> <li> <p>Built-in Functions: <pre><code>awk '{ print length($0) }' text.txt\n</code></pre>   Prints the length of each line in <code>text.txt</code>.</p> </li> <li> <p>Conditional Statements: <pre><code>awk '{ if ($1 &gt; 10) print $0 }' values.txt\n</code></pre>   Prints lines where the value in the first column is greater than 10.</p> </li> </ul> </li> </ul> <p><code>awk</code> is versatile and can be highly customized for various text processing tasks. It's especially useful for working with structured data in files.</p>"},{"location":"blog/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#head-and-tail","title":"<code>head</code> and <code>tail</code>","text":"<p>The <code>head</code> command displays the beginning of a file, while <code>tail</code> shows the end.</p> <ul> <li>Viewing the first few lines of a file with <code>head</code>:</li> </ul> <pre><code>head file.txt\n</code></pre> <p>This command displays the first few lines of <code>file.txt</code>.</p> <ul> <li>Displaying a specific number of lines at the beginning of a file with <code>head -n</code>:</li> </ul> <pre><code>head -n 10 file.txt\n</code></pre> <p>This command displays the first 10 lines of <code>file.txt</code>. You can replace <code>10</code> with any number to view a different quantity of lines.</p> <ul> <li>Viewing the last few lines of a file with <code>tail</code>:</li> </ul> <pre><code>tail file.txt\n</code></pre> <p>This command shows the last few lines of <code>file.txt</code>.</p> <ul> <li>Displaying a specific number of lines at the end of a file with <code>tail -n</code>:</li> </ul> <pre><code>tail -n 15 file.txt\n</code></pre> <p>This command shows the last 15 lines of <code>file.txt</code>. Similarly, you can adjust <code>15</code> to any desired number to see a different quantity of lines.</p> <p>Using <code>-n</code> with <code>head</code> or <code>tail</code> allows you to precisely control the number of lines displayed from the beginning or end of a file.</p>"},{"location":"blog/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#less-and-more","title":"<code>less</code> and <code>more</code>","text":"<p>Both <code>less</code> and <code>more</code> are used to view text files in a paginated manner.</p> <ul> <li>Viewing a file with <code>less</code>:</li> </ul> <pre><code>less file.txt\n</code></pre> <p><code>less</code> allows you to navigate through the file interactively.</p> <ul> <li>Viewing a file with <code>more</code>:</li> </ul> <pre><code>more file.txt\n</code></pre> <p><code>more</code> displays the file content page by page, but it has more limited navigation options compared to <code>less</code>.</p> <p>These commands provide different ways to view file contents, either scrolling through the entire file or just a section at a time.</p>"},{"location":"blog/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#find-search-for-files","title":"find (Search for Files)","text":"<p>The <code>find</code> command searches for files and directories based on various criteria.</p> <ul> <li>Basic Syntax:</li> </ul> <pre><code>find [path] [options] [expression]\n</code></pre> <ul> <li><code>path</code>: Specifies the directory or directories to start the search from. If omitted, the current directory is used.</li> <li><code>options</code>: Additional flags or arguments that modify the behavior of the <code>find</code> command.</li> <li> <p><code>expression</code>: Specifies the criteria that files must meet to be included in the search results.</p> <p>Example</p> <p><pre><code>find /path/to/directory -name \"*.txt\"\n</code></pre> This command searches for files with a <code>.txt</code> extension within the <code>/path/to/directory</code> and its subdirectories.</p> <p>for case insensitive search, use <code>-iname</code> instead of <code>-name</code></p> Common Use Cases: <ul> <li> <p>Searching by Name: <pre><code>find . -name \"example.txt\"\n</code></pre>   Searches for a file named \"example.txt\" in the current directory and its subdirectories.</p> </li> <li> <p>Searching files matching pattern: <pre><code>find /path/to/search -name \"req*.txt\"\n</code></pre>   Searches <code>req*.txt</code> in <code>/path/to/search</code> and its subdirectories.</p> </li> <li> <p>Searching by Type: <pre><code>find /home/user -type d\n</code></pre>   Finds directories under <code>/home/user</code>.</p> </li> </ul> <p><code>-type f</code> search files when <code>-type d</code> is for directories</p> <ul> <li> <p>Searching by Size: <pre><code>find /var/log -size +1M\n</code></pre>   Finds files larger than 1 megabyte in size under <code>/var/log</code>.</p> </li> <li> <p>Searching by Modification Time: <pre><code>find /etc -mtime -7\n</code></pre>   Finds files modified within the last 7 days under <code>/etc</code>.</p> </li> <li> <p>Combining Multiple Criteria: <pre><code>find /tmp -name \"*.log\" -size +100k\n</code></pre>   Finds log files larger than 100 kilobytes in size under <code>/tmp</code>.</p> </li> <li> <p>Find and delete files: <pre><code>find /path/to/search -name \"file_to_delete.txt\" -delete\n</code></pre>   This command finds a file named <code>file_to_delete.txt</code> and deletes it.</p> </li> </ul> Advanced Features: <ul> <li> <p>Executing Commands on Found Files: <pre><code>find /home -type f -exec chmod 644 {} \\;\n</code></pre>   Changes the permissions of all files under <code>/home</code> to 644.</p> </li> <li> <p>Using Logical Operators: <pre><code>find /var/log \\( -name \"*.log\" -o -name \"*.txt\" \\)\n</code></pre>   Finds files with either <code>.log</code> or <code>.txt</code> extensions under <code>/var/log</code>.</p> </li> <li> <p>Combining with Other Commands: <pre><code>find /etc -type f -exec grep -H \"keyword\" {} \\;\n</code></pre>   Searches for the keyword \"keyword\" within files under <code>/etc</code>.</p> </li> </ul> </li> </ul> <p><code>find</code> is a powerful utility for searching and locating files based on various criteria such as name, type, size, and modification time.</p>"},{"location":"blog/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#xargs-extended-arguments","title":"xargs (Extended Arguments)","text":"<p><code>xargs</code> is a command in Linux/Unix operating systems that allows you to build and execute command lines from standard input. It's particularly useful when you have a list of items from some source (like the output of another command, a file, or user input), and you want to pass these items as arguments to another command.</p> <ul> <li>Basic Syntax:</li> </ul> <pre><code>xargs [options] command [initial-arguments]\n</code></pre> <ul> <li><code>options</code>: Various options that modify the behavior of <code>xargs</code>.</li> <li><code>command</code>: The command to be executed with the arguments passed by <code>xargs</code>.</li> <li> <p><code>initial-arguments</code>: Optional initial arguments to be used with the command.</p> How do <code>xargs</code> works ? <p>Here's how <code>xargs</code> typically works:</p> <ol> <li> <p>Standard Input: <code>xargs</code> reads data from standard input (stdin) by default. This data can be piped into <code>xargs</code> from the output of another command or provided directly via keyboard input.</p> </li> <li> <p>Tokenization: <code>xargs</code> breaks the input into pieces or tokens. By default, it splits the input into items based on whitespace (spaces, tabs, and newlines), but you can specify a different delimiter using the <code>-d</code> option.</p> </li> <li> <p>Command Execution: <code>xargs</code> takes each token or item from the input and appends it to the end of a command line template. It then executes the resulting command line.</p> </li> <li> <p>Argument Limit: <code>xargs</code> automatically splits the input into multiple command invocations if the number of items exceeds the maximum argument limit for the operating system.</p> </li> <li> <p>Handling Spaces and Special Characters: <code>xargs</code> ensures that each item is properly quoted so that spaces and other special characters are correctly interpreted by the command being executed.</p> </li> </ol> <p>Here's a simple example to illustrate how <code>xargs</code> works:</p> <pre><code>$ echo \"file1 file2 file3\" | xargs ls -l\n</code></pre> <p>In this example:</p> <ul> <li><code>echo \"file1 file2 file3\"</code> prints the string \"file1 file2 file3\" to stdout.</li> <li><code>|</code> (pipe) sends the output of <code>echo</code> as input to <code>xargs</code>.</li> <li><code>xargs ls -l</code> takes each space-separated item from the input (\"file1\", \"file2\", \"file3\") and appends them to the <code>ls -l</code> command, resulting in <code>ls -l file1 file2 file3</code>.</li> <li><code>ls -l</code> is executed with the provided arguments, listing the details of the specified files.</li> </ul> <p>This is just a basic usage example. <code>xargs</code> has many options and can be used in various complex scenarios to construct and execute command lines efficiently.</p> <p>Example</p> <p><pre><code>echo \"file1 file2 file3\" | xargs ls -l\n</code></pre> This command lists the details of files <code>file1</code>, <code>file2</code>, and <code>file3</code>.</p> Common Use Cases: <ol> <li> <p>Reading from Standard Input: <pre><code>echo \"file1 file2 file3\" | xargs rm\n</code></pre>   Removes files <code>file1</code>, <code>file2</code>, and <code>file3</code>.</p> </li> <li> <p>Passing Filenames from a File: <pre><code>cat files.txt | xargs grep \"pattern\"\n</code></pre>   Searches for the specified pattern in each file listed in <code>files.txt</code>.</p> </li> <li> <p>Limiting Arguments per Command: <pre><code>echo \"file1 file2 file3\" | xargs -n 1 rm\n</code></pre>   Removes each file one by one.</p> </li> <li> <p>Parallel Execution: <pre><code>find . -name \"*.txt\" | xargs -P 4 wc -l\n</code></pre>   Counts the lines in <code>.txt</code> files, executing up to four <code>wc -l</code> commands concurrently.</p> </li> </ol> Advanced Features: <ul> <li> <p>Null-Terminated Input: <pre><code>find . -type f -print0 | xargs -0 rm\n</code></pre>   Safely removes files with names containing spaces or special characters.</p> </li> <li> <p>Interactive Execution: <pre><code>echo \"file1 file2\" | xargs -p -n 1 rm\n</code></pre>   Prompts for confirmation before removing each file.</p> </li> <li> <p>Verbose Output: <pre><code>echo \"file1 file2\" | xargs -t rm\n</code></pre>   Prints the <code>rm</code> command being executed for each file.</p> </li> <li> <p>Specifying Maximum Processes: <pre><code>find . -name \"*.txt\" | xargs -P 4 wc -l\n</code></pre>   Limits the number of concurrent <code>wc -l</code> processes to four.</p> </li> <li> <p>Specifying Maximum Arguments per Command: <pre><code>echo \"file1 file2 file3\" | xargs -L 1 rm\n</code></pre>   Removes each file one by one by executing <code>rm</code> command for each input line.</p> </li> <li> <p>Interactive Replacement String: <pre><code>echo \"file1 file2\" | tr \" \" \"\\n\" | xargs -I {} cp {} ./backup\n</code></pre>   Copies <code>file1</code> and <code>file2</code> to <code>./backup</code> directory, replacing <code>{}</code> with each input item.   <pre><code>echo \"file1 file2\" | tr \" \" \"\\n\" | xargs -I {} cp {} ./backup/{}.bak\n</code></pre>   I can rename the files like above to <code>file1.bak</code> and <code>file2.bak</code></p> </li> <li> <p>Verbose Mode: <pre><code>echo \"file1 file2\" | xargs -t rm\n</code></pre>   Prints the <code>rm</code> command being executed for each file.</p> </li> </ul> </li> </ul> <p><code>xargs</code> is a flexible tool for constructing and executing commands with arguments from standard input or files. It's particularly useful for batch processing and handling large sets of data efficiently.</p>"},{"location":"blog/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#sed-stream-editor","title":"sed (Stream Editor)","text":"<ul> <li>Basic Syntax:</li> </ul> <pre><code>sed [options] 'command' file.txt\n</code></pre> <ul> <li><code>options</code>: Additional flags or arguments that modify the behavior of the <code>sed</code> command.</li> <li><code>command</code>: Specifies the editing operation to be performed on the input.</li> <li> <p><code>file.txt</code>: The input file to be processed by <code>sed</code>.</p> <p>Example</p> <p><pre><code>sed 's/old/new/' file.txt\n</code></pre> This command replaces the first occurrence of \"old\" with \"new\" on each line of <code>file.txt</code>.</p> Common Use Cases: <ol> <li> <p>Substitution: <pre><code>sed 's/foo/bar/g' file.txt\n</code></pre>   Substitutes all occurrences of \"foo\" with \"bar\" in <code>file.txt</code>.</p> </li> <li> <p>Deleting Lines: <pre><code>sed '/pattern/d' file.txt\n</code></pre>   Deletes lines containing \"pattern\" from <code>file.txt</code>.</p> </li> <li> <p>Printing Lines: <pre><code>sed -n '10,20p' file.txt\n</code></pre>   Prints lines 10 to 20 of <code>file.txt</code>.</p> </li> <li> <p>Inserting and Appending Text: <pre><code>sed '1i Inserted Text' file.txt\n</code></pre>   Inserts \"Inserted Text\" before the first line of <code>file.txt</code>.</p> </li> <li> <p>Using Regular Expressions: <pre><code>sed -E 's/([0-9]+)\\.([0-9]+)/\\2.\\1/' file.txt\n</code></pre>   Reverses the order of numbers separated by a dot in <code>file.txt</code>.</p> </li> </ol> Advanced Features: <ul> <li> <p>In-Place Editing: <pre><code>sed -i 's/old/new/g' file.txt\n</code></pre>   Performs in-place editing, modifying <code>file.txt</code> directly.</p> </li> <li> <p>Multiple Commands: <pre><code>sed -e 's/foo/bar/' -e 's/baz/qux/' file.txt\n</code></pre>   Executes multiple editing commands sequentially on <code>file.txt</code>.</p> </li> <li> <p>Using Hold and Pattern Space: <pre><code>sed '1h;1!H;$!d;g' file.txt\n</code></pre>   Collects all lines into the hold space and then prints them in reverse order.</p> </li> <li> <p>Conditional Execution: <pre><code>sed '/pattern/ { s/old/new/ }' file.txt\n</code></pre>   Substitutes \"old\" with \"new\" only on lines containing \"pattern\".</p> </li> </ul> </li> </ul>"},{"location":"blog/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#conclusion-basic-commands","title":"Conclusion - basic commands","text":"<p>These commands offer different functionalities:</p> <ul> <li><code>wc</code> counts lines, words, or characters in a file.</li> <li><code>du</code> estimates disk usage for files and directories.</li> <li><code>grep</code> searches for patterns in files and prints lines containing the specified pattern.</li> <li><code>awk</code> is a powerful text processing tool for pattern scanning and processing.</li> <li>The <code>find</code> command in Linux is a powerful tool used for searching files and directories based on various criteria.</li> <li><code>xargs</code>, constructs and executes commands with arguments from standard input or files</li> <li><code>sed</code>, performs various editing operations on text files, making it invaluable for automation and scripting tasks.</li> </ul> <p>You can use these commands to perform various operations related to file content, size estimation, and pattern matching within files.</p>"},{"location":"blog/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#2-combining-find-with-other-commands","title":"2. Combining find with Other Commands","text":"<p>In this section, We explore how to combine the previous commands using pipes (<code>|</code>), <code>-exec {} \\;</code>, or <code>-exec {} +</code>:</p>"},{"location":"blog/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#pipes","title":"Pipes (<code>|</code>)","text":"<p>Using pipes to pass the output of one command as input to another.</p> <ul> <li> <p>Finding specific files and counting them:</p> <pre><code>find /path/to/search -name \"*.txt\" | wc -l\n</code></pre> <p>Finds <code>.txt</code> files and counts them using <code>wc -l</code>.</p> </li> </ul>"},{"location":"blog/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#-exec-find-and-perform-an-action-on-each-file","title":"<code>-exec {} \\;</code>: Find and perform an action on each file","text":"<p>Executing a command for each matched file or directory.</p> <ul> <li> <p>Finding files and displaying their sizes:</p> <pre><code>find /path/to/search -type f -exec du -h {} \\;\n</code></pre> <p>Displays sizes of files (each file in a different command) found by <code>find</code> using <code>du -h</code>.</p> </li> <li> <p>Finding files and performing deletion:</p> <pre><code>find /path/to/search -name \"file_to_delete.txt\" -exec rm {} \\;\n</code></pre> <p>Deletes files (each file in a different command) matching the name <code>file_to_delete.txt</code>.</p> </li> <li> <p>Finding and searching patterns:</p> <pre><code>find /path/to/search -name \"*.txt\" -exec grep \"pattern\" {} \\;\n</code></pre> <p>This command finds all <code>.txt</code> files in the specified directory and runs <code>grep</code> to search for a specific pattern within each of those files.</p> </li> </ul>"},{"location":"blog/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#-exec-find-and-perform-an-action-on-all-files-at-once","title":"<code>-exec {} +</code>: Find and perform an action on all files at once","text":"<p>Optimizing efficiency by passing multiple arguments to a command.</p> <ul> <li> <p>Finding files and performing deletion:</p> <pre><code>find /path/to/search -name \"file_to_delete.txt\" -exec rm {} +\n</code></pre> <p>Deletes files (all in one command) matching the name <code>file_to_delete.txt</code></p> </li> </ul>"},{"location":"blog/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#-exec-c-option","title":"<code>-exec -c</code> option","text":"<ul> <li> <p>get and <code>.txt</code> files and change their extension to <code>.md</code> using a combination of the <code>find</code> command and <code>sed</code> (stream editor)</p> <pre><code>find /your/folder/path -type f -name \"*.txt\" -exec sh -c 'mv \"$0\" \"${0%.txt}.md\"' {} \\;\n</code></pre> <p>Here's a breakdown of what's happening</p> <ul> <li> <p><code>find /your/folder/path -type f -name \"*.txt\"</code>: This command finds all files (<code>-type f</code>) with the <code>.txt</code> extension in the specified folder and its subdirectories.</p> </li> <li> <p><code>-exec sh -c 'mv \"$0\" \"${0%.txt}.md\"' {} \\;</code>: For each file found, it executes the <code>sh</code> shell command to rename the file. <code>${0%.txt}.md</code> is a parameter expansion that replaces <code>.txt</code> with <code>.md</code> in the filename.</p> </li> </ul> <p>Make sure to replace <code>/your/folder/path</code> with the actual path to your folder.</p> </li> </ul>"},{"location":"blog/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#grouping-expressions","title":"<code>\\(</code> ... <code>\\)</code>: Grouping Expressions","text":"<p>When using <code>find</code> to search for files based on multiple criteria, such as file name patterns, types, or sizes, you may need to combine these criteria using logical operators like <code>-and</code>, <code>-or</code>, or <code>-not</code>. The <code>\\( ... \\)</code> construct allows you to group these expressions together to ensure they are evaluated as a single logical unit.</p> <p>Grouping multiple expressions together for logical operations.</p> <ul> <li> <p>Grouping Expressions in <code>find</code>:</p> <pre><code>find /path/to/search \\( -name \"*.txt\" -o -name \"*.pdf\" \\) -size +1M\n</code></pre> <p>Groups the conditions for finding files with either <code>.txt</code> or <code>.pdf</code> extensions and with a size greater than 1MB.</p> <p>Using <code>\\( ... \\)</code> allows for the proper grouping of expressions within a <code>find</code> command, ensuring that logical operations are applied correctly.</p> </li> </ul> <p>Overall, <code>\\( ... \\)</code> is a crucial construct in <code>find</code> commands for combining multiple search criteria and ensuring their proper evaluation. It helps create more complex search patterns while maintaining clarity and precision in the command syntax.</p>"},{"location":"blog/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#conclusion-combine-commands","title":"conclusion - combine commands","text":"<p><code>find</code> is an incredibly versatile command that can be combined with various flags and options to perform advanced searches based on filenames, types, sizes, modification times, and more. It's a great tool for locating specific files or performing actions on groups of files based on specific criteria.</p>"},{"location":"blog/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#3-application-showcases","title":"3. Application showcases","text":""},{"location":"blog/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#counting-files-in-a-folder","title":"Counting Files in a Folder","text":"<p>To count the number of files in a folder, you can use the following commands:</p> <ol> <li> <p>Using <code>find</code>:</p> <pre><code>find /path/to/folder -maxdepth 1 -type f | wc -l\n</code></pre> <p>More</p> <ul> <li>This command uses <code>find</code> to search for files (<code>-type f</code>) in the specified folder without going into subdirectories (<code>-maxdepth 1</code>).</li> <li>The output is then piped to <code>wc -l</code>, which counts the number of lines, effectively giving you the count of files.</li> </ul> </li> <li> <p>Using <code>ls</code>:</p> <pre><code>ls -l /path/to/folder | grep \"^-\" | wc -l\n</code></pre> <p>More</p> <p>Here, - <code>ls -l</code> lists the contents of the folder with detailed information - <code>grep \"^-\"</code> filters out only the lines that represent files (as opposed to directories or other types of items) - <code>wc -l</code> counts the number of lines, providing the count of files in the folder.</p> </li> </ol>"},{"location":"blog/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#counting-filesfolders-in-a-folder","title":"Counting Files/Folders in a Folder","text":"<pre><code>find /path/to/folder -maxdepth 1 | wc -l\n</code></pre> <p>or</p> <pre><code>ls -l /path/to/folder | wc -l\n</code></pre>"},{"location":"blog/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#determining-the-number-of-columns-in-a-csv-file","title":"Determining the Number of Columns in a CSV File","text":"<pre><code>head -n 1 input/google-form-data.csv | grep -o \",\" | wc -l\n</code></pre> <p>This command reads the first line, apply the <code>,</code> separator then count</p>"},{"location":"blog/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#finding-requirementstxt-files-containing-openpyxl","title":"Finding <code>requirements.txt</code> Files Containing \"openpyxl\"","text":"<ul> <li>find all requirements.txt files</li> </ul> <pre><code>find . -name requirements.txt\n</code></pre> <ul> <li>find all requirements.txt files who contain \"openpyxl\"</li> </ul> <pre><code>find . -name requirements.txt -exec grep -l \"openpyxl\" {} \\;\n</code></pre>"},{"location":"blog/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#utilizing-maxdepth-for-search","title":"Utilizing <code>maxdepth</code> for Search","text":"<ul> <li>find all <code>.txt</code> files non recursively</li> </ul> <pre><code>find . -maxdepth 1 -type f -name \"*.txt\"\n</code></pre> <ul> <li>find ...</li> </ul> <pre><code>find . -maxdepth 3\n</code></pre> <ul> <li>save the result</li> </ul> <pre><code>find . -maxdepth 3 &gt; output.txt\n</code></pre>"},{"location":"blog/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#skipping-certain-paths-in-a-search","title":"Skipping Certain Paths in a Search","text":"<ul> <li>find all py files but skip venv folders (paths containing venv)</li> </ul> <pre><code>find . -name \"*.py\" ! -path \"*venv*\"\n</code></pre> <ul> <li>find all py files but skip venv folders and apply yapf on each file</li> </ul> <pre><code>find . -name \"*.py\" ! -path \"*venv*\" -exec yapf --in-place {} \\;\n</code></pre> <ul> <li>find all py files but skip folders(likely env) and apply yapf on each file</li> </ul> <pre><code>find . -name \"*.py\" ! -path \"*env/Scripts*\" -exec yapf --in-place {} \\;\n</code></pre> <ul> <li>search files where the word wrappers is mentionned and avoid some folders</li> </ul> <pre><code>find . -type f -not -path '*/node_modules/*' -not -path '*env*' -not -name '*_*' -name '*.py' -exec grep -l 'wrappers' {} +\n</code></pre>"},{"location":"blog/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#git-grep-for-version-controlled-files","title":"git grep for version controlled files","text":"<ul> <li>search files where the word wrappers is mentionned withing the version controlled files</li> </ul> <pre><code>git grep -l \"wrapper\" -- \"*.py\"\n</code></pre>"},{"location":"blog/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#using-head-and-tail-commands","title":"Using <code>head</code> and <code>tail</code> Commands","text":"<ul> <li>display the last 50 lines of a file</li> </ul> <pre><code>tail -50 cli.log\n</code></pre> <ul> <li>filter the output of another command</li> </ul> <pre><code>tail -50 cli.log | grep \"/api/\"\n</code></pre> <p>This command will display the last 50 lines of the <code>cli.log</code> file and filter out only the lines that contain \"/api/\". This combination of <code>tail</code> and <code>grep</code> will help you isolate and display the relevant lines.</p> <ul> <li>install the first lines of <code>requirements.txt</code> using <code>head</code> and <code>xargs</code></li> </ul> <pre><code>head -n 18 requirements.txt | xargs -n 1 pip3 install\n</code></pre> <p>This command will read the first 18 lines of <code>requirements.txt</code>, then install each package listed there using <code>pip3</code>.</p> <p>An improvement of this command has been proposed here using <code>sed</code> to remove from the requirement file, spaces, comment, empty lines, ...</p>"},{"location":"blog/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#search-for-lines-containing-the-word-black-within-sh-files","title":"search for lines containing the word \"black\" within <code>.sh</code> files","text":"<pre><code>find /path/to/search -type f -name \"*.sh\" -exec grep -l \"black\" {} +\n</code></pre> More on find and grep options <p>This command will search for lines containing the word \"black\" within <code>.sh</code> files. The command (<code>grep</code>) displays the actual lines containing \"black\" within the files</p> <ul> <li> <p>grep options: <code>grep</code> vs <code>grep -l</code>     To only show the filenames without the matches, use the command (<code>grep</code>) instead of (<code>grep -l</code>)</p> </li> <li> <p><code>-exec</code> option in the <code>find</code> command     This syntax uses <code>+</code> at the end of the <code>-exec</code> option. It gathers the file names that match the criteria (<code>*.sh</code>) and passes them to <code>grep</code> in batches, rather than invoking <code>grep</code> once per file. This is generally more efficient, especially when dealing with a large number of files.</p> <p>To invoke <code>grep</code> individually for each file that matches the criteria (<code>*.sh</code>), use instead  <code>find /path/to/search -type f -name \"*.sh\" -exec grep -l \"black\" {} \\;</code>: This syntax uses <code>\\;</code> at the end of the <code>-exec</code> option. </p> <p>This method might be less efficient, especially for a large number of files, as it starts a new <code>grep</code> process for each file separately.</p> </li> </ul>"},{"location":"blog/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#search-for-folders","title":"search for folders","text":"<ul> <li>search folder by name (ex:all name containing eigen3)</li> </ul> <pre><code>find -type d -name \"*eigen3*\"\n</code></pre> <ul> <li>search for a specific folder like \"LAStools/bin\" starting from the root directory <code>/home</code>.</li> </ul> <pre><code>sudo find /home -type d -name \"bin\" -path \"*LAStools*\"\n</code></pre> <p>This command searches the entire root directory <code>/</code> for directories (<code>-type d</code>) named \"bin\" (<code>-name \"bin\"</code>) that are part of a path containing \"LAStools\" (<code>-path \"*LAStools*\"</code>)</p> <p>Using <code>sudo</code> might be necessary to have permission to search directories that your user account doesn't have access to by default.</p>"},{"location":"blog/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#searching-for-aaa-in-files","title":"Searching for \"AAA\" in Files","text":"<ul> <li>search for \"AAA\" in all files</li> </ul> <pre><code>find . -type f -exec grep \"AAA\" {} \\;\n</code></pre> <ul> <li>search for \"AAA\" in all files with file name and line number display</li> </ul> <pre><code>find . -type f -exec grep -Hn \"AAA\" {} \\;\n</code></pre>"},{"location":"blog/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#searching-for-a-keyword-in-files-from-the-parent-directory","title":"Searching for a keyword in Files from the parent directory","text":"<ul> <li>search for \"L337\" in all files</li> </ul> <pre><code>find .. -type f -exec grep -Hn \"L337\" {} \\;\n</code></pre> <ul> <li>search for \"L337\" in all files with 5 lines after each match</li> </ul> <pre><code>find .. -type f -exec grep -Hn \"L337\" {} \\; -A 5\n</code></pre> <p>This command will find all occurrences of \"L337\" in files within the parent directory and its subdirectories and display the filename, line number, and the line containing \"L337\", along with the five lines that follow it.</p>"},{"location":"blog/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#get-unique-lines-across-files","title":"Get unique lines across files","text":"<ul> <li>see the unique lines common to all three files without repetitions</li> </ul> <pre><code>sort file1.txt file2.txt file3.txt | uniq -c | awk '$1 == 3 {print $2}'\n</code></pre> <p>Alternative:</p> <pre><code>sort file1.txt file2.txt file3.txt | uniq -c | awk '$1 &gt;= 3 {print $2}'\n</code></pre> <ul> <li>use the precedent list to filter lines from another file (<code>people</code>)</li> </ul> <pre><code>grep -f &lt;(sort file1.txt file2.txt file3.txt | uniq -c | awk '$1 &gt;= 3 {print $2}') ../people\n</code></pre> <p>This command will first find the unique lines common to all three files, then filter those lines using <code>grep -f</code> based on the patterns present in the specified file (in this case, the file containing sorted and unique lines from <code>file1.txt</code>, <code>file2.txt</code>, and <code>file3.txt</code>). Finally, it will display the lines that match in both sets and also contain the term <code>people</code>.</p>"},{"location":"blog/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#additional-commands","title":"Additional Commands","text":"<ul> <li>extract names of females with the name 'Annabel' from the 'people' file</li> </ul> <pre><code>cat people | awk '$3 == \"F\"' | grep 'Annabel' | awk '{print $1, $2}'\n</code></pre> <p>In the project, this command filter the lines of the file <code>people</code> containing the word <code>Annabel</code> and where the person is female (3<sup>rd</sup> fiel == <code>F</code>) then use <code>awk</code> to print from the filtered file, only the first and second fields. The fields in each line are separated by a space</p> <ul> <li>search for 'Annabel' in all files and extract the names of females with the name 'Annabel'</li> </ul> <pre><code>find . -type f -exec grep -w 'Annabel' {} \\; -exec awk '$3 == \"F\" {print $1, $2}' {} \\;\n</code></pre> <p>This command will apply the precedent operation on each file returned by the <code>find</code> command</p>"},{"location":"blog/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#grouping-examples","title":"Grouping Examples","text":"Finding files with either \".txt\" or \".pdf\" extensions and with a size greater than 1MB <p>Suppose you want to find files with either \".txt\" or \".pdf\" extensions and with a size greater than 1MB. You can use <code>\\( ... \\)</code> to group the size condition with the extension conditions:</p> <pre><code>find /path/to/search \\( -name \"*.txt\" -o -name \"*.pdf\" \\) -size +1M\n</code></pre> <p>In this command:</p> <ul> <li><code>\\( -name \"*.txt\" -o -name \"*.pdf\" \\)</code> groups the conditions for finding files with \".txt\" or \".pdf\" extensions.</li> <li><code>-size +1M</code> specifies the condition for files with a size greater than 1MB.</li> </ul> <p>By grouping the extension conditions together, you ensure that the size condition is applied to both \".txt\" and \".pdf\" files.</p> Finding specific files with specific extensions <p>Suppose you want to find specific files with extensions such as \".sh\", \".md\", or \"Dockerfile\" and then search for a particular pattern within them. You can use the following command:</p> <pre><code>find /home/ubuntu/Documents/GitHub/ \\( -name \"*.sh\" -o -name \"*.md\" -o -name \"Dockerfile\" \\) -exec grep -Hn \"apt install ./mongodb-database-tools-*.deb &amp;\" {} \\;\n</code></pre> <p>In this command:</p> <ul> <li><code>\\( -name \"*.sh\" -o -name \"*.md\" -o -name \"Dockerfile\" \\)</code> groups the conditions for finding files with the specified extensions.</li> <li><code>-exec grep -Hn \"apt install ./mongodb-database-tools-*.deb &amp;\" {} \\;</code> executes the <code>grep</code> command to search for the specified pattern within each matched file.</li> </ul> <p>The <code>\\( ... \\)</code> construct is used to group the <code>-name</code> expressions together. This grouping is necessary because the <code>-o</code> operator (logical OR) has lower precedence than the implicit logical AND applied to separate <code>find</code> expressions. By using <code>\\( ... \\)</code>, you ensure that the logical OR operation is applied correctly within the grouped expressions.</p> <p>Without the grouping, the command would not function as intended because each <code>-name</code> expression would be evaluated separately, potentially leading to unexpected results.</p> running a linter script md files from a repo subfolder and the readme file in the main directory <p>To find both <code>.md</code> files in the <code>./docs</code> directory and <code>README.md</code> files in the current directory, you can use the <code>-o</code> (OR) operator along with the <code>-exec</code> option. Here's how you can do it:</p> <pre><code>find . \\( -path \"./docs\" -name \"*.md\" -o -path \"./README.md\" \\) -exec markdownlint-cli2 --fix {} +\n</code></pre> <p>In this command:</p> <ul> <li><code>.</code>: Specifies the current directory as the starting point for the <code>find</code> command.</li> <li><code>\\( ... \\)</code>: Groups conditions together.</li> <li><code>-path \"./docs\" -name \"*.md\"</code>: Specifies files with the <code>.md</code> extension in the <code>./docs</code> directory.</li> <li><code>-o</code>: Acts as the logical OR operator.</li> <li><code>-path \"./README.md\"</code>: Specifies the <code>README.md</code> file in the current directory.</li> <li><code>-exec markdownlint-cli2 --fix {} +</code>: Executes the <code>markdownlint-cli2 --fix</code> command on the found files. The <code>{}</code> is replaced by the found file names.</li> </ul> <p>This command will execute <code>markdownlint-cli2 --fix</code> on all <code>.md</code> files in the <code>./docs</code> directory and <code>README.md</code> in the current directory.</p>"},{"location":"blog/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#comment-out-some-lines-matching-a-pattern","title":"comment out some lines matching a pattern","text":"<p>To comment out all lines containing <code>font-size:</code> in a CSS file using Bash, you can use the <code>sed</code> command. <code>sed</code> is a powerful stream editor in Unix-like systems, used for performing basic text transformations on an input stream (a file or input from a pipeline).</p> <p>Here\u2019s how you can use <code>sed</code> to find all occurrences of <code>font-size:</code> and comment out those lines in a CSS file. You will use the in-place edit option <code>-i</code> to modify the file directly. Please ensure to back up your file before running such commands, as they make direct changes to your files.</p>"},{"location":"blog/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#step-by-step-command","title":"Step-by-Step Command","text":"Backup Your CSS File <p>Before running the command, it's a good idea to make a backup of your CSS file:</p> <pre><code>cp yourfile.css yourfile.backup.css\n</code></pre> <p>The following <code>sed</code> command will add <code>/*</code> and <code>*/</code> around any line that contains <code>font-size:</code>. This approach assumes that the lines do not already contain block comments (<code>/* */</code>), as nested comments are not supported in CSS and will lead to errors.</p> <pre><code>sed -i '/font-size:/ s|.*|/* &amp; */|' yourfile.css\n</code></pre> <p>Explanation:</p> <p>Here\u2019s what each part of the command does:</p> <ul> <li><code>sed -i</code>: The <code>-i</code> option edits files in-place (i.e., saves back to the original file).</li> <li><code>'/font-size:/</code>: Selects lines that match the pattern <code>font-size:</code>.</li> <li><code>s|.*|/* &amp; */|</code>: Replaces the entire line (<code>.*</code> matches everything) with <code>/*</code> followed by the original line (<code>&amp;</code> represents the matched line), followed by <code>*/</code>.</li> </ul> Alternative Command <p>If you just want to prepend <code>//</code> to lines instead of wrapping them with <code>/* */</code>, assuming this is for a CSS-like language that supports <code>//</code> comments (note: standard CSS does not support <code>//</code> for commenting), you could do:</p> <pre><code>sed -i '/font-size:/ s|^|// |' yourfile.css\n</code></pre> <p>This command adds <code>//</code> at the start of any line that contains <code>font-size:</code>.</p> Testing the Command <p>It's a good practice to test the command on a sample file or with a copy of your file to ensure it performs as expected without modifying the original file:</p> <pre><code>sed '/font-size:/ s|.*|/* &amp; */|' yourfile.css &gt; test_output.css\n</code></pre> <p>This command applies the changes and redirects the output to <code>test_output.css</code> instead of altering the original file.</p> <p>These commands should help you automatically comment out all lines containing <code>font-size:</code> in your CSS file using Bash.</p>"},{"location":"blog/mastering-essential-linux-commands-your-path-to-file-and-directory-mastery/#conclusion","title":"Conclusion","text":"<p>These commands, when mastered and strategically combined, offer a robust toolkit for proficiently managing and manipulating files and directories in a Linux environment. By leveraging these commands in tandem, users can perform intricate searches, conduct comprehensive analyses, and execute operations swiftly, significantly enhancing productivity and workflow efficiency.</p>"},{"location":"blog/run-an-application-forever-on-linux-made-easy-case-of-a-java-script/","title":"Run an application forever on linux made easy: Case of a java script","text":""},{"location":"blog/run-an-application-forever-on-linux-made-easy-case-of-a-java-script/#introduction","title":"Introduction","text":"<p>If you're looking to turn your application into a background process, you have come to the right tutorial, always using the fastest way.</p> <p>Instead of just writing theory, we we use a real world example i've worked on.</p> <p>To run a Java application as a background process and keep it running forever, you can use a process manager like <code>systemd</code> on Linux. Here's how you can set up a <code>systemd</code> service to run your Java application:</p> <p>Certainly! Here are the steps named as per their actions:</p>"},{"location":"blog/run-an-application-forever-on-linux-made-easy-case-of-a-java-script/#step-1-create-service-file","title":"Step 1: Create Service File","text":"<p>Create a new systemd service file for your Java application using a text editor:</p> <pre><code>sudo nano /etc/systemd/system/myapp.service\n</code></pre>"},{"location":"blog/run-an-application-forever-on-linux-made-easy-case-of-a-java-script/#step-2-configure-service","title":"Step 2: Configure Service","text":"<p>Paste the following configuration into the file, replacing <code>&lt;jar-file-name&gt;</code> with the name of your JAR file:</p> <pre><code>[Unit]\nDescription=My App\n\n[Service]\nUser=myuser\nExecStart=/usr/bin/java -jar /path/to/myapp/&lt;jar-file-name&gt;.jar\nSuccessExitStatus=143\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>"},{"location":"blog/run-an-application-forever-on-linux-made-easy-case-of-a-java-script/#step-3-save-and-exit","title":"Step 3: Save and Exit","text":"<p>Save the file and exit the text editor.</p>"},{"location":"blog/run-an-application-forever-on-linux-made-easy-case-of-a-java-script/#step-4-reload-daemon","title":"Step 4: Reload Daemon","text":"<p>Reload the systemd daemon to pick up the new service file:</p> <pre><code>sudo systemctl daemon-reload\n</code></pre>"},{"location":"blog/run-an-application-forever-on-linux-made-easy-case-of-a-java-script/#step-5-start-service","title":"Step 5: Start Service","text":"<p>Start the new <code>myapp</code> service:</p> <pre><code>sudo systemctl start myapp\n</code></pre>"},{"location":"blog/run-an-application-forever-on-linux-made-easy-case-of-a-java-script/#step-6-check-service-status","title":"Step 6: Check Service Status","text":"<p>Check the status of the service to make sure it's running:</p> <pre><code>sudo systemctl status myapp\n</code></pre> <p>If everything is set up correctly, you should see output indicating that the service is running. To stop the service, you can use the <code>sudo systemctl stop myapp</code> command.</p> <p>With this setup, your Java application will run as a background process and automatically start when the server boots up. If the application crashes or stops for any reason, <code>systemd</code> will automatically restart it.</p>"},{"location":"blog/my-ubuntu-setup-from-barebones-to-a-productive-dev-environment/","title":"My Ubuntu Setup: From Barebones to a Productive Dev Environment","text":"<p>When I install a fresh Linux distribution like Ubuntu 24, I'm not just setting up a computer; I'm building a personal workspace. My goal is a clean, reliable, and portable development environment that can be replicated anywhere, from my local machine to a remote VPS.</p>      Image generated using Google Gemini    <p>This isn't just about a list of commands, it's a philosophy of intentionality. Every tool I choose, and every tool I leave behind, is a step toward a more efficient and less cluttered workflow.</p> <p>This is my journey to a perfect setup, and it all starts with version control.</p>"},{"location":"blog/my-ubuntu-setup-from-barebones-to-a-productive-dev-environment/#the-first-steps-git-and-ssh-the-key-to-everything","title":"The First Steps: Git and SSH, The Key to Everything","text":"<p>First things first: Git. It's the cornerstone of my development life. I don't just use Git for code; I use it to version control documents, configurations, and even this very blog post. The ability to track changes and collaborate is a non-negotiable.</p> <p></p>      The Git command in a terminal window, courtesy of Google Gemini Image generator  <p>So, the very first command I run after updating my packages is to install Git:</p> <pre><code>sudo apt install git\n</code></pre> <p>Next, I need a secure, password-free way to connect to my Git hosting service. I used to rely on tools like GitHub Desktop, but I've found that nothing beats the raw power and security of SSH. It's the standard for server access, and it's the most practical way to interact with my repositories when I'm working on a remote machine.</p> <p>I follow the official guideline to generate a new key pair:</p> <pre><code>ssh-keygen -t ed25519 -C \"my-email@example.com\"\n</code></pre> <p>When prompted, I provide a clear, descriptive filename, like <code>~/.ssh/id_ed25519_github</code>. This is a small habit, but it's a lifesaver when you end up managing keys for multiple services like GitHub, GitLab, AWS ElasticBeanStalk cli, Azure Dev or a company's internal servers. The public key is then copied and added to my GitHub account. A quick test confirms everything is working:</p> <pre><code>ssh -T git@github.com\n</code></pre> <p>Now, my machine is securely and permanently linked to my Git repositories, and I'm ready for the next, most crucial step.</p>"},{"location":"blog/my-ubuntu-setup-from-barebones-to-a-productive-dev-environment/#the-docker-revolution-ditching-local-installs-for-a-clean-slate","title":"The Docker Revolution: Ditching Local Installs for a Clean Slate","text":"<p>This is where my philosophy truly diverges from the norm. Many developers install tools directly on their system using <code>apt</code>, <code>snap</code>, or <code>flatpak</code>. But even with these tools, you're still accumulating dependencies, caches, and configuration files that can lead to a bloated, messy system over time.</p> <p>I choose a different path. I use Docker as my primary application manager.</p> <p>Imagine a pristine, empty desktop. Now, imagine a magical box that contains all your applications and their messy dependencies, neatly sealed away. When you need a tool, you open the box. When you're done, you close it, and all the mess vanishes without a trace. That's Docker for me.</p> <p></p>      A stylized Docker whale with containers, courtesy of Google Gemini Image generator  <p>My applications are completely isolated, running in lightweight containers. This means I can run Node.js, Python, MongoDB, or any other tool without ever touching my host system's libraries. It eliminates version conflicts and ensures that my environment is always reproducible.</p> <p>The installation is straightforward, following the official Docker documentation.</p> <p>After a quick test with <code>sudo docker run hello-world</code>, I perform the single most important post-installation step: managing Docker as a non-root user.</p> <pre><code>sudo groupadd docker          # Create the docker group\nsudo usermod -aG docker $USER # Add my user to the group\nreboot                        # A restart is necessary to apply the changes\n</code></pre> <p>After the reboot, I can run <code>docker run hello-world</code> without <code>sudo</code>, and my user has full control over Docker. The real power of this approach comes from the next step: aliases.</p> <p>Read here for a deep dive into docker in practice</p>"},{"location":"blog/my-ubuntu-setup-from-barebones-to-a-productive-dev-environment/#the-power-of-aliases-docker-apps-at-my-fingertips","title":"The Power of Aliases: Docker Apps at My Fingertips","text":"<p>My <code>~/.bashrc</code> file is my command center. It contains a collection of aliases that let me use Dockerized applications as if they were natively installed. This is where the magic happens.</p>"},{"location":"blog/my-ubuntu-setup-from-barebones-to-a-productive-dev-environment/#general-utility-aliases","title":"General &amp; Utility Aliases","text":"<p>First, some quality-of-life aliases for day-to-day work:</p> <pre><code># Copy the current directory to clipboard (useful for pasting into file dialogs or a browser)\nalias cppwd=\"pwd | xclip -selection clipboard\"\n\n# Change directory to the path copied to clipboard\nalias cdpwd='cd \"$(xclip -selection clipboard -o)\"'\n\n# An alias for the graphical text editor\nalias gedit2='gnome-text-editor'\n\n# --- Navigation and Listing ---\n\n# Navigate up two directories\nalias ..='cd ..'\nalias ...='cd ../..'\n\n# Human-readable disk usage and free space\nalias du='du -h'\nalias df='df -h'\n\n# A more readable 'ls' command: long format, human-readable sizes, and shows all files\nalias ll='ls -lah'\n\n# --- Git Shortcuts ---\n\n# Git status: a command I run a dozen times a day\nalias gs='git status'\n\n# Git add all\nalias ga='git add .'\n\n# Git commit with a quick message\nalias gc='git commit -m'\n\n# Git push\nalias gp='git push'\n\n# Git pull\nalias gpl='git pull'\n\n# --- System and Updates ---\n\n# Full system update and upgrade in one command\nalias sup='sudo apt update &amp;&amp; sudo apt upgrade -y'\n\n# My version of 'clear', which clears the terminal and then shows my home directory\nalias cl='clear &amp;&amp; ls'\n</code></pre>"},{"location":"blog/my-ubuntu-setup-from-barebones-to-a-productive-dev-environment/#dockerized-aliases-from-mongo-to-markdown","title":"Dockerized Aliases: From Mongo to Markdown","text":"<p>Here's a look at how I manage specific tools. I don't install the MongoDB shell locally. Instead, I pull a lightweight Alpine image and run it via an alias:</p> <pre><code>alias mongosh=\"sudo docker run -ti --rm alpine/mongosh mongosh\"\n</code></pre> <p>This ensures I'm always using the latest version of <code>mongosh</code> without any local installation. I do the same for <code>mongoexport</code> and <code>mongoimport</code>, even creating a persistent volume to handle files:</p> <pre><code>alias mongoexport=\"mkdir -p ./out &amp;&amp; sudo docker run -v ./out:/run -ti --rm alpine/mongosh mongoexport\"\nalias mongoimport='[ -d ./out ] || { echo \"./out directory not found\" &gt;&amp;2; exit 1; } &amp;&amp; sudo docker run -v ./out:/run -ti --rm alpine/mongosh mongoimport'\n</code></pre>"},{"location":"blog/my-ubuntu-setup-from-barebones-to-a-productive-dev-environment/#nodejs-development-without-the-pain","title":"Node.js Development Without the Pain","text":"<p>I used to be a heavy user of <code>nvm</code> to manage multiple Node.js versions. But even <code>nvm</code> adds complexity and can sometimes break. My solution? One simple <code>docker run</code> command for my entire development workflow.</p> <pre><code>sudo docker run --rm -p 3005:3001 -v $PWD:/app -w /app node:18 sh -c \"npm install &amp;&amp; npm run dev\"\n</code></pre> <p>This single command mounts my project folder, installs dependencies, and runs the dev server in a clean, isolated environment. I apply the same logic to linters and formatters like Prettier and markdownlint-cli2:</p> <pre><code>alias prettier-npx='sudo docker run --rm -v \"$PWD\":/work -w /work node:22 npx prettier'\nalias markdownlint-cli2='sudo docker run --rm -v \"$PWD\":/work -w /work node:22 npx markdownlint-cli2'\n</code></pre>"},{"location":"blog/my-ubuntu-setup-from-barebones-to-a-productive-dev-environment/#embracing-markdown-and-latex-with-docker","title":"Embracing Markdown and LaTeX with Docker","text":"<p>I've fallen in love with Markdown and its rich ecosystem. Unlike proprietary word processors, Markdown files are plain text, making them perfect for Git. I've even created aliases for tools like Pandoc or Grip, a GitHub-like markdown viewer, which I run from a Docker container:</p> <pre><code>alias grip='sudo docker run --rm -it -v \"$PWD\":/home -p 6419:6419 -w /home python:3.11-slim bash -c '\\''pip install grip &amp;&amp; file=$1 &amp;&amp; shift &amp;&amp; grip \"$file\" 0.0.0.0 \"$@\"'\\'' --'\n</code></pre> <p>For professional or academic documents, nothing beats LaTeX. I remember being blown away when I first discovered it\u2014the idea of coding a document. But a full TeX Live installation can be enormous!</p> <p>My solution, of course, is a Docker container. I created minimal images with just the tools I need, like <code>latexindent</code> and <code>pdflatex</code>, to keep the size down.</p> <p>Dockerfile for <code>latexindent</code>:</p> <pre><code>FROM debian:bookworm-slim\nLABEL maintainer=\"Hermann Agossou &lt;agossouhermann7@gmail.com&gt;\"\nLABEL description=\"A lightweight Docker image for running latexindent\"\n\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y texlive-extra-utils perl &amp;&amp; \\\n    apt-get clean &amp;&amp; \\\n    rm -rf /var/lib/apt/lists/*\n\nWORKDIR /data\nENTRYPOINT [\"latexindent\"]\n</code></pre> <p>I build this once, then use it as needed:</p> <pre><code>alias latexindent='sudo docker run --rm -v \"$(pwd)\":/data -w /data ahlk/latexindent'\n</code></pre> <p>Dockerfile for <code>pdflatex</code>:</p> <pre><code># Use a lightweight base image with TeX Live\nFROM debian:bookworm-slim\n\n\n# Set metadata for the image\nLABEL maintainer=\"Hermann Agossou &lt;agossouhermann7@gmail.com&gt;\"\nLABEL description=\"A lightweight Docker image for running pdflatex\"\n\n# Install pdflatex and clean up to minimize image size\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y --no-install-recommends \\\n    texlive-latex-base \\\n    texlive-fonts-recommended \\\n    texlive-fonts-extra \\\n    texlive-latex-extra &amp;&amp; \\\n    apt-get clean &amp;&amp; \\\n    rm -rf /var/lib/apt/lists/*\n\n# Set the working directory inside the container\nWORKDIR /data\n\n# Default command to run pdflatex\nENTRYPOINT [\"pdflatex\"]\n</code></pre> <p>I build this once, then use it as needed:</p> <p>For example,</p> <pre><code>alias pdflatex = 'docker run --rm --volume \"$PWD:/data\" pdflatex main.tex'\n</code></pre> <p>This approach extends to my most powerful document workflow: converting complex Markdown files to beautiful PDFs using Pandoc. For simple conversions, the default <code>pandoc/latex</code> image is fine, but for academic writing, I need more advanced features like glossaries, mini-tables of contents (<code>minitoc</code>), and elegant drop caps (<code>lettrine</code>). The <code>pandoc/extra</code> image is a great starting point, but it's still missing some key packages.</p> <p>So, I wrote my own custom Dockerfile to build on top of it:</p> <pre><code>FROM pandoc/extra\n\nRUN tlmgr update --self &amp;&amp; \\\n    tlmgr install glossaries minitoc lettrine\n\nENTRYPOINT [\"pandoc\"]\n</code></pre> <p>I build this image once, and it becomes my ultimate document conversion tool. By using a customized Eisvogel template and custom Lua filters, I can transform a simple Markdown file or a complex file structure into a professional PDF that rivals anything created with a proprietary word processor. I access this powerful tool with a simple alias:</p> <pre><code>alias pandock-more-extras='sudo docker run --rm -v \"$(pwd):/data\" -w /data -u $(id -u):$(id -g) pandoc/more-extras'\n</code></pre>"},{"location":"blog/my-ubuntu-setup-from-barebones-to-a-productive-dev-environment/#the-clean-slate-philosophy-keeping-docker-tidy","title":"The Clean Slate Philosophy: Keeping Docker Tidy","text":"<p>One of the greatest benefits of the Docker-first approach is how easy it is to maintain a clean system. With traditional installations, applications leave behind caches, configuration files, and dependencies that clutter your storage over time. Docker simplifies this dramatically.</p> <p>When I'm done with a container or a project, I can instantly reclaim disk space by removing all unused resources. The <code>docker system prune</code> command is a powerful ally in this.</p> <pre><code>docker system prune -a\n</code></pre> <p>This single command sweeps away all stopped containers, unused networks, dangling images, and build cache layers. It's the ultimate cleanup tool, ensuring that your system remains lean and performant. And if I need that tool again? No problem. The next time I run my alias, Docker will automatically pull the required image, and I'll be back to work in seconds. This cycle of use, prune, and reuse is the essence of the clean, intentional workflow I've built.</p> <p>My Ubuntu setup isn't just about a list of commands; it\u2019s a commitment to a clean, modular, and portable development environment. By embracing Git for version control and Docker as my application manager, I\u2019ve built a workspace that is both powerful and blissfully simple. I invite you to try this approach\u2014it just might change the way you think about your system.</p>"},{"location":"blog/grip-a-github-like-markdown-viewer-in-your-computer/","title":"grip: A github-like markdown viewer in your computer","text":""},{"location":"blog/grip-a-github-like-markdown-viewer-in-your-computer/#introduction","title":"Introduction","text":"<p>grip is a cli tool that you run in a directory from the terminal. He can parse you mardown files like github do.</p> But How that works and how to use it ?  <p>He access (endpoint based) all files from the repo and parse markdown files after sending them to github unless you use the offline renderer.</p>"},{"location":"blog/grip-a-github-like-markdown-viewer-in-your-computer/#alternatives","title":"Alternatives","text":"<ul> <li>VScode Extension markdown preview enhanced: not working on ubuntu (see ref to solution), i use grip</li> </ul>"},{"location":"blog/grip-a-github-like-markdown-viewer-in-your-computer/#prereqsites","title":"Prereqsites","text":"<ul> <li>python: you can install the latest version</li> </ul>"},{"location":"blog/grip-a-github-like-markdown-viewer-in-your-computer/#installation","title":"Installation","text":"<p>Install it with <code>pip install grip</code></p> As simple as that !"},{"location":"blog/grip-a-github-like-markdown-viewer-in-your-computer/#usage","title":"Usage","text":"<pre><code>cd path/to/project\n</code></pre> <ul> <li>run readme.md as web app</li> </ul> <pre><code>grip \n</code></pre> <ul> <li>or run another file</li> </ul> <pre><code>grip my-file-name.md\n</code></pre>"},{"location":"blog/grip-a-github-like-markdown-viewer-in-your-computer/#exports","title":"Exports","text":""},{"location":"blog/grip-a-github-like-markdown-viewer-in-your-computer/#export-to-pdf-or-html","title":"export to pdf or html","text":"<p>You can export you file to pdf or html. To use this feature, add <code>--export</code> option followed by the filename (with html or pdf extension)</p> <p>troubleshots</p> <ul> <li>he can export with <code>grip my-file-name.md --export inut.pdf</code> but there is a bug</li> <li>so i installed with <code>sudo apt install grip</code> but it takes forever</li> <li>anyway, the web view is cool. And if i neeeed pdf, from this discussion, i can use:</li> <li>windows: the extension <code>markdown preview enhanced</code> works there fine</li> <li>mardown-pdf from npm</li> <li>print the page from the webview and select non-empty pages. The pb is the ref links: the ref inside text to biblio will try to lead the grip webview that will not be open if not run on cli</li> </ul>"},{"location":"blog/grip-a-github-like-markdown-viewer-in-your-computer/#rate-limiting","title":"rate limiting","text":"<p><code>grip</code> have a rate limit of 60 requests/hour. So each time you save your work and the grip server is running, you lose one request. And it you use VSCode auto-save, you're kind of screwed. Prefer either</p> <ul> <li>not to save, unless you finish modifications</li> <li>not to run grip while editing</li> <li>or add a <code>--norefresh</code> option</li> </ul> <p>You can have 5000 requests/hour if you add option --user with your credentials like this</p> <pre><code>grip my_file.md --user hermann-web:&lt;my-token&gt;\n</code></pre>"},{"location":"blog/grip-a-github-like-markdown-viewer-in-your-computer/#offline-renderer","title":"Offline renderer","text":"<p>Note</p> <p>There is an offline renderer but it didn't make 2.0 release.</p> <p>When it will be available, it quite is useful if</p> <ul> <li>you don't have internet connection</li> <li>or have have issue about sending your sensitive files to github-microsoft You can use it like this</li> </ul> <pre><code>grip my_file.md --ofline-renderer\n</code></pre>"},{"location":"blog/grip-a-github-like-markdown-viewer-in-your-computer/#cool-features","title":"Cool features","text":"<ul> <li>add <code>-b</code> if you want it to open a browser tab for you</li> <li><code>--quiet</code> to avoid printing in the terminal Use <code>--help</code> to see more of this</li> </ul>"},{"location":"blog/pandoc-convert-most-files-without-online-services/","title":"pandoc: convert most files without online services","text":""},{"location":"blog/pandoc-convert-most-files-without-online-services/#introduction","title":"Introduction","text":"<p>Pandoc is a versatile document conversion tool that can convert Markdown documents to PDF, HTML, Word DOCX, and many other formats. Pandoc provides a wide range of options to customize the output of the converted document. Here is a list of some of the most commonly used options:</p> <ul> <li><code>-s</code>: Create a standalone document with a header and footer.</li> <li><code>-o</code>: Specify the output file name.</li> <li><code>--from</code>: Specify the input format explicitly.</li> <li><code>--to</code>: Specify the output format explicitly.</li> </ul> <ul> <li><code>-V/--variable</code>: Set a template variable when rendering the document in standalone mode.</li> <li><code>--defaults</code>: Specify a package of options in the form of a YAML file.</li> <li><code>--list-input-formats</code>: Print a list of supported input formats.</li> <li><code>--list-output-formats</code>: Print a list of supported output formats.</li> <li><code>--list-highlight-styles</code>: Print a list of supported syntax highlighting styles.</li> <li><code>-f</code>: Specify the input format.</li> <li><code>-t</code>: Specify the output format.</li> </ul> <p>For a full list of options, see the Pandoc User's Guide<sup>1</sup> or the Pandoc manual <sup>2</sup>.</p>"},{"location":"blog/pandoc-convert-most-files-without-online-services/#installing-pandoc","title":"Installing pandoc","text":"<ul> <li>Installing Pandoc on Ubuntu remove any previous versions of Pandoc.</li> </ul> <pre><code>sudo apt-get purge --auto-remove pandoc\n</code></pre> <ul> <li>Download the latest version of Pandoc from the Pandoc GitHub releases page for example,</li> </ul> <pre><code>wget https://github.com/jgm/pandoc/releases/download/3.1.8/pandoc-3.1.8-1-arm64.deb\n</code></pre> <ul> <li>Install the downloaded package by running the command sudo dpkg -i pandoc--1-amd64.deb, replacing  with the version number of the package you downloaded. for example, <pre><code>dpkg -i pandoc-3.1.8-1-arm64.deb\n</code></pre> <ul> <li>Run the following command</li> </ul> <pre><code>pandoc -f markdown -t latex input.md -o output.tex\n</code></pre> <p>where <code>input.md</code> is the name of your markdown file and <code>output.tex</code> is the name you want to give to the resulting LaTeX file.</p> <p>[For a hard markdown user like me, pandoc is a big time relief as i can note in markdown, store mostly markdown files and still, receiving and sharing files in proprietary like pdf, docx, ...]</p>"},{"location":"blog/pandoc-convert-most-files-without-online-services/#get-your-options","title":"get your options","text":"<p>You can convert from any to any. To see available input (-f) format, use <code>pandoc --list-input-formats</code> To see available output (-t) format, use <code>pandoc --list-output-formats</code></p> <pre><code>pandoc -f markdown -t latex input.md -o output.tex\n</code></pre> <p>where <code>input.md</code> is the name of your markdown file and <code>output.tex</code> is the name you want to give to the resulting LaTeX file.</p>"},{"location":"blog/pandoc-convert-most-files-without-online-services/#quick-examples","title":"Quick examples","text":"<ul> <li>md to docx</li> </ul> <pre><code>pandoc my_file.md -s -t docx -o my_file.docx\n</code></pre> <p><code>-s</code> is for standalone, <code>-o</code> to specify output file path, <code>-t</code> to specify output format (but no need as he guess from output file format given)</p> <ul> <li>md to tex</li> </ul> <pre><code>pandoc my_file.md -s -t latex -o my_file.tex\n</code></pre> <ul> <li>one md to pdf</li> </ul> <pre><code>pandoc my_file.md -o my_file.pdf\n</code></pre> <ul> <li>if there is an encoding problem</li> </ul> <pre><code>pandoc my_file.md -o my_file.pdf --pdf-engine=lualatex\n</code></pre> <p>found in this stackexchange discussion</p> <ul> <li>many md to pdf</li> </ul> <pre><code>pandoc *.md -o markdown_book.pdf \n</code></pre> <p>found in this stackoverflow discussion</p> <ul> <li>many md to pdf accross folders</li> </ul> <pre><code>pandoc *.md */*.md -o markdown_book.pdf --pdf-engine=lualatex\n</code></pre> <p>note that images not accessible from the current directory will not be parsed</p> <ul> <li>add automatic section numbering like in latex</li> </ul> <pre><code>pandoc my_file.md -s -o my_file.pdf --number-sections\n</code></pre>"},{"location":"blog/pandoc-convert-most-files-without-online-services/#cool-features-in-md-to-pdf","title":"Cool features in md to pdf","text":"<p>from is a great blog in this <sup>3</sup> There are several cool options available when converting Markdown to PDF using Pandoc. Here are some of them:</p> <ol> <li><code>--toc</code>: Adds a table of contents to the beginning of the PDF that links to the various sections of the document.</li> <li><code>--template</code>: Allows you to use a custom LaTeX template to modify the appearance of the PDF.</li> <li><code>--variable</code>: Allows you to set variables that can be used in the LaTeX template. For example, you can set the font size or color of the text.</li> <li><code>--highlight-style</code>: Allows you to set the syntax highlighting style for code blocks.</li> <li><code>--number-sections</code>: Numbers the sections of the document.</li> <li><code>--metadata</code>: Allows you to set metadata for the PDF, such as the title, author, and date.</li> <li><code>-f markdown-implicit_figures</code>: ...</li> </ol> <p>For example, here is a command that uses some of these options:</p> <pre><code>pandoc input.md -o output.pdf --toc --template=mytemplate.tex --variable=fontsize:12pt --highlight-style=pygments --number-sections --metadata=title:\"Document\" --metadata=author:\"Hermann Agossou\"\n</code></pre> <p>This command adds a table of contents, uses a custom LaTeX template called <code>mytemplate.tex</code>, sets the font size to 12pt, uses the Pygments syntax highlighting style, numbers the sections, and sets the title and author metadata for the PDF.</p>"},{"location":"blog/pandoc-convert-most-files-without-online-services/#adding-footnote-citations-in-markdown-files-for-pandoc-pdf-conversion","title":"Adding Footnote Citations in Markdown Files for Pandoc PDF Conversion","text":"<p>This section is about adding footnote citations using Pandoc and referencing a BibTeX file</p>"},{"location":"blog/pandoc-convert-most-files-without-online-services/#1-creating-a-bibtex-file","title":"1. Creating a BibTeX File","text":"<p>In a separate BibTeX file (e.g., <code>references.bib</code>), store your references in the BibTeX format. Here's an example:</p> <pre><code>@online{las-1,\n  author       = \"{ArcGIS}\",\n  title        = \"{Storing lidar data}\",\n  howpublished = \"\\url{https://desktop.arcgis.com/fr/arcmap/latest/manage-data/las-dataset/storing-lidar-data.htm}\",\n}\n\n@online{las-2,\n  author       = \"{American Society for Photogrammetry and Remote Sensing}\",\n  title        = \"{LAS specification, version 1.4 \u2013 R13}\",\n  date         = \"2013-07-15\",\n  url          = \"https://www.asprs.org/wp-content/uploads/2019/07/LAS_1_4_r15.pdf\",\n}\n</code></pre>"},{"location":"blog/pandoc-convert-most-files-without-online-services/#1-formatting-citations-in-markdown","title":"1. Formatting Citations in Markdown","text":"<p>To include citations in your Markdown file for conversion to PDF using Pandoc, use the <code>[@citation]</code> format within the text where you want the citation to appear. Here is an example:</p> <pre><code>Here is a statement requiring citation [@las-2].\n</code></pre> <p>Or you can also list the references. they will be parsed as regular mardown content</p> <pre><code>Here is a statement requiring citation [@las-2].\n\n...\n\n# References\n\n[@las-1]: ArcGIS. \"Storing lidar data.\" [Link](https://desktop.arcgis.com/fr/arcmap/latest/manage-data/las-dataset/storing-lidar-data.htm)\n\n[@las-2]: American Society for Photogrammetry and Remote Sensing. \"LAS specification, version 1.4 \u2013 R13.\" [Link](https://www.asprs.org/wp-content/uploads/2019/07/LAS_1_4_r15.pdf)\n</code></pre>"},{"location":"blog/pandoc-convert-most-files-without-online-services/#3-using-pandoc-for-conversion-to-pdf","title":"3. Using Pandoc for Conversion to PDF","text":"<p>When converting the Markdown file to PDF using Pandoc, include the following options:</p> <ul> <li><code>--citeproc</code>: Enables citation processing.</li> <li><code>--bibliography</code>: Specifies the path to your bibliography file.</li> </ul> <p>Use the following command:</p> <pre><code>pandoc myfile.md -s --citeproc --bibliography=references.bib -o output.pdf\n</code></pre> <p>Replace <code>myfile.md</code> with the name of your Markdown file and <code>references.bib</code> with the actual name of your bibliography file.</p> <p>Pandoc will process the citations marked in <code>[@citation]</code> format within your Markdown file and generate the corresponding footnotes or bibliography entries in the resulting PDF.</p> <p>Remember to adjust the citation style and bibliography file as per your requirements. https://pandoc.org/chunkedhtml-demo/8.20-citation-syntax.html</p>"},{"location":"blog/pandoc-convert-most-files-without-online-services/#footnote-citations","title":"footnote citations","text":"<p>You still can use <code>[^]</code> based citations. There will appear at the end of each page, not at the end of the file.</p> <p>Examples</p> <pre><code>[^citation-1]: Full citation details here.\n[^citation-2]: https://pandoc.org/chunkedhtml-demo/8.20-citation-syntax.html\n[^citation-3]: [Full citation details here.](https://pandoc.org/chunkedhtml-demo/8.20-citation-syntax.html)\n</code></pre> <p>You can see more on citation styles with <sup>4</sup></p>"},{"location":"blog/pandoc-convert-most-files-without-online-services/#resize-image-in-markdown","title":"Resize image in markdown","text":"<p>For example, if you want the image to take 50% of the page wifth, use</p> <pre><code>![Caption text](/path/to/image){ width=50% }\n</code></pre>"},{"location":"blog/pandoc-convert-most-files-without-online-services/#related-links","title":"Related links","text":"<ul> <li>Introducing Two New Packages for Streamlining File Conversions in Python</li> </ul> <ol> <li> <p>Pandoc Manual - pandoc.org \u21a9</p> </li> <li> <p>Pandoc General Writer Options - pandoc.org \u21a9</p> </li> <li> <p>Converting Markdown to Beautiful PDF with Pandoc - jdhao.github.io \u21a9</p> </li> <li> <p>more on citation styles \u21a9</p> </li> </ol>"},{"location":"blog/a-beginners-guide-to-grpc-with-python/","title":"A Beginner's Guide to gRPC with Python","text":""},{"location":"blog/a-beginners-guide-to-grpc-with-python/#introduction-to-grpc","title":"Introduction to gRPC","text":"<p>Have you heard of gRPC, high-performance, open-source framework that allows developers to build distributed systems and microservices ?</p> <p>gPRC uses protocol buffers as its interface definition language and provides features such as bi-directional streaming and flow control.</p> <p>In this blog post, we will explore how to get started with gRPC in Python using the official gRPC Python library. We will walk through a simple working example that demonstrates how to:</p> <ol> <li>Define a service in a <code>.proto</code> file</li> <li>Generate server and client code using the protocol buffer compiler</li> <li>Use the Python gRPC API to write a simple client and server for your service</li> </ol>"},{"location":"blog/a-beginners-guide-to-grpc-with-python/#advantages-of-grpc","title":"Advantages of gRPC","text":"<p>gRPC offers several advantages, making it a versatile and efficient choice for building distributed systems and microservices:</p> <ul> <li>Language Independence: gRPC supports multiple languages seamlessly, allowing developers to build distributed systems using their preferred programming language.</li> <li>Open Source &amp; Multilingual Support: Being open source, gRPC enjoys support across various programming languages, making it a widely adopted solution for building distributed systems.</li> <li>Boilerplate Elimination: gRPC generates code, reducing the need for boilerplate code and simplifying the development process.</li> <li>Efficient Data Encoding: gRPC utilizes buffers instead of JSON for data encoding, resulting in lighter data transmission.</li> </ul>"},{"location":"blog/a-beginners-guide-to-grpc-with-python/#getting-started-with-grpc-in-python","title":"Getting Started with gRPC in Python","text":""},{"location":"blog/a-beginners-guide-to-grpc-with-python/#quick-setup","title":"Quick Setup","text":"<p>Follow the steps below to set up a Python environment for gRPC <sup>1</sup>:</p> <ol> <li>Quick Setup:</li> </ol> <pre><code>cd path/to/my/folder\npython -m venv .venv\nsource .venv/bin/activate\npython -m pip install --upgrade pip\n</code></pre> <ol> <li>Install gRPC and gRPC Tools:</li> </ol> <pre><code>python -m pip install grpcio\npython -m pip install grpcio-tools\n</code></pre> <ul> <li>gRPC Tools Include:<ul> <li><code>protoc</code>, the buffer compiler</li> <li>A plugin to generate client and server-side code from <code>.proto</code> files.</li> </ul> </li> </ul>"},{"location":"blog/a-beginners-guide-to-grpc-with-python/#testing-an-example","title":"Testing an Example","text":"<p>Clone the gRPC repository to access a sample project:</p> <pre><code>git clone -b v1.60.0 --depth 1 --shallow-submodules https://github.com/grpc/grpc\n</code></pre> <p>Run the server in one terminal:</p> <pre><code>cd grpc/examples/python/helloworld\npython greeter_server.py\n</code></pre> <p>The output</p> Output <pre><code>Server started, listening on 50051\n</code></pre> <p>Run the client in another:</p> <pre><code>cd grpc/examples/python/helloworld\npython greeter_client.py\n</code></pre> Output <pre><code>Will try to greet world ...\nGreeter client received: Hello, you!\n</code></pre> <p>Congratulations! You've run your first gRPC application!</p>"},{"location":"blog/a-beginners-guide-to-grpc-with-python/#what-the-code-does","title":"What the code does","text":"<p>The provided code includes a Protocol Buffers (protobuf) definition, a server-side implementation in Python, and a client-side implementation in Python. The protobuf definition defines a <code>Greeter</code> service with three RPC methods: <code>SayHello</code>, and <code>SayHelloStreamReply</code>. The server-side implementation defines the behavior of the <code>SayHello</code> method, while the client-side implementation makes use of these methods to communicate with the server.</p> <p>The <code>helloworld.proto</code> file defines the <code>Greeter</code> service with three RPC methods. The <code>greeter_server.py</code> file implements the server for the <code>Greeter</code> service, and the <code>greeter_client.py</code> file implements the client to communicate with the server. The <code>python -m grpc_tools.protoc</code> command is used to compile the <code>.proto</code> file and generate the necessary Python code for the server and client.</p>"},{"location":"blog/a-beginners-guide-to-grpc-with-python/#adding-an-extra-method-on-the-server","title":"Adding an Extra Method on the Server","text":"<ul> <li>Modify <code>../../protos/helloworld.proto</code> (<code>grpc/examples/protos/helloworld.proto</code>) and the files <code>greeter_server.py</code> and <code>greeter_client.py</code> in the <code>examples/python/helloworld</code> folder.</li> </ul> <code>examples/protos/helloworld.proto</code> <code>greeter_server.py</code> <code>greeter_client.py</code> <pre><code>...\nservice Greeter {\n  // Sends a greeting\n  rpc SayHello (HelloRequest) returns (HelloReply) {}\n\n  // Sends another greeting\n  rpc SayHelloAgain (HelloRequest) returns (HelloReply) {}\n  rpc SayHelloStreamReply (HelloRequest) returns (stream HelloReply) {}\n  ...\n}\n...\n</code></pre> <pre><code>...\nclass Greeter(helloworld_pb2_grpc.GreeterServicer):\n\n    def SayHello(self, request, context):\n        return helloworld_pb2.HelloReply(message=f\"Hello, {request.name}!\")\n\n    def SayHelloAgain(self, request, context):\n        return helloworld_pb2.HelloReply(message=f\"Hello again, {request.name}!\")\n...\n</code></pre> <pre><code>...\ndef run():\n    with grpc.insecure_channel('localhost:50051') as channel:\n        stub = helloworld_pb2_grpc.GreeterStub(channel)\n        response = stub.SayHello(helloworld_pb2.HelloRequest(name='you'))\n        print(\"Greeter client received: \" + response.message)\n        response = stub.SayHelloAgain(helloworld_pb2.HelloRequest(name='you'))\n        print(\"Greeter client received: \" + response.message)\n...\n</code></pre> <ul> <li>Compile the <code>.proto</code> file and generate the necessary Python code for the server and client</li> </ul> <pre><code>python -m grpc_tools.protoc -I../../protos --python_out=. --pyi_out=. --grpc_python_out=. ../../protos/helloworld.proto\n</code></pre> <ul> <li>Run the client and server again:</li> </ul> <pre><code>python greeter_server.py\npython greeter_client.py\n</code></pre>"},{"location":"blog/a-beginners-guide-to-grpc-with-python/#what-just-happened","title":"What just happened","text":"<p>Well, we have added another RPC method, called here <code>SayHelloAgain</code>. The implementation includes:</p> <ul> <li>The protobuf definition in the <code>Greeter</code> service in the <code>greeter_server.py</code></li> <li>The server-side implementation in <code>greeter_server.py</code></li> </ul> <p>So, when running the server then the client, we should receive two responses</p> <p>The server output should remain the same</p> <pre><code>Server started, listening on 50051\n</code></pre> <p>But the client will receive two responses from the server.</p> <pre><code>Will try to greet world ...\nGreeter client received: Hello, you!\nGreeter client received: Hello again, you!\n</code></pre> <p>The <code>python -m grpc_tools.protoc</code> command is used to compile the <code>.proto</code> file and generate the necessary Python code for the server and client. This command takes the following arguments:</p> <ul> <li><code>-I../../protos</code>: Specifies the directory containing the <code>.proto</code> file.</li> <li><code>--python_out=.</code>: Specifies the output directory for the generated Python code.</li> <li><code>--grpc_python_out=.</code>: Specifies the output directory for the generated gRPC Python code.</li> </ul> <p>This command generates the <code>helloworld_pb2.py</code> file, which contains the generated request and response classes, and the <code>helloworld_pb2_grpc.py</code> file, which contains the generated server and client stubs.</p> <p>The <code>python -m grpc_tools.protoc</code> command is the recommended way to generate Python code from a <code>.proto</code> file for use with gRPC.</p> <p>For more information, you can refer to the gRPC Python documentation and the Protocol Buffer Basics: Python tutorial.</p> <p>If you need to compile <code>.proto</code> files for other programming languages, the process may differ, and you can refer to the respective language's gRPC documentation for guidance.</p>"},{"location":"blog/a-beginners-guide-to-grpc-with-python/#further-reading","title":"Further Reading","text":"<ul> <li>Introduction to gRPC</li> <li>gRPC Core Concepts</li> <li>Explore the Python API Reference to discover functions and classes.</li> </ul> <p>For more detailed instructions, refer to the gRPC Python Quickstart <sup>1</sup>.</p> <ol> <li> <p>https://grpc.io/docs/languages/python/quickstart/ \u21a9\u21a9</p> </li> </ol>"},{"location":"blog/deploying-any-web-application-with-nginx-example-of-flask/","title":"Deploying any Web application with Nginx: Example of Flask","text":""},{"location":"blog/deploying-any-web-application-with-nginx-example-of-flask/#introduction","title":"Introduction","text":"<p>You have created your flask application. How nice ! Now, you want to go a step further and deploy it. For most hosting services, you have nice interfaces to deploy your python applications with support for flask. But sometimes, you only have access via ssh to the server.</p> <p>This is a very straigthforward tutorial on how to do it.</p> <p>This tutorial also applies to any web server you can run on local but want to deploy</p>"},{"location":"blog/deploying-any-web-application-with-nginx-example-of-flask/#step-1-install-nginx-and-flask","title":"Step 1: Install Nginx and Flask","text":"<p>Make sure you have Nginx and Flask installed on your server. If not, install them using the appropriate package manager for your operating system.</p>"},{"location":"blog/deploying-any-web-application-with-nginx-example-of-flask/#step-2-configure-nginx","title":"Step 2: Configure Nginx","text":"<p>Create a new Nginx configuration file for your Flask app in the <code>/etc/nginx/sites-available/</code> directory. For example, you could name it <code>myapp.conf</code>. Edit the file and add the following configuration:</p> <pre><code>server {\n    listen 80;\n    server_name yourdomain.com;\n    location / {\n        proxy_pass http://localhost:5000; # assuming Flask is running on port 5000\n        include /etc/nginx/proxy_params;\n        proxy_redirect off;\n    }\n}\n</code></pre> <p>This tells Nginx to listen on port 80 (HTTP), forward all requests to your Flask app running on <code>localhost:5000</code>, and include some proxy parameters.</p>"},{"location":"blog/deploying-any-web-application-with-nginx-example-of-flask/#step-3-create-a-symbolic-link","title":"Step 3: Create a symbolic link","text":"<p>Create a symbolic link from the <code>sites-available</code> directory to the <code>sites-enabled</code> directory by running the following command:</p> <pre><code>sudo ln -s /etc/nginx/sites-available/myapp.conf /etc/nginx/sites-enabled/\n</code></pre> <p>This will enable your Nginx configuration.</p>"},{"location":"blog/deploying-any-web-application-with-nginx-example-of-flask/#step-4-test-the-configuration","title":"Step 4: Test the configuration","text":"<p>Test your Nginx configuration by running the following command:</p> <pre><code>sudo nginx -t\n</code></pre> <p>If there are no errors, reload Nginx by running:</p> <pre><code>sudo service nginx reload\n</code></pre>"},{"location":"blog/deploying-any-web-application-with-nginx-example-of-flask/#step-5-start-the-flask-app","title":"Step 5: Start the Flask app","text":"<p>Start your Flask app by running the following command:</p> <pre><code>python app.py\n</code></pre> <p>Your Flask app should now be running and accessible through Nginx at <code>http://yourdomain.com</code>.</p>"},{"location":"blog/deploying-any-web-application-with-nginx-example-of-flask/#exemple-of-flask-app","title":"exemple of flask app","text":"<pre><code>from flask import Flask\n\napp = Flask(__name__)\n\n@app.route('/')\ndef index():\n    return 'Hello, World!'\n\nif __name__ == '__main__':\n    app.run()\n</code></pre>"},{"location":"blog/deploying-any-web-application-with-nginx-example-of-flask/#related-posts","title":"Related Posts","text":"<ul> <li>Mastering SSH and File Transfers to Remote servers: A Beginner's Handbook</li> <li>Navigating Redirect Challenges With GitHub Pages: A Creative Approach to Domain Migration</li> </ul>"},{"location":"blog/navigating-redirect-challenges-with-github-pages-a-creative-approach-to-domain-migration/","title":"Navigating Redirect Challenges With GitHub Pages: A Creative Approach to Domain Migration","text":""},{"location":"blog/navigating-redirect-challenges-with-github-pages-a-creative-approach-to-domain-migration/#introduction","title":"Introduction","text":"<p>Imagine having a GitHub Pages website. Now, you've migrated to project on another GitHub Pages website. As reports surfaced about users being unable to access the site, the need for a swift redirection from old URLs to the current ones became paramount.</p> <p>The catch? The solution had to operate within the constraints of a static web page, using only HTML, CSS, and JavaScript.</p> <p>While conventional methods like Flask and Frozen-Flask failed, the journey led to a creative solution using HTML and JavaScript. In this blog post, I'll share the step-by-step process of how I navigated through the obstacles and achieved seamless redirection.</p>"},{"location":"blog/navigating-redirect-challenges-with-github-pages-a-creative-approach-to-domain-migration/#the-initial-attempts-static-html","title":"The Initial Attempts: Static HTML","text":""},{"location":"blog/navigating-redirect-challenges-with-github-pages-a-creative-approach-to-domain-migration/#using-html-for-single-page-redirect","title":"Using HTML for Single Page Redirect","text":"<p>My first inclination was to use HTML for redirection. To redirect only one page, a simple HTML script could be used. For instance:</p> <p>Using HTML for Single Page Redirect</p> <pre><code>&lt;!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\"&gt;\n&lt;html lang=\"fr\"&gt;\n    &lt;head&gt;\n        &lt;title&gt;Accueil&lt;/title&gt;\n        &lt;script&gt;window.location.replace(\"https://hermann-web.github.io/blog/\")&lt;/script&gt;\n    &lt;/head&gt;\n&lt;body&gt;&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"blog/navigating-redirect-challenges-with-github-pages-a-creative-approach-to-domain-migration/#static-html-challenges","title":"Static HTML Challenges","text":"<p>However, it quickly became apparent that defining redirects for every page statically was impractical, as I would need to create an HTML page for each endpoint. Then, I thought of using React or Flask, but they require explicitly defined routes too.</p> <p>Regardless, there is a module that uses Flask to implement the first option. So, I can combine the list of URLs with the Frozen-Flask module.</p> <p>The existence of this URL list proved crucial, as without it, I would have encountered additional difficulties.</p>"},{"location":"blog/navigating-redirect-challenges-with-github-pages-a-creative-approach-to-domain-migration/#another-way-flask-and-frozen-flask","title":"Another Way: Flask and Frozen-Flask","text":"<p>Next, I explored Flask and Frozen-Flask, but challenges arose when dealing with dynamic endpoints in a static context. The attempt to freeze the Flask app yielded an error, highlighting the limitations of this approach.</p>"},{"location":"blog/navigating-redirect-challenges-with-github-pages-a-creative-approach-to-domain-migration/#first-attempts-with-flask","title":"First Attempts with Flask","text":"Prerequisites <p>Before diving into the implementation, I set up the development environment with the following commands:</p> <pre><code># Install Flask and Frozen-Flask\npython -m venv .venv\nsource .venv/bin/activate\npip install Flask Frozen-Flask\n\n# Download the list of endpoints\nwget https://raw.githubusercontent.com/Hermann-web/blog/gh-pages/sitemap.xml\n</code></pre> <p>After environment setup, I've created three python files</p> <ul> <li><code>utils.py</code> to parse the sitemap.xml file and extract the necessary endpoints</li> <li><code>app.py</code> responsible for handling the redirection logic</li> <li><code>main.py</code> convert the Flask app into a static website using <code>frozen-flask</code> module</li> </ul> First Attemps with Flask <code>utils.py</code> <code>app.py</code> <code>main.py</code> <pre><code>import xml.etree.ElementTree as ET\n\n# Load the XML file\nxml_file_path = \"sitemap.xml\"\ntree = ET.parse(xml_file_path)\nroot = tree.getroot()\n\n# Define the namespace\nnamespace = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n\nWEBSITE_URL = \"https://hermann-web.github.io/blog\"\n\ndef endpoint_parser(endpoint:str):\n    if endpoint.startswith(\"/\"):\n        endpoint = endpoint[1:]\n    if endpoint.endswith(\"/\"):\n        endpoint = endpoint[:-1]\n    return endpoint\n\n# Extract endpoint URLs\ndef get_endpoints():\n    endpoints = [url_element.text for url_element in root.findall('.//ns:loc', namespace)]\n    endpoints = [endpoint_parser(url.replace(WEBSITE_URL, \"\")) for url in endpoints]\n    return endpoints\n</code></pre> <pre><code>from flask import Flask, redirect\nfrom utils import WEBSITE_URL, endpoint_parser\n\napp = Flask(__name__)\ntarget_domain = WEBSITE_URL\n\nassert target_domain\n\n\n@app.route('/')\ndef index():\n    return redirect(target_domain)\n\n# Create a route for redirection\n@app.route('/&lt;path:endpoint&gt;')\ndef redirect_to_another_server(endpoint):\n    endpoint = endpoint_parser(endpoint)\n    print(\"endpoint:\",endpoint)\n    if endpoint in endpoints:\n        target_url = f\"{target_domain}/{endpoint}\"\n        return redirect(target_url)\n    else:\n        return redirect(target_domain)\n\nif __name__ == \"__main__\":\n    app.run(debug=True)\n</code></pre> <pre><code>from flask_frozen import Freezer\nfrom app import app\n\nfreezer = Freezer(app)\n\nif __name__ == '__main__':\n    freezer.freeze()\n</code></pre> <p>Running the app went fine:</p> <pre><code>python app.py\n</code></pre> <pre><code> * Serving Flask app 'app'\n * Debug mode: on\nWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n * Running on http://127.0.0.1:5000\nPress CTRL+C to quit\n * Restarting with stat\n * Debugger is active!\n</code></pre> <p>However, running <code>python main.py</code> to freeze the Flask app using Frozen-Flask failed due to the inability to follow external redirects.</p> <pre><code>RuntimeError: Following external redirects is not supported.\n</code></pre> <p>This makes sense. A static webpage cannot accept dynamic endpoints. So, I should create the endpoints manually using a for-loop, hoping it will work with Flask.</p> <p>So, i've tried some workarounds</p>"},{"location":"blog/navigating-redirect-challenges-with-github-pages-a-creative-approach-to-domain-migration/#workarounds-with-flask","title":"Workarounds with Flask","text":""},{"location":"blog/navigating-redirect-challenges-with-github-pages-a-creative-approach-to-domain-migration/#workaround-1-manually-creating-routes","title":"Workaround 1: Manually Creating Routes","text":"<p>However, failed. The issue was that I should not have duplicated function names for the routes:</p> Manually Creating Routes <pre><code>from flask import Flask, redirect\nfrom get_endpoints import get_endpoints, WEBSITE_URL, endpoint_parser\n\napp = Flask(__name__)\ntarget_domain = WEBSITE_URL\n\nassert target_domain\n\nendpoints = get_endpoints()\n\n# Create route functions for each endpoint\nfor endpoint in endpoints:\n    @app.route(f'/{endpoint}')\n    def redirect_to_another_server():\n        target_url = f\"{target_domain}/{endpoint}\"\n        return redirect(target_url)\n\n\nif __name__ == \"__main__\":\n    app.run(debug=True)\n</code></pre> <p>A first attempt to automatically generate route functions in Flask failed, as <code>redirect_to_another_server</code> was duplicated:</p> <pre><code>AssertionError: View function mapping is overwriting an existing endpoint function: redirect_to_another_server\n</code></pre> <p>So, that approach faced an issue with function duplication, prompting a need for a workaround.</p>"},{"location":"blog/navigating-redirect-challenges-with-github-pages-a-creative-approach-to-domain-migration/#workaround-2-dynamic-function-generation","title":"Workaround 2: Dynamic Function Generation","text":"<p>There is another solution that involves encapsulating the route functions within another function, ensuring a unique context for each endpoint.</p> <p>Another solution can be to change the function name with a decorator, but it is not possible. So, I figured I can define the functions inside another function, hoping it will work.</p> Dynamic Function Generation <pre><code>def generate_endpoint(endpoint):\n    @app.route(f'/{endpoint_parser(endpoint)}')\n    def dynamic_function():\n        target_url = f\"{target_domain}/{endpoint}\"\n        return redirect(target_url)\n\n# Create route functions for each endpoint\nfor endpoint in endpoints:\n    generate_endpoint(endpoint)\n</code></pre> <p>The attempt to generate dynamic functions also faced an issue with function duplication.</p> <pre><code>AssertionError: View function mapping is overwriting an existing endpoint function: dynamic_function\n</code></pre>"},{"location":"blog/navigating-redirect-challenges-with-github-pages-a-creative-approach-to-domain-migration/#final-solution-overcoming-a-static-constraint","title":"Final Solution: Overcoming a Static Constraint","text":"<p>When attempting to freeze the Flask app using Frozen-Flask, a runtime error occurs due to the inability to follow external redirects. This limitation is inherent in static web pages, preventing the use of dynamic endpoints.</p> <p>To work around this constraint, a custom <code>404.html</code> page is created, embedding JavaScript to correct the URL before redirecting. This clever solution ensures that even erroneous URLs lead users to the correct destination.</p> <pre><code>&lt;!-- 404.html --&gt;\n&lt;!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\"&gt;\n&lt;html lang=\"en\"&gt;\n\n&lt;head&gt;\n    &lt;title&gt;Page Not Found&lt;/title&gt;\n    &lt;script&gt;\n        // Redirect logic to correct the URL\n        let new_url = \"/\";\n        if (window.location.href.startsWith(\"https://hermann-web.github.io/web\")) {\n            new_url = window.location.href.replace(\"https://hermann-web.github.io/web\", \"/blog\");\n        }\n        window.location.replace(new_url);\n    &lt;/script&gt;\n&lt;/head&gt;\n\n&lt;body&gt;&lt;/body&gt;\n\n&lt;/html&gt;\n</code></pre> <p>I noticed that all erroneous URLs redirect the user to the <code>404.html</code> page. For GitHub Pages, I made the remark that even on the <code>404.html</code> page, the erroneous URLs are conserved in the browser. So, I can just correct the last endpoint <code>/web/*</code> to the correct one <code>/blog/*</code> using JavaScript.</p>"},{"location":"blog/navigating-redirect-challenges-with-github-pages-a-creative-approach-to-domain-migration/#conclusion","title":"Conclusion","text":"<p>In this guide, we explored a step-by-step approach to redirecting all pages from one domain to another using Flask and Frozen-Flask. From parsing the <code>sitemap.xml</code> file to handling dynamic endpoints and overcoming static constraints, each aspect was covered in detail. The use of a custom <code>404.html</code> page with JavaScript ensures a smooth redirection experience for users, making this solution both effective and elegant.</p>"},{"location":"blog/navigating-redirect-challenges-with-github-pages-a-creative-approach-to-domain-migration/#related-pages","title":"Related pages","text":"<ul> <li>Deploying any Web application with Nginx: Example of Flask</li> <li>Managing Local Modifications and Remote Changes in Git</li> <li>How to deploy a Streamlit Application on Hugging Face</li> </ul>"},{"location":"blog/getting-started-with-mkdocs-for-documentation/","title":"MkDocs: Your Straightforward Documentation Companion","text":""},{"location":"blog/getting-started-with-mkdocs-for-documentation/#introduction","title":"Introduction","text":"<p>Welcome to MkDocs: the hassle-free documentation solution!</p> <p>In search of a tool that makes documentation creation a breeze? MkDocs is your answer!</p> <p>This straightforward platform simplifies the process of generating professional project documentation.</p> <p>This guide is your gateway to exploring MkDocs' user-friendly approach. You'll uncover how this tool streamlines the creation of polished and organized documentation for all your projects. Let's dive in and harness MkDocs' straightforwardness for your documentation needs.</p>"},{"location":"blog/getting-started-with-mkdocs-for-documentation/#comparing-documentation-tools","title":"Comparing Documentation Tools","text":"<p>When it comes to documenting projects, various tools offer unique features and complexities. Let's explore a few:</p> <ul> <li> <p>Sphinx: Known for its robustness and flexibility, Sphinx is powerful but can be intricate for beginners due to its configuration requirements.</p> </li> <li> <p>Docusaurus: Ideal for creating user-centric documentation with React, but might feel overwhelming for those unfamiliar with JavaScript frameworks.</p> </li> <li> <p>GitBook: Offers a user-friendly interface, yet its extensive feature set might be more than needed for straightforward documentation needs.</p> </li> <li> <p>MkDocs: Unlike some other tools, MkDocs stands out for its simplicity. It's based on Markdown, a plain text format, making it incredibly easy to use. With MkDocs, creating professional documentation feels straightforward and hassle-free.</p> </li> </ul>"},{"location":"blog/getting-started-with-mkdocs-for-documentation/#mkdocs-embracing-simplicity","title":"MkDocs: Embracing Simplicity","text":"<p>MkDocs adopts Markdown, a plain text format widely accessible and intuitive for beginners. Its minimalistic approach enables users to focus on content creation without getting lost in complex configurations.</p>"},{"location":"blog/getting-started-with-mkdocs-for-documentation/#features-of-mkdocs","title":"Features of MkDocs","text":"<ul> <li>Simple Configuration: MkDocs requires minimal setup, with a straightforward configuration file.</li> <li>User-Friendly: Its Markdown-based structure simplifies content creation for all levels of users.</li> <li>Live Preview: Offers a live preview of documentation, ensuring instant visual feedback.</li> <li>Extensibility: While basic, MkDocs supports various themes and plugins for enhanced functionality.</li> </ul> <p>MkDocs excels in providing a straightforward and efficient way to create professional documentation without overwhelming users with unnecessary complexities. It's the perfect choice for those seeking a quick and easy documentation solution.</p>"},{"location":"blog/getting-started-with-mkdocs-for-documentation/#tutorial-getting-started-with-mkdocs","title":"Tutorial: Getting Started with MkDocs","text":"<p>MkDocs simplifies the process of creating documentation for your Python projects. Follow these steps to create a documentation site using MkDocs:</p>"},{"location":"blog/getting-started-with-mkdocs-for-documentation/#1-install-mkdocs","title":"1. Install MkDocs","text":"<p>Install MkDocs by running the following command in your terminal:</p> <pre><code>pip install mkdocs\n</code></pre>"},{"location":"blog/getting-started-with-mkdocs-for-documentation/#2-set-up-your-project","title":"2. Set Up Your Project","text":"<p>Create a new directory for your project and initialize an MkDocs project:</p> <pre><code>mkdir my-project\ncd my-project\nmkdocs new .\n</code></pre> <p>This creates a new <code>mkdocs.yml</code> configuration file and a <code>docs</code> directory with a sample Markdown file.</p>"},{"location":"blog/getting-started-with-mkdocs-for-documentation/#3-install-a-theme","title":"3. Install a Theme","text":"<p>Enhance your documentation's appearance by installing a theme like <code>mkdocs-material</code>:</p> <pre><code>pip install mkdocs-material\n</code></pre>"},{"location":"blog/getting-started-with-mkdocs-for-documentation/#4-configure-your-site","title":"4. Configure Your Site","text":"<p>Edit the <code>mkdocs.yml</code> file to configure your documentation site. Define the title, theme, and pages to include. Check examples.</p> <ul> <li>Configure <code>docs_dir</code> to specify the folder where MkDocs will find <code>.md</code> files.</li> <li>Use the <code>nav</code> section to structure your files into tabs.</li> </ul>"},{"location":"blog/getting-started-with-mkdocs-for-documentation/#5-write-documentation","title":"5. Write Documentation","text":"<p>Create your documentation in Markdown format and save the files in the <code>docs</code> directory.</p>"},{"location":"blog/getting-started-with-mkdocs-for-documentation/#6-preview-your-site","title":"6. Preview Your Site","text":"<p>To preview your site locally, run:</p> <pre><code>mkdocs serve\n</code></pre> <p>This will start a local web server and open your documentation site in your default web browser. You can make changes to your documentation and the site will automatically update.</p>"},{"location":"blog/getting-started-with-mkdocs-for-documentation/#7-optional-more-options","title":"7. (optional) More options","text":"<p>You can add more options. For example,</p> <pre><code>mkdocs serve --dirty -a localhost:8001\n</code></pre> <p>Note</p> <ul> <li><code>--dirty</code>: Only re-build files that have changed.</li> <li><code>-a, --dev-addr &lt;IP:PORT&gt;</code>: IP address and port to serve documentation locally (default: localhost:8000)</li> <li>use <code>mkdocs serve -h</code> for more options</li> </ul> <p>warning</p> <p><code>A 'dirty' build [...] will likely lead to inaccurate navigation and other links within your site. This option is designed for site development purposes only.</code>, mkdocs</p>"},{"location":"blog/getting-started-with-mkdocs-for-documentation/#8-build-your-site","title":"8. Build Your Site","text":"<p>Generate a static HTML site by running:</p> <pre><code>mkdocs build\n</code></pre> <p>This creates a <code>site</code> directory containing the built site. You can deploy this to a web server.</p> <p>Remember, MkDocs supports numerous plugins, such as <code>mkdocs-run-shell-cmd-plugin</code>, enabling extended functionalities.</p>"},{"location":"blog/getting-started-with-mkdocs-for-documentation/#conclusion","title":"Conclusion","text":"<p>MkDocs provides a straightforward way to create and manage documentation for Python projects. With its simple setup, configuration, and live preview features, it streamlines the documentation process, making it an excellent choice for developers.</p>"},{"location":"blog/getting-started-with-mkdocs-for-documentation/#more-ressources","title":"More ressources","text":"<p>Explore the resources below to dive deeper into MkDocs:</p> <ul> <li>Real Python - Python Project Documentation with MkDocs</li> <li>MkDocs - Getting Started</li> <li>MkDocs - User Guide CLI</li> <li>MkDocs - Issues and Discussions</li> <li>YouTube - Getting Started with MkDocs</li> <li>MkDocs Run Shell Cmd Plugin</li> </ul>"},{"location":"blog/getting-started-with-mkdocs-for-documentation/#related-pages","title":"Related Pages","text":"<ul> <li>MkDocs: Your Straightforward Documentation Companion</li> </ul>"},{"location":"blog/using-mkdocs-with-docker-streamlining-documentation-workflow/","title":"Using MkDocs with Docker: Streamlining Documentation Workflow","text":""},{"location":"blog/using-mkdocs-with-docker-streamlining-documentation-workflow/#introduction","title":"Introduction","text":"<p>Looking to streamline your documentation workflow using MkDocs and Docker?</p> <p>Documentation lies at the heart of every successful project. MkDocs offers a straightforward way to create elegant documentation sites, while Docker ensures a consistent and isolated environment for various applications. Combining these tools optimizes the documentation process and enhances collaboration within development teams.</p> <p>This tutorial serves as your guide, illustrating how to set up MkDocs within a Docker container effectively. By following these steps, you'll establish a robust documentation framework, facilitating seamless documentation creation and deployment.</p> <p>Let's dive into the process of integrating MkDocs with Docker to revolutionize your documentation workflow.</p>"},{"location":"blog/using-mkdocs-with-docker-streamlining-documentation-workflow/#prerequisites","title":"Prerequisites","text":"<p>Before starting, make sure you have the following files in the same directory:</p> <ol> <li><code>docker-compose.yml</code></li> <li><code>requirements.txt</code> (or your specific requirements file)</li> </ol> <p>For example,</p> <pre><code># python 3.9.18\nmkdocs==1.5.3\nmkdocs-material==9.4.7\nmkdocs-material-extensions==1.3\nmkdocs-minify-plugin==0.7.1\nmkdocs-roamlinks-plugin==0.3.2\n</code></pre> <ol> <li><code>mkdocs.yml</code> (the MkDocs configuration file)</li> <li>A folder named <code>docs</code> containing your documentation files</li> <li>Optionally, a <code>.dockerignore</code> file to exclude unnecessary files from the Docker image</li> </ol> <p>For example,</p> <pre><code>venv/\n</code></pre>"},{"location":"blog/using-mkdocs-with-docker-streamlining-documentation-workflow/#setting-up-mkdocs-with-docker","title":"Setting Up MkDocs with Docker","text":"<p>Let's create a <code>docker-compose.yml</code> file:</p> <pre><code>version: '3'\n\nservices:\n  mkdocs:\n    image: python:3.9.18\n    volumes:\n      - ./:/app/\n    working_dir: /app\n    ports:\n      - \"49162:8000\"\n    command: &gt;\n      bash -c \"\n        pip install -r requirements.txt &amp;&amp;\n        mkdocs serve -a 0.0.0.0:8000\"\n</code></pre> <p>Ensure your <code>requirements.txt</code> contains the necessary dependencies as outlined in the example. Customize it based on your project's requirements.</p> <p>After placing all the required files in the same directory, open a terminal or command prompt and navigate to this directory.</p> <p>Execute the following command:</p> <pre><code>docker-compose up\n</code></pre> <p>This command builds the Docker image and starts the MkDocs server. Access the MkDocs site by visiting <code>http://localhost:49162</code> in your web browser.</p> <p>hot reload</p> <p>As mkdocs rebuild all the files when changes are made, you may want to add the <code>--dirty</code> option to <code>mkdocs serve</code> to rebuild only the modified files. Read more about it in the mkdocs tutorial</p>"},{"location":"blog/using-mkdocs-with-docker-streamlining-documentation-workflow/#conclusion","title":"Conclusion","text":"<p>Integrating MkDocs with Docker simplifies the setup process, ensuring consistency across different environments. It provides an isolated space for documentation management, enhancing collaboration and deployment.</p> <p>Remember to replace placeholder file names (<code>requirements.txt</code>, <code>docs</code>, etc.) with your actual file names if they differ.</p> <p>By following this guide, you've streamlined your documentation workflow using MkDocs within a Docker container, fostering efficient documentation management for your projects.</p>"},{"location":"blog/using-mkdocs-with-docker-streamlining-documentation-workflow/#related-pages","title":"Related Pages","text":"<ul> <li>MkDocs: Your Straightforward Documentation Companion</li> <li>Mastering Docker: A Comprehensive Guide to Efficient Container Management</li> <li>Simple guide to using Docker on Windows 10 and access from WSL 2</li> </ul>"},{"location":"blog/mastering-ssh-and-file-transfers-to-remote-servers-a-beginners-handbook/","title":"Mastering SSH and File Transfers to Remote servers: A Beginner's Handbook","text":""},{"location":"blog/mastering-ssh-and-file-transfers-to-remote-servers-a-beginners-handbook/#introduction","title":"Introduction","text":"<p>Do you find yourself baffled by the intricacies of SSH connections and file transfers to remote servers ?</p> <p>Navigating the landscape of SSH connections, troubleshooting connection issues, and securely transferring files across servers can be a daunting task, especially for newcomers.</p> <p>This guide is your compass in the world of SSH, unraveling the complexities and providing step-by-step instructions for establishing secure connections and transferring files seamlessly using Git Bash or WSL2 for Windows users and straightforward methods for Linux enthusiasts.</p> <p>Whether you're a developer, sysadmin, or tech enthusiast stepping into the realm of remote server access or seeking efficient file transfer solutions, this guide is tailored to demystify SSH, troubleshoot common pitfalls, and equip you with the skills to maneuver through file transfers effortlessly.</p> <p>This document break down the process of connecting via SSH and file transfer and should help someone new to SSH understand the process, troubleshoot common issues, and handle file transfers easily</p>"},{"location":"blog/mastering-ssh-and-file-transfers-to-remote-servers-a-beginners-handbook/#requirements","title":"Requirements","text":"<ul> <li>For windows users, use Git Bash installed on your computer or use wsl2 (for windows &gt;=10)</li> <li>For linux users, this should be straighforward</li> </ul>"},{"location":"blog/mastering-ssh-and-file-transfers-to-remote-servers-a-beginners-handbook/#connecting-via-ssh-from-cli","title":"Connecting via SSH from cli","text":""},{"location":"blog/mastering-ssh-and-file-transfers-to-remote-servers-a-beginners-handbook/#steps-to-connect","title":"Steps to Connect","text":"<ol> <li> <p>Open Terminal: Or search for Git Bash in your applications and open it.</p> </li> <li> <p>Accessing the Server:</p> <ul> <li> <p>Use the command</p> <pre><code>ssh {username}@{domain}\n</code></pre> <p>or</p> <pre><code>ssh {username}@{server_ip}\n</code></pre> </li> <li> <p>Replace <code>{username}</code> with your remote server username.</p> </li> <li>Replace <code>{domain}</code> with the domain name or <code>{server_ip}</code> with the server's IP address.</li> </ul> </li> <li> <p>Adding a Specific Port:</p> <ul> <li>If the server uses a different port (usually 22), use <code>ssh {username}@{domain} -p {port}</code>. Replace <code>{port}</code> with the correct port number.</li> </ul> </li> <li> <p>Entering Credentials:</p> <ul> <li>After executing the command, you'll be prompted to enter your remote server account password. Type it in and press Enter.</li> </ul> </li> </ol>"},{"location":"blog/mastering-ssh-and-file-transfers-to-remote-servers-a-beginners-handbook/#troubleshooting-ssh-connection-issues","title":"Troubleshooting SSH Connection Issues","text":""},{"location":"blog/mastering-ssh-and-file-transfers-to-remote-servers-a-beginners-handbook/#error-unable-to-negotiate-with-port","title":"Error: \"Unable to negotiate with... port...\"","text":"<p>If you encounter the error <code>Unable to negotiate with &lt;IP&gt; port &lt;Port&gt;: no matching host key type found. Their offer: ssh-rsa,ssh-dss</code></p> <p>The Solution</p> <ul> <li>Configure the client to accept the host key sent by the server.</li> <li>Edit the <code>~/.ssh/config</code> file:</li> </ul> <pre><code>Host {domain}\n    HostKeyAlgorithms +ssh-rsa,ssh-dss\n</code></pre> <ul> <li>Use a text editor like Nano, Vim, or Notepad to modify the file.</li> </ul> Other Common SSH errors: <ul> <li>Permissions: Ensure correct file permissions for <code>~/.ssh</code> and authorized_keys.</li> <li>Network problems: Check firewall settings and network connectivity.</li> </ul>"},{"location":"blog/mastering-ssh-and-file-transfers-to-remote-servers-a-beginners-handbook/#alternative-to-ssh-access-though-a-cli-putty-and-openssh","title":"Alternative to ssh access though a cli: Putty and OpenSSH","text":"<ul> <li>Putty (Windows):</li> </ul> <p>Known for its user-friendly GUI, Putty offers a straightforward interface for establishing SSH connections on Windows systems. It's particularly popular among users who prefer a graphical interface for SSH connections.</p> <p>That's why Putty is a popular SSH client primarily used on Windows systems. However, it's worth noting that while Putty is predominantly associated with Windows, it can also be utilized on other operating systems through compatibility layers or third-party tools like Wine on Linux or macOS.</p> <ul> <li>OpenSSH:</li> </ul> <p>OpenSSH, on the other hand, is an open-source implementation of the SSH protocol. It's available not just for Windows but also for Linux, macOS, and various Unix-like operating systems. OpenSSH provides both the client (ssh) and server (sshd) components, making it a versatile and widely adopted solution for secure remote access, file transfer, and tunneling across different platforms. It offers a robust set of features, including secure remote access, file transfer (using tools like <code>scp</code> and <code>sftp</code>), and tunneling capabilities.</p> <p>Thats's why OpenSSH is often preferred by users who work in mixed environments or want a consistent SSH experience across different operating systems. It's commonly used in command-line environments and scripts due to its versatile nature.</p>"},{"location":"blog/mastering-ssh-and-file-transfers-to-remote-servers-a-beginners-handbook/#file-transfer-using-ssh","title":"File Transfer Using SSH","text":"<p>You can use <code>scp</code> command to transfer files directly between two servers (local to remote or one remote to another) by specifying their addresses and file paths</p>"},{"location":"blog/mastering-ssh-and-file-transfers-to-remote-servers-a-beginners-handbook/#sending-files-to-remote-server","title":"Sending Files to Remote Server","text":"<ul> <li>Use the <code>scp</code> command:</li> </ul> <pre><code>local_file=\"/path/to/local/file\"\nremote_file=\"$remote_user@$remote_host:/path/to/remote/file/or/folder\"\nscp \"$local_file\" \"$remote_file\"\n</code></pre> <ul> <li>You'll be prompted for the password before the file is sent.</li> </ul>"},{"location":"blog/mastering-ssh-and-file-transfers-to-remote-servers-a-beginners-handbook/#transferring-between-servers","title":"Transferring Between Servers","text":"<p>Similar to local to server transfer, use <code>scp</code> between two remote servers by specifying their addresses.</p> <p>Use the <code>scp</code> command to transfer files directly between two remote servers:</p> <pre><code>```bash\nremote_file_source=\"$remote_user1@$remote_host1:/path/to/source/file\"\nremote_file_destination=\"$remote_user2@$remote_host2:/path/to/destination/folder\"\nscp \"$remote_file_source\" \"$remote_file_destination\"\n```\n</code></pre> <p>Replace:     - <code>$remote_user1</code> with the username for the first remote server.     - <code>$remote_host1</code> with the address or IP of the first remote server.     - <code>$remote_user2</code> with the username for the second remote server.     - <code>$remote_host2</code> with the address or IP of the second remote server.</p> <p>This will transfer the specified file from the first remote server to the specified folder on the second remote server.</p>"},{"location":"blog/mastering-ssh-and-file-transfers-to-remote-servers-a-beginners-handbook/#scp-options","title":"SCP Options","text":"<p>Using <code>-r</code> for recursive copying of directories:</p> <pre><code>scp -r local_directory username@remote_host:/remote_directory\n</code></pre>"},{"location":"blog/mastering-ssh-and-file-transfers-to-remote-servers-a-beginners-handbook/#alternative-file-transfer-methods","title":"Alternative File Transfer Methods","text":"<p>Using <code>rsync</code> for efficient synchronization:</p> <pre><code>rsync -avz --progress /path/to/source username@remote_host:/path/to/destination\n</code></pre>"},{"location":"blog/mastering-ssh-and-file-transfers-to-remote-servers-a-beginners-handbook/#running-commands-on-a-remote-server-without-accessing-its-cli","title":"Running commands on a remote server without accessing its cli","text":"<p>For example, using ssh to Fetch latest changes from the remote repository</p> <pre><code>ssh \"$remote_user@$remote_host\" \"cd $remote_path &amp;&amp; git fetch\"\n</code></pre>"},{"location":"blog/mastering-ssh-and-file-transfers-to-remote-servers-a-beginners-handbook/#simplifying-ssh-access-with-sshpass","title":"Simplifying SSH Access with sshpass","text":"<p>To avoid being prompted to write the password, <code>sshpass</code> is a tool you want</p> <ol> <li> <p>Install sshpass:     If needed, install sshpass using <code>sudo apt install sshpass</code>.</p> </li> <li> <p>Accessing SSH without Password Prompt:     Instead of</p> <pre><code>ssh \"$remote_user@$remote_host\"\n</code></pre> <p>use</p> <pre><code>sshpass -p \"$password\" ssh \"$remote_user@$remote_host\"\n</code></pre> </li> </ol> <p>to tetch latest changes from the remote repository, run</p> <pre><code>sshpass -p \"$password\" ssh \"$remote_user@$remote_host\" \"cd $remote_path &amp;&amp; git fetch\"\n</code></pre>"},{"location":"blog/mastering-ssh-and-file-transfers-to-remote-servers-a-beginners-handbook/#conclusion","title":"Conclusion","text":"<p>Congratulations! You've now mastered the fundamentals of SSH connections and file transfers using Git Bash.</p> <p>In this guide, we've covered the essential steps to initiate SSH connections, troubleshoot common errors, and conduct seamless file transfers between local and remote servers. You've learned to troubleshoot connection issues, enhance security configurations, and optimize file transfers using <code>scp</code> and <code>rsync</code>.</p> <p>Remember, SSH is a powerful tool for secure communication and file transfer, and understanding its nuances empowers you to work efficiently across different servers and systems.</p> <p>As you continue your journey, keep exploring the capabilities of SSH, experiment with different options and configurations, and don't hesitate to delve deeper into security best practices.</p> <p>Whether you're a developer collaborating on remote repositories or a system administrator managing servers, the knowledge gained here will serve as a solid foundation for your endeavors.</p> <p>Embrace the power of SSH, continue to explore, and may your future endeavors in remote access and file transfer be secure, efficient, and hassle-free!</p>"},{"location":"blog/mastering-ssh-and-file-transfers-to-remote-servers-a-beginners-handbook/#related-posts","title":"Related Posts","text":"<ul> <li>Comprehensive Guide to SSH: Tunneling, File Transfers, and Key-Based Authentication</li> </ul>"},{"location":"blog/setting-up-remote-desktop-access-with-remmina-on-ubuntu/","title":"How to Set Up Remote Desktop Access from Linux to Windows 10 Using Remmina","text":"<p>Remote desktop access has become an essential feature in today's digital landscape, allowing users to connect to their computers from anywhere. While Windows users have built-in options for remote desktop access, Linux users often need to rely on third-party applications.</p> <p>In this guide, we'll explore how to set up remote desktop access from a Linux system to a Windows 10 machine using Remmina.</p>"},{"location":"blog/setting-up-remote-desktop-access-with-remmina-on-ubuntu/#microsofts-remote-desktop-services","title":"Microsoft's Remote Desktop Services","text":"<p>Microsoft offers extensive documentation on remote desktop services, providing official clients for various platforms such as Windows 10, macOS, and others. However, there isn't an official client for Linux systems. This gap has led Linux users to explore alternative solutions, with Remmina being a popular choice.</p>"},{"location":"blog/setting-up-remote-desktop-access-with-remmina-on-ubuntu/#step-1-preparing-your-windows-10-machine","title":"Step 1: Preparing Your Windows 10 Machine","text":"<p>Before connecting remotely to your Windows 10 machine, you'll need to enable remote connections.</p> <ol> <li> <p>Enable Remote Connections: Navigate to the \"Remote Desktop settings\" on your Windows 10 machine and ensure that remote connections are allowed.</p> </li> <li> <p>Find the IP Address: While the computer name is usually used for remote connections, you can also use the private IP address of the Windows machine. You can find this IP address by running <code>ipconfig</code> in the Command Prompt and copying the IPv4 address listed under \"Carte r\u00e9seau sans fil Wi-Fi &gt; Adresse IPv4\".</p> </li> <li> <p>Add Remote Desktop Account: Add the account (Windows session) you plan to use for remote access. While administrator accounts should work, it's recommended to use a specific user account for a smoother experience.</p> </li> </ol>"},{"location":"blog/setting-up-remote-desktop-access-with-remmina-on-ubuntu/#step-2-installing-and-configuring-remmina-on-linux","title":"Step 2: Installing and Configuring Remmina on Linux","text":"<p>Remmina is an open-source remote desktop client for Linux systems, offering an intuitive interface and robust features.</p> <p>1. Install Remmina: Open a terminal on your Linux system and install Remmina using your package manager:</p> <pre><code>sudo apt install remmina\n</code></pre> <p>2. Create a New Connection: Launch Remmina and create a new connection profile. Enter the private IP address of your Windows 10 machine as the server address, and provide the username and password of the session you added in the previous step.</p> <p>3. Establish the Connection: Once you've entered the necessary information, click \"Connect\" to establish the remote desktop connection to your Windows 10 machine.</p>"},{"location":"blog/setting-up-remote-desktop-access-with-remmina-on-ubuntu/#step-3-testing-remote-desktop-access","title":"Step 3: Testing Remote Desktop Access","text":"<p>Now that you've set up the connection, it's time to test remote desktop access.</p> <ol> <li> <p>Within the Same Network: Connect to your Windows 10 machine from your Linux system while both devices are on the same network. This allows you to ensure that everything is set up correctly.</p> </li> <li> <p>Outside the Network (Optional): If you encounter issues connecting from outside the network, it may be due to network restrictions or firewall settings. In such cases, you may need to contact your network administrator to allow remote desktop connections from external locations. Alternatively, consider using a virtual private network (VPN) to establish a secure connection to your network and access the Windows 10 machine remotely.</p> </li> </ol> <p>By testing remote desktop access within and potentially outside the network, you can verify the functionality of your setup and troubleshoot any connectivity issues effectively.</p> <p>By following these steps, you can enjoy seamless remote desktop access from your Linux system to a Windows 10 machine, enhancing your productivity and flexibility.</p>"},{"location":"blog/setting-up-remote-desktop-access-with-remmina-on-ubuntu/#conclusion","title":"Conclusion","text":"<p>Setting up remote desktop access from a Linux system to a Windows 10 machine using Remmina offers convenience and flexibility, allowing users to access their computers remotely from anywhere. While the process involves a few initial setup steps and potential network considerations, the ability to connect seamlessly enhances productivity and enables efficient remote work. By following the steps outlined in this guide and troubleshooting any connectivity issues, users can enjoy the benefits of remote desktop access with ease.</p>"},{"location":"blog/setting-up-remote-desktop-access-with-remmina-on-ubuntu/#related-posts","title":"Related Posts","text":"<ul> <li>Comprehensive Guide to SSH: Tunneling, File Transfers, and Key-Based Authentication</li> <li>Mastering SSH and File Transfers to Remote servers: A Beginner's Handbook</li> </ul>"},{"location":"blog/comprehensive-guide-to-ssh-tunneling-file-transfers-and-key-based-authentication/","title":"Comprehensive Guide to SSH: Tunneling, File Transfers, and Key-Based Authentication","text":"<p>SSH (Secure Shell) is a powerful protocol used for secure communication between computers, offering a wide range of functionalities including secure remote access, file transfers, and key-based authentication.</p> <p>In this guide, we'll cover various aspects of SSH, including:</p> <ul> <li>Port tunneling for secure access to services like MySQL.</li> <li>Secure file transfer using SFTP and SCP.</li> <li>SSH agent forwarding and key-based authentication for enhanced security.</li> <li>Dynamic port forwarding (SOCKS proxy) for secure browsing.</li> </ul> <p>This document provides detailed instructions and examples to help you harness the full potential of SSH for secure communication and efficient remote server management.</p>"},{"location":"blog/comprehensive-guide-to-ssh-tunneling-file-transfers-and-key-based-authentication/#port-tunneling","title":"Port Tunneling","text":"<p>SSH port tunneling securely forwards network traffic from a local machine to a remote server. This is crucial for accessing services like MySQL databases running on remote servers.</p>"},{"location":"blog/comprehensive-guide-to-ssh-tunneling-file-transfers-and-key-based-authentication/#local-port-forwarding","title":"Local Port Forwarding","text":"<p>Local port forwarding forwards a port on your local machine to a port on the remote server.</p>"},{"location":"blog/comprehensive-guide-to-ssh-tunneling-file-transfers-and-key-based-authentication/#example-command","title":"Example Command","text":"<p>To access a MySQL database on a remote server:</p> <pre><code>ssh -L &lt;local_port&gt;:&lt;db_host&gt;:&lt;db_port&gt; &lt;SSH_USER&gt;@&lt;SSH_HOST&gt;\n</code></pre> <ul> <li><code>&lt;local_port&gt;</code>: Local port to forward (e.g., <code>3306</code>).</li> <li><code>&lt;db_host&gt;</code>: Hostname or IP of the database server (often <code>localhost</code>).</li> <li><code>&lt;db_port&gt;</code>: Port of the database service (e.g., <code>3306</code> for MySQL).</li> <li><code>&lt;SSH_USER&gt;</code>: Your SSH username.</li> <li><code>&lt;SSH_HOST&gt;</code>: IP address or hostname of the remote server.</li> </ul>"},{"location":"blog/comprehensive-guide-to-ssh-tunneling-file-transfers-and-key-based-authentication/#example","title":"Example","text":"<pre><code>ssh -L 3306:localhost:3306 user@remote-server.com\n</code></pre> <p>This command forwards traffic from <code>localhost:3306</code> on your local machine to <code>localhost:3306</code> on <code>remote-server.com</code>. You can then connect to the remote MySQL database using a local MySQL client.</p>"},{"location":"blog/comprehensive-guide-to-ssh-tunneling-file-transfers-and-key-based-authentication/#remote-port-forwarding","title":"Remote Port Forwarding","text":"<p>Remote port forwarding forwards a port from the remote server to a local machine.</p>"},{"location":"blog/comprehensive-guide-to-ssh-tunneling-file-transfers-and-key-based-authentication/#example-command_1","title":"Example Command","text":"<pre><code>ssh -R &lt;remote_port&gt;:&lt;local_host&gt;:&lt;local_port&gt; &lt;SSH_USER&gt;@&lt;SSH_HOST&gt;\n</code></pre> <ul> <li><code>&lt;remote_port&gt;</code>: Port on the remote server to forward.</li> <li><code>&lt;local_host&gt;</code>: Hostname or IP of your local machine (often <code>localhost</code>).</li> <li><code>&lt;local_port&gt;</code>: Port on your local machine.</li> </ul>"},{"location":"blog/comprehensive-guide-to-ssh-tunneling-file-transfers-and-key-based-authentication/#example_1","title":"Example","text":"<pre><code>ssh -R 8080:localhost:80 user@remote-server.com\n</code></pre> <p>This forwards traffic from <code>remote-server.com:8080</code> to <code>localhost:80</code> on your local machine.</p>"},{"location":"blog/comprehensive-guide-to-ssh-tunneling-file-transfers-and-key-based-authentication/#mysql-tunneling-example","title":"MySQL Tunneling Example","text":"<p>Below is an example script to tunnel MySQL traffic using SSH. Ensure MySQL is installed on both the client and remote server and that necessary permissions are set up:</p>"},{"location":"blog/comprehensive-guide-to-ssh-tunneling-file-transfers-and-key-based-authentication/#prerequisites-remote-server","title":"Prerequisites (Remote Server)","text":"<ul> <li>MySQL installed and running on the remote server.</li> <li>Proper permissions set for SSH access and MySQL user (<code>dbuser</code> in this example).</li> </ul>"},{"location":"blog/comprehensive-guide-to-ssh-tunneling-file-transfers-and-key-based-authentication/#example-script","title":"Example Script","text":"<pre><code># Configuration\nSSH_PORT=22\nHOST=example.com\nUSER=username\n\nLOCAL_PORT=5523\nREMOTE_DB_HOST=127.0.0.1\nREMOTE_DB_PORT=3306\n\n# Establish SSH Tunnel (Run on client machine)\nssh -f ${USER}@${HOST} -p ${SSH_PORT} -L ${LOCAL_PORT}:${REMOTE_DB_HOST}:${REMOTE_DB_PORT} -N\n\n# Connect to MySQL (Run on client machine)\nmysql -u dbuser -p -h 127.0.0.1 -P ${LOCAL_PORT}\n</code></pre>"},{"location":"blog/comprehensive-guide-to-ssh-tunneling-file-transfers-and-key-based-authentication/#explanation","title":"Explanation","text":"<ol> <li> <p>Variables:</p> <ul> <li><code>SSH_PORT</code>: Port used for SSH connections (default is <code>22</code>).</li> <li><code>HOST</code>: Remote server\u2019s address.</li> <li><code>USER</code>: Your SSH username.</li> <li><code>LOCAL_PORT</code>: Local port to forward MySQL traffic (e.g., <code>5523</code>).</li> <li><code>REMOTE_DB_HOST</code>: Remote database host (usually <code>127.0.0.1</code>).</li> <li><code>REMOTE_DB_PORT</code>: Remote database port (default for MySQL is <code>3306</code>).</li> </ul> </li> <li> <p>SSH Command:</p> <ul> <li><code>-f</code>: Runs command in the background.</li> <li><code>-L</code>: Sets up local port forwarding from <code>LOCAL_PORT</code> to <code>REMOTE_DB_HOST:REMOTE_DB_PORT</code>.</li> <li><code>-N</code>: Prevents execution of remote commands (only sets up the tunnel).</li> </ul> </li> <li> <p>MySQL Connection:</p> <p>Connects to MySQL using the local tunnel.</p> <ul> <li><code>-u dbuser</code>: Specifies MySQL username.</li> <li><code>-p</code>: Prompts for password.</li> <li><code>-h 127.0.0.1</code>: Connects to localhost (tunneled).</li> <li><code>-P ${LOCAL_PORT}</code>: Specifies local port for the tunnel.</li> </ul> </li> </ol>"},{"location":"blog/comprehensive-guide-to-ssh-tunneling-file-transfers-and-key-based-authentication/#usage","title":"Usage","text":"<ol> <li>Ensure MySQL is installed and accessible on both client and remote servers.</li> <li>Run the SSH tunneling script on the client machine to establish the tunnel.</li> <li>Use the MySQL command to connect to the remote database via the local tunnel (<code>localhost:5523</code> in this example).</li> </ol> <p>This method securely encrypts MySQL traffic, maintaining data privacy during transmission. Adjust ports and credentials as per your specific setup.</p>"},{"location":"blog/comprehensive-guide-to-ssh-tunneling-file-transfers-and-key-based-authentication/#secure-file-transfer-protocol-sftp","title":"Secure File Transfer Protocol (SFTP)","text":"<p>SFTP <sup>1</sup> is a secure way to transfer files between your local machine and a remote server using SSH. It encrypts both commands and data, providing a high level of security.</p>"},{"location":"blog/comprehensive-guide-to-ssh-tunneling-file-transfers-and-key-based-authentication/#steps-to-use-sftp","title":"Steps to Use SFTP","text":"<ol> <li>Connect to the Remote Server</li> </ol> <p>Use the following command to start an SFTP session:</p> <pre><code>sftp &lt;SSH_USER&gt;@&lt;SSH_HOST&gt;\n</code></pre> <ul> <li>Replace <code>&lt;SSH_USER&gt;</code> with your SSH username.</li> <li> <p>Replace <code>&lt;SSH_HOST&gt;</code> with your remote server's IP address or hostname.</p> </li> <li> <p>Common SFTP Commands</p> <ul> <li><code>ls</code>: List files on the remote server.</li> <li><code>cd &lt;directory&gt;</code>: Change directory on the remote server.</li> <li><code>get &lt;remote_file&gt;</code>: Download a file from the remote server.</li> <li><code>put &lt;local_file&gt;</code>: Upload a file to the remote server.</li> <li><code>exit</code>: Close the SFTP session.</li> </ul> </li> </ul>"},{"location":"blog/comprehensive-guide-to-ssh-tunneling-file-transfers-and-key-based-authentication/#example-usage","title":"Example Usage","text":"<pre><code>sftp username@example.com\nsftp&gt; ls\nsftp&gt; cd /path/to/directory\nsftp&gt; get remote_file.txt\nsftp&gt; put local_file.txt\nsftp&gt; exit\n</code></pre>"},{"location":"blog/comprehensive-guide-to-ssh-tunneling-file-transfers-and-key-based-authentication/#secure-copy-scp","title":"Secure Copy (SCP)","text":"<p>SCP <sup>2</sup> allows you to securely transfer files between hosts using SSH. It's a straightforward way to copy files securely.</p>"},{"location":"blog/comprehensive-guide-to-ssh-tunneling-file-transfers-and-key-based-authentication/#example-commands","title":"Example Commands","text":"<ul> <li>Copy a file from local to remote:</li> </ul> <pre><code>scp &lt;local_file&gt; &lt;SSH_USER&gt;@&lt;SSH_HOST&gt;:&lt;remote_path&gt;\n</code></pre> <ul> <li>Copy a file from remote to local:</li> </ul> <pre><code>scp &lt;SSH_USER&gt;@&lt;SSH_HOST&gt;:&lt;remote_file&gt; &lt;local_path&gt;\n</code></pre>"},{"location":"blog/comprehensive-guide-to-ssh-tunneling-file-transfers-and-key-based-authentication/#ssh-agent-forwarding","title":"SSH Agent Forwarding","text":"<p>SSH agent forwarding <sup>3</sup> allows you to use your local SSH keys on remote servers, enabling seamless access to additional remote servers without copying keys.</p>"},{"location":"blog/comprehensive-guide-to-ssh-tunneling-file-transfers-and-key-based-authentication/#usage_1","title":"Usage","text":"<pre><code>ssh -A &lt;SSH_USER&gt;@&lt;SSH_HOST&gt;\n</code></pre>"},{"location":"blog/comprehensive-guide-to-ssh-tunneling-file-transfers-and-key-based-authentication/#dynamic-port-forwarding-socks-proxy","title":"Dynamic Port Forwarding (SOCKS Proxy)","text":"<p>Using SSH, you can create a SOCKS proxy <sup>4</sup> that routes traffic from applications through the SSH tunnel, allowing secure browsing.</p>"},{"location":"blog/comprehensive-guide-to-ssh-tunneling-file-transfers-and-key-based-authentication/#command","title":"Command","text":"<pre><code>ssh -D &lt;local_port&gt; &lt;SSH_USER&gt;@&lt;SSH_HOST&gt;\n</code></pre>"},{"location":"blog/comprehensive-guide-to-ssh-tunneling-file-transfers-and-key-based-authentication/#ssh-key-based-authentication","title":"SSH Key-Based Authentication","text":"<p>For enhanced security, use SSH keys instead of passwords for authentication <sup>5</sup>. This prevents unauthorized access and simplifies the login process.</p>"},{"location":"blog/comprehensive-guide-to-ssh-tunneling-file-transfers-and-key-based-authentication/#steps-to-set-up","title":"Steps to Set Up","text":"<ol> <li>Generate an SSH Key Pair</li> </ol> <pre><code>ssh-keygen -t rsa -b 4096 -C \"&lt;email@example.com&gt;\"\n</code></pre> <ol> <li>Copy the Public Key to the Remote Server</li> </ol> <pre><code>ssh-copy-id &lt;SSH_USER&gt;@&lt;SSH_HOST&gt;\n</code></pre> <p>This setup allows you to log in securely without entering a password.</p>"},{"location":"blog/comprehensive-guide-to-ssh-tunneling-file-transfers-and-key-based-authentication/#example-ssh-key-based-authentication-with-github","title":"Example: SSH Key-Based Authentication with GitHub","text":"<p>Setting up SSH key-based authentication with GitHub <sup>6</sup> enhances security while maintaining ease of use for your development workflows. Recently, GitHub deprecated the use of RSA keys with SHA-1 due to security concerns, requiring users to switch to more secure algorithms like <code>ed25519</code> <sup>7</sup>.</p>"},{"location":"blog/comprehensive-guide-to-ssh-tunneling-file-transfers-and-key-based-authentication/#error-message","title":"Error Message","text":"<p>If you encounter the following error:</p> <pre><code>ERROR: You're using an RSA key with SHA-1, which is no longer allowed.\nPlease use a newer client or a different key type.\n</code></pre> <p>You need to switch to <code>ed25519</code>:</p>"},{"location":"blog/comprehensive-guide-to-ssh-tunneling-file-transfers-and-key-based-authentication/#steps-to-set-up-ssh-key-based-authentication","title":"Steps to Set Up SSH Key-Based Authentication","text":"<ol> <li>Generate an SSH Key Pair</li> </ol> <p>Use <code>ssh-keygen</code> to generate a new SSH key pair with the Ed25519 algorithm:</p> <pre><code>ssh-keygen -t ed25519 -C \"your-email-address\"\n</code></pre> <p>Follow the prompts to save the key pair in the default location (<code>~/.ssh/id_ed25519</code>).</p> <ol> <li>Start the SSH Agent</li> </ol> <p>To manage your SSH keys, start the SSH agent:</p> <pre><code>eval \"$(ssh-agent -s)\"\n</code></pre> <ol> <li>Add the SSH Private Key to the SSH Agent</li> </ol> <p>Add your SSH private key to the SSH agent:</p> <pre><code>ssh-add ~/.ssh/id_ed25519\n</code></pre> <ol> <li>Copy the SSH Public Key to GitHub</li> </ol> <p>Retrieve your SSH public key and copy its contents:</p> <pre><code>cat ~/.ssh/id_ed25519.pub\n</code></pre> <p>Copy the entire output.</p> <ol> <li> <p>Add the SSH Key to GitHub</p> </li> <li> <p>Go to GitHub Settings &gt; SSH and GPG keys.</p> </li> <li> <p>Click on \"New SSH key\" or \"Add SSH key\", paste your SSH public key, and give it a descriptive title.</p> </li> <li> <p>Verify SSH Connection to GitHub</p> </li> </ol> <p>Test your SSH connection to GitHub:</p> <pre><code>ssh -T git@github.com\n</code></pre> <p>You should see a message indicating successful authentication.</p>"},{"location":"blog/comprehensive-guide-to-ssh-tunneling-file-transfers-and-key-based-authentication/#example-clone-a-github-repository-using-ssh","title":"Example: Clone a GitHub Repository Using SSH","text":"<p>To clone a repository from GitHub:</p> <pre><code>git clone git@github.com:your-username/your-repo.git\n</code></pre> <p>Replace <code>your-username</code> and <code>your-repo</code> with your GitHub username and repository name.</p>"},{"location":"blog/comprehensive-guide-to-ssh-tunneling-file-transfers-and-key-based-authentication/#configuration-update-for-ssh-ed25519","title":"Configuration Update for <code>ssh-ed25519</code>","text":"<p>If your system does not support <code>ed25519</code> by default, update your SSH configuration file (<code>~/.ssh/config</code>) to include it.</p> <p>For example, i once changed from:</p> <pre><code>HostKeyAlgorithms ssh-rsa,rsa-sha2-512\nPubkeyAcceptedKeyTypes ssh-rsa\n</code></pre> <p>to:</p> <pre><code>HostKeyAlgorithms ssh-rsa,rsa-sha2-512,ssh-ed25519\nPubkeyAcceptedKeyTypes ssh-rsa,ssh-ed25519\n</code></pre> <p>This ensures compatibility with <code>ed25519</code> keys, providing enhanced security for your connections.</p>"},{"location":"blog/comprehensive-guide-to-ssh-tunneling-file-transfers-and-key-based-authentication/#conclusion","title":"Conclusion","text":"<p>SSH is a versatile tool that offers secure communication, file transfers, and more. By using SSH, you can protect your data and manage remote servers efficiently, especially with updated algorithms like <code>ed25519</code> ensuring enhanced security.</p>"},{"location":"blog/comprehensive-guide-to-ssh-tunneling-file-transfers-and-key-based-authentication/#related-posts","title":"Related Posts","text":"<ul> <li>Mastering SSH and File Transfers to Remote servers: A Beginner's Handbook</li> </ul> <ol> <li> <p>SSH File Transfer Protocol (SFTP) \u21a9</p> </li> <li> <p>Secure Copy Protocol (SCP) \u21a9</p> </li> <li> <p>SSH Agent Forwarding \u21a9</p> </li> <li> <p>SOCKS Proxy (Dynamic Port Forwarding) \u21a9</p> </li> <li> <p>SSH Key Authentication \u21a9</p> </li> <li> <p>Connecting to GitHub with SSH \u21a9</p> </li> <li> <p>Improving Git Protocol Security on GitHub \u21a9</p> </li> </ol>"},{"location":"blog/understanding-git-pull-vs-merge-in-git-workflow/","title":"Understanding Git Pull vs Merge in Git Workflow","text":""},{"location":"blog/understanding-git-pull-vs-merge-in-git-workflow/#introduction","title":"Introduction","text":"<p>Did you know <code>git pull</code> and <code>git merge</code> are quite similar commands ?</p> <p>When it comes to managing branches in Git, understanding the nuances between <code>git pull</code> and <code>git merge</code> can significantly impact your workflow's efficiency.</p> <p>Both commands, <code>git pull</code> and <code>git merge</code>, serve the purpose of integrating changes from a remote branch (<code>dev</code>) into your local branch. However, they employ different strategies to achieve this.</p> <p>In this exploration, we'll delve into the differences between <code>git pull origin dev</code> and <code>git merge origin/dev</code>, unraveling their distinct approaches and highlighting the practical implications of their usage. Understanding these differences will empower you to make informed decisions while managing your Git branches effectively.</p> <p>Let's dive into the nuances of <code>git pull</code> and <code>git merge</code> to optimize your Git workflow and ensure seamless collaboration across teams.</p>"},{"location":"blog/understanding-git-pull-vs-merge-in-git-workflow/#git-pull-vs-merge","title":"Git Pull vs Merge","text":""},{"location":"blog/understanding-git-pull-vs-merge-in-git-workflow/#git-checkout-master-git-merge-dev","title":"<code>git checkout master &amp;&amp; git merge dev</code>","text":"<ol> <li><code>git checkout master</code>: Switches to the local <code>master</code> branch.</li> <li><code>git merge dev</code>: Attempts to merge the local <code>dev</code> into the local <code>master</code>.</li> </ol>"},{"location":"blog/understanding-git-pull-vs-merge-in-git-workflow/#git-checkout-master-git-merge-origindev","title":"<code>git checkout master &amp;&amp; git merge origin/dev</code>","text":"<ol> <li><code>git checkout master</code>: Switches to the local <code>master</code> branch.</li> <li><code>git merge origin/dev</code>: Attempts to merge the remote <code>dev</code> into the local <code>master</code>.</li> </ol>"},{"location":"blog/understanding-git-pull-vs-merge-in-git-workflow/#git-checkout-master-git-pull-origin-dev","title":"<code>git checkout master &amp;&amp; git pull origin dev</code>","text":"<ol> <li><code>git checkout master</code>: Switches to the local <code>master</code> branch.</li> <li><code>git pull origin dev</code>:<ul> <li>Fetches changes from the remote <code>dev</code> to the local <code>dev</code> like <code>git fetch origin dev</code>.</li> <li>Attempts to merge the remote <code>dev</code> into the local <code>master</code> like <code>git merge origin/dev</code>.</li> </ul> </li> </ol>"},{"location":"blog/understanding-git-pull-vs-merge-in-git-workflow/#understanding-the-differences","title":"Understanding the Differences","text":"<p>Technically, <code>git pull origin dev</code> and <code>git merge origin/dev</code> both aim to integrate changes from a remote branch (<code>dev</code>) into your current local branch.</p> <p>However, they differ in approach:</p> <code>git pull origin dev</code> <code>git merge origin/dev</code> <ul> <li>Combines <code>git fetch</code> (retrieve changes from the remote repository) and <code>git merge</code> (integrate changes into your local branch) in one step.</li> <li>Fetches changes from the remote <code>dev</code> branch and immediately merges them into your current local branch.</li> </ul> <ul> <li>Directly attempts to merge changes from the remote <code>dev</code> branch into your current local branch without explicitly fetching changes separately.</li> <li>Assumes you already have the remote branch's changes available in your local repository.</li> </ul>"},{"location":"blog/understanding-git-pull-vs-merge-in-git-workflow/#practical-considerations","title":"Practical Considerations","text":"<ul> <li><code>git pull</code> is often preferred for its convenience and safety in ensuring your local branch is up-to-date with the remote before merging.</li> <li><code>git merge</code> requires manually fetching changes beforehand.</li> <li>If unsure about the status of your local branch compared to the remote or if there might be new changes on the remote branch, <code>git pull origin dev</code> is a safer option.</li> <li>It fetches and merges changes in a single step, reducing chances of conflicts due to outdated local information.</li> </ul>"},{"location":"blog/understanding-git-pull-vs-merge-in-git-workflow/#conclusion","title":"Conclusion","text":"<ul> <li><code>git pull</code> is essentially a <code>git fetch</code> followed by a <code>git merge</code> in one step, useful for updating your local branch with changes from a remote branch.</li> <li><code>git pull origin dev</code> is equivalent to <code>git fetch origin dev</code> + <code>git merge origin/dev</code>.</li> <li>Using <code>git pull</code> can be more concise and convenient, but separating actions (fetch and merge) provides explicit control over each step, allowing review of changes fetched from the remote branch before merging into the local branch.</li> </ul>"},{"location":"blog/understanding-git-pull-vs-merge-in-git-workflow/#related-pages","title":"Related pages","text":"<ul> <li>Managing Local Modifications and Remote Changes in Git</li> <li>Mastering Git Merge Strategies: A Developer's Guide</li> <li>Nesting Repositories with Git Submodules: A Newbie's Guide</li> <li>Mastering Git Branch Handling: Strategies for Deletion and Recovery</li> </ul>"},{"location":"blog/nesting-repositories-with-git-submodules-a-newbies-guide/","title":"Nesting Repositories with Git Submodules: A Newbie's Guide","text":""},{"location":"blog/nesting-repositories-with-git-submodules-a-newbies-guide/#introduction","title":"Introduction","text":"<p>Are you facing the challenge of handling multiple code pieces scattered across different repositories in your project, unsure how to seamlessly integrate them?</p> <p>For developers new to the concept, managing disparate repositories within a single project can be overwhelming. Git submodules offer a guiding light, acting as a map through the maze of organizing and linking these separate codebases or libraries within your projects.</p>"},{"location":"blog/nesting-repositories-with-git-submodules-a-newbies-guide/#real-life-scenario-aligning-frontend-and-backend-strategies","title":"Real-Life Scenario: Aligning Frontend and Backend Strategies","text":"<p>Back in 2022, I found myself as the lead developer overseeing the backend team, while collaborating closely with a talented frontend developer responsible for crafting engaging user interfaces.</p> <p>Our teams operated independently, each excelling in our specialized domains. However, this independence led to distinct branch strategies. The backend team adopted a unique approach, separate from the frontend's strategy.</p> <p>Over time, this divergence in branch strategies caused disparities between our repositories' states. Aligning frontend changes with the evolving backend structures became a complex task. Ensuring seamless integration between our frontend branches and specific backend versions posed challenges.</p> <p>Recognizing these challenges, I engaged in a discussion with the frontend developer. We brainstormed solutions to synchronize versions and segregate our branch strategies effectively.</p> <p>During our deliberation, we explored the idea of utilizing Git submodules. It wasn't merely about syncing versions but aligning our separate branch strategies while maintaining distinct team autonomy.</p> <p>The proposal envisioned Git submodules as the bridge between our frontend and backend repositories, facilitating version synchronization and accommodating separate yet aligned branch strategies. This approach aimed to streamline collaboration and ensure smoother integration between our teams' work.</p> <p>Motivated by the vision of enhanced collaboration and harmonized branch strategies, we collectively agreed to integrate Git submodules. This decision promised a more cohesive development environment, allowing both teams to synchronize versions and align branch strategies seamlessly.</p>"},{"location":"blog/nesting-repositories-with-git-submodules-a-newbies-guide/#extending-to-a-computer-vision-project","title":"Extending to a Computer Vision Project","text":"<p>Additionally, in a computer vision project, I encountered a similar challenge. Testing code from various repositories required frequent modifications, causing inefficiencies. Managing these disparate codebases led me to prefer a unified repository managed with Git submodules. This approach enabled me to centralize and manage all required codebases efficiently, adapting them as needed.</p> <p>Think of Git submodules as containers that neatly organize and link external repositories to your main project\u2014providing a solution to the discomfort of handling disjointed pieces of code. Join us as we embark on a journey to explore how to clone, set up, and effortlessly synchronize these submodules within your projects.</p> <p></p> <p>This document simplifies Git submodules in a beginner-friendly way, offering developers new to the concept a clear path to effectively manage multiple repositories as cohesive parts of their projects.</p>"},{"location":"blog/nesting-repositories-with-git-submodules-a-newbies-guide/#cloning-a-repository-with-submodules-and-cloning-a-specific-submodule","title":"Cloning a Repository with Submodules and Cloning a Specific Submodule","text":""},{"location":"blog/nesting-repositories-with-git-submodules-a-newbies-guide/#clone-the-main-repository","title":"Clone the Main Repository","text":"<ol> <li> <p>Open your terminal and navigate to the desired directory for cloning:</p> <pre><code>cd /desired/directory/path\n</code></pre> </li> <li> <p>Clone the main repository:</p> <pre><code>git clone &lt;repository_url&gt;\n</code></pre> <p>Replace <code>&lt;repository_url&gt;</code> with the URL of the main repository.</p> </li> <li> <p>Change your working directory to the repository:</p> <pre><code>cd &lt;repository_directory&gt;\n</code></pre> <p>Replace <code>&lt;repository_directory&gt;</code> with the name of the cloned directory.</p> </li> </ol>"},{"location":"blog/nesting-repositories-with-git-submodules-a-newbies-guide/#initialize-and-update-submodules","title":"Initialize and Update Submodules","text":"<ol> <li> <p>Initialize the submodules:</p> <pre><code>git submodule init\n</code></pre> <p>This sets up necessary Git configurations for submodules.</p> </li> <li> <p>Update the submodules:</p> <pre><code>git submodule update\n</code></pre> <p>This fetches submodule contents based on references in the main repository.</p> </li> </ol>"},{"location":"blog/nesting-repositories-with-git-submodules-a-newbies-guide/#clone-a-specific-submodule","title":"Clone a Specific Submodule","text":"<ol> <li> <p>Clone a specific submodule:</p> <pre><code>git submodule update --recursive -- &lt;submodule_path&gt;\n</code></pre> <p>Replace <code>&lt;submodule_path&gt;</code> with the specific submodule path. This command updates only the specified submodule and its dependencies, leaving others unchanged. - The <code>--recursive</code> flag initializes nested submodules within the specified submodule.</p> </li> </ol> <p>Now that you've successfully cloned the main repository along with its submodules, let's explore how to create and manage submodules within an existing repository.</p> <p>If the update fails, you may want to read this stackoverflow thread</p>"},{"location":"blog/nesting-repositories-with-git-submodules-a-newbies-guide/#creating-a-git-submodule","title":"Creating a Git Submodule","text":"<ol> <li> <p>Move to the parent repository's root directory:</p> <pre><code>cd /path/to/parent/repository\n</code></pre> </li> <li> <p>Add the Submodule:</p> <pre><code>git submodule add &lt;submodule_repository_url&gt; &lt;submodule_path&gt;\n</code></pre> <ul> <li><code>&lt;submodule_repository_url&gt;</code>: URL of the submodule repository.</li> <li><code>&lt;submodule_path&gt;</code>: Path within the parent repository to place the submodule.</li> </ul> <p>Example:</p> <pre><code>git submodule add https://github.com/example/submodule-repo.git path/to/submodule\n</code></pre> </li> <li> <p>Commit the Changes:</p> <pre><code>git commit -m \"Add submodule: &lt;submodule_path&gt;\"\n</code></pre> <p>Replace <code>&lt;submodule_path&gt;</code> with the actual path used when adding the submodule.</p> </li> <li> <p>Push Changes (Optional):</p> <pre><code>git push\n</code></pre> </li> </ol> <p>Now, let's delve into pulling changes from both the main repository and its submodules to keep your local copy up to date.</p>"},{"location":"blog/nesting-repositories-with-git-submodules-a-newbies-guide/#pulling-changes-from-the-main-repository-and-submodules","title":"Pulling Changes from the Main Repository and Submodules","text":""},{"location":"blog/nesting-repositories-with-git-submodules-a-newbies-guide/#update-the-main-repository","title":"Update the Main Repository","text":"<ol> <li> <p>Navigate to the main repository's directory:</p> <pre><code>cd /path/to/main/repository\n</code></pre> </li> <li> <p>Fetch the latest changes:</p> <pre><code>git pull origin main\n</code></pre> <p>This command fetches and merges the latest changes from the remote repository into your local <code>main</code> branch.</p> </li> </ol>"},{"location":"blog/nesting-repositories-with-git-submodules-a-newbies-guide/#update-submodules","title":"Update Submodules","text":"<ol> <li> <p>Update submodules to the latest commits:</p> <pre><code>git submodule update --remote\n</code></pre> <p>This updates each submodule to the commit specified by the main repository.</p> </li> <li> <p>Update a specific submodule:</p> <ul> <li> <p>Using <code>git submodule update --remote &lt;submodule_path&gt;</code>:</p> <pre><code>git submodule update --remote path/to/submodule\n</code></pre> </li> <li> <p>Or manually in the submodule directory:</p> <pre><code>cd path/to/submodule\ngit pull origin master\n</code></pre> </li> </ul> </li> </ol>"},{"location":"blog/nesting-repositories-with-git-submodules-a-newbies-guide/#pushing-updated-submodule-references-bonus","title":"Pushing Updated Submodule References (Bonus)","text":"<ol> <li> <p>Inside the main repository, after updating submodule references:</p> <pre><code>git commit -am \"Update submodule references\"\ngit push origin main\n</code></pre> </li> <li> <p>If there are changes in the submodules themselves:</p> <pre><code>cd path/to/submodule\ngit commit -am \"Update submodule\"\ngit push origin master\n</code></pre> </li> </ol>"},{"location":"blog/nesting-repositories-with-git-submodules-a-newbies-guide/#removing-a-git-submodule","title":"Removing a Git Submodule","text":"<p>To remove a submodule from your repository, follow these steps. The following instructions are based on a detailed explanation found on Stack Overflow (source).</p> <ol> <li> <p>Move to the parent repository's root directory:</p> <p>Before removing the submodule, move it temporarily to a different location within your working directory.</p> <pre><code>mv &lt;submodule_path&gt; &lt;submodule_path&gt;_tmp\n</code></pre> </li> <li> <p>Deinitialize the Submodule:     Use the following command to deinitialize the submodule:</p> <pre><code>git submodule deinit -f -- &lt;submodule_path&gt;\n</code></pre> </li> <li> <p>Remove Submodule Configuration:     Delete the submodule's configuration from the <code>.git/modules</code> directory:</p> <pre><code>rm -rf .git/modules/&lt;submodule_path&gt;\n</code></pre> </li> <li> <p>Remove Submodule from Repository:     There are two options to remove the submodule from the repository:</p> <p>a. If you want to completely remove it from the repository and your working tree:</p> <pre><code>git rm -f &lt;submodule_path&gt;\n</code></pre> <p>Note: Replace <code>&lt;submodule_path&gt;</code> with the actual submodule path.</p> <p>b. If you want to keep it in your working tree but remove it from the repository:</p> <pre><code>git rm --cached &lt;submodule_path&gt;\n</code></pre> </li> <li> <p>Restore Submodule (Optional):     If you moved the submodule in step 0, restore it to its original location:</p> <pre><code>mv &lt;submodule_path&gt;_tmp &lt;submodule_path&gt;\n</code></pre> </li> </ol> <p>By following these steps, you'll effortlessly manage main repositories and their submodules, ensuring your projects are up to date.</p> <p>Stay tuned for more Git tips and tricks on our blog for seamless collaboration and version control!</p>"},{"location":"blog/nesting-repositories-with-git-submodules-a-newbies-guide/#related-pages","title":"Related pages","text":"<ul> <li>Managing Local Modifications and Remote Changes in Git</li> <li>Mastering Git Merge Strategies: A Developer's Guide</li> <li>Understanding Git Pull vs Merge in Git Workflow</li> <li>Mastering Git Branch Handling: Strategies for Deletion and Recovery</li> </ul>"},{"location":"blog/mastering-git-branch-handling-strategies-for-deletion-and-recovery/","title":"Mastering Git Branch Handling: Strategies for Deletion and Recovery","text":""},{"location":"blog/mastering-git-branch-handling-strategies-for-deletion-and-recovery/#introduction","title":"Introduction","text":"<p>Are you looking to master the art of handling Git branches with finesse?</p> <p>Git branches are pivotal to managing project versions effectively. Understanding how to delete branches locally and remotely, as well as recovering deleted branches, is essential for maintaining a clean and organized repository. This guide serves as your compass, navigating you through the realm of Git branch management and ensuring a smooth and efficient version control process.</p>"},{"location":"blog/mastering-git-branch-handling-strategies-for-deletion-and-recovery/#deleting-a-branch-locally-cli","title":"Deleting a Branch Locally (CLI)","text":"<p>Deleting a branch in Git locally can be done using the <code>git branch -d</code> command:</p> <pre><code>git branch -d &lt;branch-name&gt;\n</code></pre>"},{"location":"blog/mastering-git-branch-handling-strategies-for-deletion-and-recovery/#deleting-a-branch-remotely-cli","title":"Deleting a Branch Remotely (CLI)","text":"<p>To delete a remote branch from your local repository and push that deletion to the remote repository (e.g., GitHub), use:</p> <pre><code>git push origin --delete &lt;branch-name&gt;\n</code></pre>"},{"location":"blog/mastering-git-branch-handling-strategies-for-deletion-and-recovery/#deleting-a-branch-online-github-interface","title":"Deleting a Branch Online (GitHub Interface)","text":"<ul> <li>Go to the repository on GitHub.</li> <li>Click on the \"Branches\" tab.</li> <li>Locate the branch you want to delete.</li> <li>Click on the trash can icon or \"Delete\" button next to the branch name.</li> <li>Confirm the deletion if prompted.</li> </ul>"},{"location":"blog/mastering-git-branch-handling-strategies-for-deletion-and-recovery/#fetching-and-cleaning-up-deletions","title":"Fetching and Cleaning Up Deletions","text":"<p>After deleting branches remotely, update your local repository to reflect these deletions:</p> <pre><code>git fetch --prune\n</code></pre> <p>This command fetches changes from the remote and prunes (removes) any remote-tracking references that no longer exist on the remote repository.</p>"},{"location":"blog/mastering-git-branch-handling-strategies-for-deletion-and-recovery/#recovering-deleted-branches","title":"Recovering Deleted Branches","text":"<p>If a branch was mistakenly deleted and not yet pruned, it might be recoverable.</p> <ol> <li> <p>Check the Reflog:    Use <code>git reflog show</code> to view recently deleted branches and find the one you want to restore.</p> </li> <li> <p>Recover the Branch:    Identify the commit hash associated with the deleted branch in the reflog and create a new branch at that commit:</p> </li> </ol> <pre><code>git checkout -b &lt;branch-name&gt; &lt;commit-hash&gt;\n</code></pre> <p>Replace <code>&lt;branch-name&gt;</code> with the branch name and <code>&lt;commit-hash&gt;</code> with the commit hash from the reflog.</p> <p>Note: The ability to recover a deleted branch depends on recent activity and whether Git has pruned references.</p>"},{"location":"blog/mastering-git-branch-handling-strategies-for-deletion-and-recovery/#related-pages","title":"Related pages","text":"<ul> <li>Managing Local Modifications and Remote Changes in Git</li> <li>Mastering Git Merge Strategies: A Developer's Guide</li> <li>Understanding Git Pull vs Merge in Git Workflow</li> <li>Nesting Repositories with Git Submodules: A Newbie's Guide</li> </ul>"},{"location":"blog/simplifying-large-file-management-in-git-with-git-lfs/","title":"Simplifying Large File Management in Git with Git LFS","text":""},{"location":"blog/simplifying-large-file-management-in-git-with-git-lfs/#introduction","title":"Introduction","text":""},{"location":"blog/simplifying-large-file-management-in-git-with-git-lfs/#introduction_1","title":"Introduction","text":"<p>Have you ever faced the challenge of managing large files within a Git repository?</p> <p>Whether you're an experienced developer or just beginning your coding journey, dealing with large files in version control can be perplexing. Often, developers resort to <code>.gitignore</code> to exclude files, but what if there are essential large files crucial for your project's integrity?</p> <p>Enter Git LFS (Large File Storage), a solution designed to revolutionize how Git repositories handle large files. While some files are pivotal to track, keeping repositories lean and efficient remains a priority.</p> <p>This guide unlocks the potential of Git LFS, providing a step-by-step approach to seamlessly incorporate it into your version control workflow. Discover how Git LFS streamlines large file management, ensuring your repository stays clean and optimized.</p> <p>Let's navigate the realm of large file management in Git, ensuring your projects stay organized and efficient.</p>"},{"location":"blog/simplifying-large-file-management-in-git-with-git-lfs/#steps-to-implement-git-lfs","title":"Steps to Implement Git LFS","text":""},{"location":"blog/simplifying-large-file-management-in-git-with-git-lfs/#1-installing-git-lfs","title":"1. Installing Git LFS","text":"<p>Begin by installing Git LFS. Visit the Git LFS website for installation instructions tailored to your operating system.</p>"},{"location":"blog/simplifying-large-file-management-in-git-with-git-lfs/#2-initializing-git-lfs","title":"2. Initializing Git LFS","text":"<p>In your repository, run the command:</p> <pre><code>git lfs install\n</code></pre> <p>This command sets up Git LFS for your project, preparing it to manage large files.</p>"},{"location":"blog/simplifying-large-file-management-in-git-with-git-lfs/#3-tracking-large-files","title":"3. Tracking Large Files","text":"<p>Identify the large files you want to store using Git LFS and begin tracking them. You can manually specify these files using:</p> <pre><code>git lfs track \"path/to/your/large/file\"\n</code></pre> Example: Track files <code>.avi</code> and <code>.gif</code> files larger than 19MB <pre><code>For an efficient approach to track multiple files of specific extensions and sizes (such as `.avi` and `.gif` files larger than 19MB), a script can simplify the process. For exa:\n\n```bash\n#!/bin/bash\n\n# Find .avi files larger than 19MB and track them with Git LFS\nfind . -type f -name \"*.avi\" -size +19M | while read -r file; do\n    git lfs track \"$file\"\ndone\n\n# Find .gif files larger than 19MB and track them with Git LFS\nfind . -type f -name \"*.gif\" -size +19M | while read -r file; do\n    git lfs track \"$file\"\ndone\n```\n\nEnsure to execute this script within your Git repository directory.\n</code></pre> <p>Always review in <code>.gitattributes</code> the file selections to confirm they match your requirements before committing changes to Git LFS.</p> <p>You should also note there is a quota limit. read more here</p>"},{"location":"blog/simplifying-large-file-management-in-git-with-git-lfs/#4-updating-gitattributes","title":"4. Updating <code>.gitattributes</code>","text":"<p>After tracking the files, update your <code>.gitattributes</code> file with the tracking information:</p> <pre><code>git add .gitattributes\ngit commit -m \"Track large .avi and .gif files with Git LFS\"\n</code></pre>"},{"location":"blog/simplifying-large-file-management-in-git-with-git-lfs/#5-commit-and-push-changes","title":"5. Commit and Push Changes","text":"<p>Following the usual Git workflow, add and commit your changes:</p> <pre><code>git add .\ngit commit -m \"Message\"\n</code></pre> <p>Finally, push the changes to your remote repository:</p> <pre><code>git push origin master\n</code></pre> <p>Assuming you're on the master branch, this step uploads the large files to the Git LFS server.</p> <p>By successfully implementing Git LFS, your large files are now efficiently managed within the repository, enhancing version control capabilities.</p> <p>For detailed installation instructions and additional information about Git LFS, refer to the Git LFS installation guide.</p>"},{"location":"blog/simplifying-large-file-management-in-git-with-git-lfs/#more-ressources","title":"More ressources","text":""},{"location":"blog/simplifying-large-file-management-in-git-with-git-lfs/#-untrack-with-git-lfs","title":"- untrack with git-lfs","text":""},{"location":"blog/managing-local-modifications-and-remote-changes-in-git/","title":"Managing Local Modifications and Remote Changes in Git","text":""},{"location":"blog/managing-local-modifications-and-remote-changes-in-git/#introduction","title":"Introduction","text":"<p>Git is, without discussion, a powerful version control system that enables collaborative development.</p> <p>Ever found yourself in a twist trying to mix changes you made with updates from others in Git?</p> <p>It's like trying to blend your cooking style with someone else's recipe without making a mess. Git's awesome for team coding, but when your tweaks clash with online updates, how do you sort it out ?</p> <p>Indeed, when local modifications clash with remote changes, navigating these conflicts efficiently becomes crucial. Let's explore different strategies to handle this situation effectively.</p> <p></p> <p>by Johnson Huang &lt;https://github.com/jshuang0520/git&gt;</p>"},{"location":"blog/managing-local-modifications-and-remote-changes-in-git/#options-for-handling-local-and-remote-changes-in-git","title":"Options for Handling Local and Remote Changes in Git","text":"<p>When you encounter local modifications and remote changes in your Git workflow, several options are available:</p>"},{"location":"blog/managing-local-modifications-and-remote-changes-in-git/#1-incorporating-local-changes-with-remote-changes","title":"1. Incorporating Local Changes with Remote Changes","text":"<p>If no conflicts exist between your local changes and the remote changes, use:</p> <pre><code>git pull --rebase\n</code></pre> <ul> <li>This command integrates your commits after the remote commits, making it seem as if your changes were made after the remote changes.</li> <li>Manual resolution is required if conflicts arise during the rebase process.</li> </ul> <p>To apply rebase whenever you do a <code>git pull</code>, run this command to modify git default behavior</p> <pre><code>git config pull.rebase true\n</code></pre> <p>Example Scenario: Consider a scenario where ...</p> <p>Pros:</p> <ul> <li>Keeps a linear, clean commit history.</li> <li>Integrates local changes after remote ones, maintaining chronological order.</li> </ul> <p>Cons:</p> <ul> <li>Requires manual resolution of conflicts that may arise during rebase.</li> </ul>"},{"location":"blog/managing-local-modifications-and-remote-changes-in-git/#2-preserving-local-changes-without-rebasing","title":"2. Preserving Local Changes without Rebasing","text":"<p>In this scenario:</p> <ul> <li>Git will consider the point where you started modifying without pulling as the base.</li> <li>There's no predetermined order or priority between your local commits and the remote commits.</li> <li>Git combines changes from your local repository and the remote commits into a single commit.</li> </ul> <p>You set the default pulling as no-rebase with</p> <pre><code>git config pull.rebase false\n</code></pre> <p>You pull with the no-rebase option with</p> <pre><code>git pull --no-rebase\n</code></pre> <p>Example Scenario: Suppose you've made substantial local changes and need to pull remote updates without modifying your commit history:</p> <pre><code>git pull --no-rebase origin main\n</code></pre> <p>Pros:</p> <ul> <li>Creates a single combined commit representing both local and remote changes.</li> <li>Retains the original commit structure without altering history.</li> </ul> <p>Cons:</p> <ul> <li>Might lose individual context from multiple local commits.</li> </ul>"},{"location":"blog/managing-local-modifications-and-remote-changes-in-git/#3-pulling-with-strict-fast-forward-mode-git-config-pullff-only","title":"3. Pulling with Strict Fast-Forward Mode (<code>git config pull.ff only</code>)","text":"<p>In this scenario:</p> <ul> <li>Git checks for any absence of remote commits.</li> <li>If no remote commits exist, it adds your commits seamlessly.</li> <li>However, conflicts prompt resolution when remote commits are present.</li> </ul> <p>You set the default pulling as no-rebase with</p> <pre><code>git config pull.ff only\n</code></pre> <p>You pull with the fast-forward option with</p> <pre><code>git pull --ff-only\n</code></pre> <p>Example Scenario: When ensuring a linear history is a priority and avoiding merge commits:</p> <pre><code>git pull --ff-only origin main\n</code></pre> <p>Pros:</p> <ul> <li>Enforces a strictly linear history if possible, avoiding merge commits.</li> <li>Facilitates a cleaner commit timeline for easier tracking.</li> </ul> <p>Cons:</p> <ul> <li>Requires conflict resolution if the remote branch diverges.</li> </ul>"},{"location":"blog/managing-local-modifications-and-remote-changes-in-git/#4-default-behavior-fast-forward-merge-git-config-pullff-merge","title":"4. Default Behavior: Fast-Forward Merge (<code>git config pull.ff merge</code>)","text":"<p>In this scenario (default behavior):</p> <ul> <li>Proceeds as usual when no remote commits are present.</li> <li>Performs a rebase based on the <code>git config pull.rebase</code> setting if remote commits exist.</li> </ul> <p>Example Scenario: Pulling changes with flexibility based on configured rebase settings:</p> <pre><code>git pull --ff-merge origin main\n</code></pre> <p>Pros:</p> <ul> <li>Offers flexibility based on configured rebase settings (<code>pull.rebase true/false</code>).</li> <li>Can accommodate both linear and non-linear commit histories.</li> </ul> <p>Cons:</p> <ul> <li>May result in a non-linear history with merge commits in certain scenarios.</li> </ul>"},{"location":"blog/managing-local-modifications-and-remote-changes-in-git/#5-git-push-force","title":"5. <code>git push --force</code>","text":"<ul> <li>Enables forceful pushing, overriding any changes made by others.</li> <li>Caution is advised as it can lead to the loss of other developers' work.</li> <li>Coordination with the team is crucial for a smooth collaboration process.</li> </ul> <p>Example Scenario: When pushing changes forcefully becomes necessary:</p> <pre><code>git push --force origin feature-branch\n</code></pre> <p>Pros:</p> <ul> <li>Allows correcting mistakes or overriding changes when needed.</li> <li>Provides a quick resolution to divergent branch issues.</li> </ul> <p>Cons:</p> <ul> <li>Risks losing or overwriting others' work, disrupting collaboration.</li> <li>Requires careful coordination within the team.</li> </ul>"},{"location":"blog/managing-local-modifications-and-remote-changes-in-git/#conclusion","title":"Conclusion","text":"<p>Choosing the right Git approach involves understanding the implications and trade-offs associated with each method. Experimenting with these options within the context of your team's workflow helps determine the most suitable approach for a smoother collaborative Git environment.</p> <p>Play it safe !</p> <p>In this guide, remember to play it safe: make backups as you work on different branches, decide if you're merging or rebasing changes like picking different tools for different jobs, and keep feature branches separate from the main code like organizing toys into different boxes. These simple tips will keep your code kitchen running smoothly!</p>"},{"location":"blog/managing-local-modifications-and-remote-changes-in-git/#related-pages","title":"Related pages","text":"<ul> <li>Mastering Git Merge Strategies: A Developer's Guide</li> <li>Understanding Git Pull vs Merge in Git Workflow</li> <li>Nesting Repositories with Git Submodules: A Newbie's Guide</li> <li>Mastering Git Branch Handling: Strategies for Deletion and Recovery</li> </ul>"},{"location":"blog/mastering-git-merge-strategies-a-developers-guide/","title":"Mastering Git Merge Strategies: A Developer's Guide","text":""},{"location":"blog/mastering-git-merge-strategies-a-developers-guide/#introduction","title":"Introduction","text":"<p>Have you ever found yourself tangled in a web of Git branches, unsure of the best path to weave your changes together?</p> <p>The world of version control can be a maze, especially when deciding between Git's merge strategies. Fear not! This guide is your compass through the wilderness of rebases and merges, shedding light on the best routes to keep your repository history tidy and your sanity intact.</p> <p>Git offers two primary trails: the rebase, known for its clean and linear history, and the merge, preserving the unique storylines of each branch. Join us on this journey as we navigate the pros, cons, and conflict resolution techniques, empowering you to choose the right path for your project's narrative.</p> <p>So, this document provides guidance on using Git merge strategies, specifically focusing on the rebase and merge options.</p>"},{"location":"blog/mastering-git-merge-strategies-a-developers-guide/#rebase-creating-a-linear-history","title":"Rebase: Creating a Linear History","text":"<pre><code>git checkout feature-branch\ngit pull --rebase origin main\n</code></pre> <p>or</p> <pre><code>git fetch origin main \ngit checkout feature-branch\ngit rebase origin main \n</code></pre> <code>Pros</code> <code>Cons</code> <ul> <li>Keeps a linear, clean commit history.</li> <li>Integrates local changes after remote ones, maintaining chronological order.</li> </ul> <ul> <li>Requires manual resolution of conflicts that may arise during rebase.</li> </ul>"},{"location":"blog/mastering-git-merge-strategies-a-developers-guide/#handling-conflicts-during-rebase","title":"Handling Conflicts during Rebase","text":"<p>During the process of rebasing branches, conflicts might arise when applying commits from one branch onto another. Git requires manual resolution of conflicts that occur during a rebase operation.</p>"},{"location":"blog/mastering-git-merge-strategies-a-developers-guide/#resolving-conflicts-manually","title":"Resolving Conflicts Manually","text":"<p>When conflicts occur during a rebase, Git halts the process and prompts you to resolve conflicts in the files where they arise. After resolving conflicts, you can continue the rebase using:</p> <pre><code>git rebase --continue\n</code></pre>"},{"location":"blog/mastering-git-merge-strategies-a-developers-guide/#vscode-for-conflict-handling","title":"VSCode for Conflict Handling","text":"<p>Visual Studio Code (VSCode) offers a user-friendly interface to resolve conflicts during a rebase operation. Follow these steps within the VSCode environment:</p> <ol> <li> <p>Start the Rebase: Execute the rebase command in your terminal:</p> <pre><code>git rebase &lt;branch_name&gt;\n</code></pre> <p>This command initiates the rebase process.</p> </li> <li> <p>Conflict Indication: When conflicts occur, VSCode visually highlights them within the editor. You'll notice markers indicating the conflicted sections.</p> </li> <li> <p>Resolve Conflicts: Navigate to the conflicted file(s) in VSCode. Locate the sections marked as conflicted, displaying both versions of the conflicting changes.</p> </li> <li> <p>Choose Resolution: Review the changes and decide which version to keep or edit the content to create a resolution. Remove conflict markers (&lt;&lt;&lt;&lt;&lt;&lt;&lt;, =======, &gt;&gt;&gt;&gt;&gt;&gt;&gt;) once the conflict is resolved.</p> </li> <li> <p>Stage Changes: After resolving conflicts in each file, stage the changes using the Source Control panel in VSCode.</p> </li> <li> <p>Continue Rebase: Once conflicts are resolved and staged, return to your terminal and continue the rebase:</p> <pre><code>git rebase --continue\n</code></pre> <p>This command proceeds with the rebase process using the resolved changes.</p> </li> </ol> <p>VSCode streamlines the conflict resolution process by providing a visual and intuitive interface, making it easier to handle conflicts during a rebase operation.</p>"},{"location":"blog/mastering-git-merge-strategies-a-developers-guide/#merge-preserving-branch-narratives","title":"Merge: Preserving Branch Narratives","text":"<p>Using <code>merge</code> in Git combines changes from different branches, preserving their individual commit histories. This method creates a new commit to capture the integration of changes from one branch into another.</p> <code>Pros</code> <code>Cons</code> <ul> <li>Preserves the complete history of changes made in each branch.</li> <li>Maintains a clear track record of when and where changes were integrated.</li> </ul> <ul> <li>May result in a non-linear history with multiple merge commit points.</li> <li>Can potentially clutter the commit history with merge commits.</li> </ul>"},{"location":"blog/mastering-git-merge-strategies-a-developers-guide/#handling-conflicts","title":"Handling Conflicts","text":"<p>Similar to the rebase operation, merging branches in Git can lead to conflicts, especially when changes made in the same file or code sections conflict with each other. Git provides options to manage these conflicts during a merge operation.</p>"},{"location":"blog/mastering-git-merge-strategies-a-developers-guide/#resolving-conflicts-by-favoring-a-specific-branch","title":"Resolving Conflicts by Favoring a Specific Branch","text":"<p>Suppose you're merging <code>branchA</code> into <code>branchB</code> and wish to favor the changes from <code>branchB</code> in case of conflicts:</p> <pre><code>git checkout branchB  # Switch to the target branch (branchB)\ngit merge -X ours branchA  # Merge branchA into branchB, favoring branchB changes in conflicts\n</code></pre> <p>Explanation:</p> <ol> <li> <p><code>git checkout branchB</code>: Switches to the target branch where changes will be merged (in this case, <code>branchB</code>).</p> </li> <li> <p><code>git merge -X ours branchA</code>: Merges <code>branchA</code> into <code>branchB</code>, and the <code>-X ours</code> option ensures conflicts are resolved by favoring changes from the current branch (<code>branchB</code>).</p> </li> </ol> <p>Upon executing this command, Git will merge the changes from <code>branchA</code> into <code>branchB</code>, automatically resolving conflicts by favoring the modifications present in <code>branchB</code>.</p>"},{"location":"blog/mastering-git-merge-strategies-a-developers-guide/#other-merge-strategies","title":"Other Merge Strategies","text":"<p>Git provides various merge strategies such as <code>recursive</code>, <code>octopus</code>, and <code>resolve</code>, each with its own approach to handling merges. Choosing the right strategy depends on the project's requirements and the nature of changes between branches.</p>"},{"location":"blog/mastering-git-merge-strategies-a-developers-guide/#conclusion","title":"Conclusion","text":"<p>As we conclude this journey through Git's merge strategies, remember the beauty lies in choice. Rebase crafts a linear tale, while merge celebrates branch narratives. The decision depends on your project's needs and the story you wish to tell.</p> <p>Experiment, explore, and harness the power of Git's merging artistry to sculpt your repository's history. Beyond rebase and merge, Git unveils a treasure trove of strategies, offering endless possibilities for your collaborative coding adventure.</p> <p>So, venture forth armed with this knowledge, shaping your repository's saga amidst the ever-evolving landscape of team collaboration.</p>"},{"location":"blog/mastering-git-merge-strategies-a-developers-guide/#related-pages","title":"Related pages","text":"<ul> <li>Managing Local Modifications and Remote Changes in Git</li> <li>Understanding Git Pull vs Merge in Git Workflow</li> <li>Nesting Repositories with Git Submodules: A Newbie's Guide</li> <li>Mastering Git Branch Handling: Strategies for Deletion and Recovery</li> </ul>"},{"location":"blog/database-management-cli-equivalence-in-mysql-postgresql-and-mongodb/","title":"Database Management CLI: Equivalence in MySQL, PostgreSQL, and MongoDB","text":""},{"location":"blog/database-management-cli-equivalence-in-mysql-postgresql-and-mongodb/#introduction","title":"Introduction","text":"<p>Database management and querying are critical tasks for developers and database administrators. This guide explores syntax equivalences in MySQL, PostgreSQL, and MongoDB, enabling you to transition seamlessly between these systems using their command-line interfaces (CLI).</p> <p>Understanding the corresponding syntaxes in each database system facilitates code portability and collaboration among developers and administrators across different platforms.</p>"},{"location":"blog/database-management-cli-equivalence-in-mysql-postgresql-and-mongodb/#key-considerations","title":"Key Considerations","text":""},{"location":"blog/database-management-cli-equivalence-in-mysql-postgresql-and-mongodb/#choosing-the-right-database-system","title":"Choosing the Right Database System","text":"<ul> <li>Query Language: SQL for relational databases (MySQL, PostgreSQL) and MongoDB Query Language (MQL) for NoSQL.</li> <li>Use Case: Transactional applications, data analytics, or document storage.</li> <li>Scalability: Horizontal vs. vertical scaling.</li> <li>Community Support: Size and activity of the user community.</li> <li>Performance: Performance requirements for read and write operations.</li> </ul>"},{"location":"blog/database-management-cli-equivalence-in-mysql-postgresql-and-mongodb/#databases-overview","title":"Databases Overview","text":"MySQL PostgreSQL MongoDB <ul> <li>CLI Tool: <code>mysql</code></li> <li>Query Language: SQL</li> <li>Syntax Highlights: Standard SQL with some MySQL-specific extensions.</li> </ul> <ul> <li>CLI Tool: <code>psql</code></li> <li>Query Language: SQL</li> <li>Syntax Highlights: Advanced SQL features, support for JSON, and extensive indexing options.</li> </ul> <ul> <li>CLI Tool: <code>mongosh</code></li> <li>Query Language: MongoDB Query Language (MQL)</li> <li>Syntax Highlights: Document-based queries, flexible schema, and aggregation framework.</li> </ul>"},{"location":"blog/database-management-cli-equivalence-in-mysql-postgresql-and-mongodb/#comparison-tables","title":"Comparison Tables","text":""},{"location":"blog/database-management-cli-equivalence-in-mysql-postgresql-and-mongodb/#connection-and-basic-commands","title":"Connection and Basic Commands","text":"Task MySQL CLI (<code>mysql</code>) PostgreSQL CLI (<code>psql</code>) MongoDB CLI (<code>mongosh</code>) Connect to Database <code>mysql -u username -p database</code> <code>psql -U username -d database</code> <code>mongosh \"mongodb://username:password@host:port/database\"</code> List Databases <code>SHOW DATABASES;</code> <code>\\l</code> <code>show dbs</code> Select Database <code>USE database;</code> <code>\\c database</code> <code>use database</code> List Collections/Tables <code>SHOW TABLES;</code> <code>\\dt</code> <code>show collections</code> Exit CLI <code>exit</code> or <code>\\q</code> <code>\\q</code> <code>exit</code>"},{"location":"blog/database-management-cli-equivalence-in-mysql-postgresql-and-mongodb/#database-management","title":"Database Management","text":"Task MySQL CLI (<code>mysql</code>) PostgreSQL CLI (<code>psql</code>) MongoDB CLI (<code>mongosh</code>) Create Database <code>CREATE DATABASE dbname;</code> <code>CREATE DATABASE dbname;</code> <code>use dbname</code> (created on first write) Delete Database <code>DROP DATABASE dbname;</code> <code>DROP DATABASE dbname;</code> <code>use dbname; db.dropDatabase()</code> Rename Database Not supported directly; use dump + restore Not supported directly; use <code>ALTER DATABASE</code> workaround or dump/restore Not directly supported; use <code>mongodump</code> + <code>mongorestore</code> Duplicate Database <code>mysqldump old_db             | mysql new_db</code> <code>pg_dump old_db | psql new_db</code> <code>mongodump --db old_db</code> + <code>mongorestore --nsFrom old_db.* --nsTo new_db.*</code> <p>notes</p> <p>MySQL and PostgreSQL don\u2019t support renaming databases easily via SQL/psql\u2014you typically use a dump &amp; restore approach. MongoDB doesn\u2019t have a built-in rename or clone command for entire databases\u2014again, <code>mongodump</code>/<code>mongorestore</code> is the way to go.</p>"},{"location":"blog/database-management-cli-equivalence-in-mysql-postgresql-and-mongodb/#tablecollection-management","title":"Table/Collection Management","text":"Task MySQL CLI (<code>mysql</code>) PostgreSQL CLI (<code>psql</code>) MongoDB CLI (<code>mongosh</code>) Create Table/Collection <code>CREATE TABLE table_name (...);</code> <code>CREATE TABLE table_name (...);</code> <code>db.createCollection(\"collection_name\")</code> Drop Table/Collection <code>DROP TABLE table_name;</code> <code>DROP TABLE table_name;</code> <code>db.collection_name.drop()</code> Describe Table/Collection <code>DESCRIBE table_name;</code> <code>\\d table_name</code> <code>db.collection_name.stats()</code> Rename Table/Collection <code>RENAME TABLE old_name TO new_name;</code> <code>ALTER TABLE old_name RENAME TO new_name;</code> <code>db.collection_name.renameCollection(\"new_name\")</code> Duplicate Table/Collection <code>CREATE TABLE new_name AS SELECT * FROM old_name;</code> <code>CREATE TABLE new_name AS TABLE old_name;</code> <code>db.new_name.insertMany(db.old_name.find().toArray())</code> <p>notes</p> <p>The duplicate commands copy data and structure (for SQL), but may not copy things like indexes, constraints, triggers. In MongoDB, the <code>insertMany(...find())</code> pattern copies documents, but not indexes or validation rules unless added separately.</p>"},{"location":"blog/database-management-cli-equivalence-in-mysql-postgresql-and-mongodb/#data-manipulation","title":"Data Manipulation","text":"Task MySQL CLI (<code>mysql</code>) PostgreSQL CLI (<code>psql</code>) MongoDB CLI (<code>mongosh</code>) Insert Data <code>INSERT INTO table_name (...) VALUES (...);</code> <code>INSERT INTO table_name (...) VALUES (...);</code> <code>db.collection_name.insertOne({...})</code> Select Data <code>SELECT * FROM table_name;</code> <code>SELECT * FROM table_name;</code> <code>db.collection_name.find({})</code> Update Data <code>UPDATE table_name SET ... WHERE ...;</code> <code>UPDATE table_name SET ... WHERE ...;</code> <code>db.collection_name.updateOne({...}, {$set: {...}})</code> Delete Data <code>DELETE FROM table_name WHERE ...;</code> <code>DELETE FROM table_name WHERE ...;</code> <code>db.collection_name.deleteOne({...})</code>"},{"location":"blog/database-management-cli-equivalence-in-mysql-postgresql-and-mongodb/#querying-data","title":"Querying Data","text":"Task MySQL CLI (<code>mysql</code>) PostgreSQL CLI (<code>psql</code>) MongoDB CLI (<code>mongosh</code>) Basic Select <code>SELECT * FROM table_name;</code> <code>SELECT * FROM table_name;</code> <code>db.collection_name.find({})</code> Where Clause <code>SELECT * FROM table_name WHERE condition;</code> <code>SELECT * FROM table_name WHERE condition;</code> <code>db.collection_name.find({condition})</code> Join Tables <code>SELECT * FROM table1 JOIN table2 ON condition;</code> <code>SELECT * FROM table1 JOIN table2 ON condition;</code> <code>db.collection1.aggregate([{$lookup: {from: \"collection2\", localField: \"field1\", foreignField: \"field2\", as: \"joined_docs\"}}])</code> Group By <code>SELECT column, COUNT(*) FROM table_name GROUP BY column;</code> <code>SELECT column, COUNT(*) FROM table_name GROUP BY column;</code> <code>db.collection_name.aggregate([{$group: {_id: \"$column\", count: {$sum: 1}}}])</code>"},{"location":"blog/database-management-cli-equivalence-in-mysql-postgresql-and-mongodb/#index-management","title":"Index Management","text":"Task MySQL CLI (<code>mysql</code>) PostgreSQL CLI (<code>psql</code>) MongoDB CLI (<code>mongosh</code>) Create Index <code>CREATE INDEX idx_name ON table_name(column);</code> <code>CREATE INDEX idx_name ON table_name(column);</code> <code>db.collection_name.createIndex({column: 1})</code> List Indexes <code>SHOW INDEX FROM table_name;</code> <code>\\di table_name</code> <code>db.collection_name.getIndexes()</code> Drop Index <code>DROP INDEX idx_name ON table_name;</code> <code>DROP INDEX idx_name;</code> <code>db.collection_name.dropIndex(\"idx_name\")</code>"},{"location":"blog/database-management-cli-equivalence-in-mysql-postgresql-and-mongodb/#transactions","title":"Transactions","text":"Task MySQL CLI (<code>mysql</code>) PostgreSQL CLI (<code>psql</code>) MongoDB CLI (<code>mongosh</code>) Begin Transaction <code>START TRANSACTION;</code> <code>BEGIN;</code> <code>session = db.getMongo().startSession(); session.startTransaction();</code> Commit Transaction <code>COMMIT;</code> <code>COMMIT;</code> <code>session.commitTransaction(); session.endSession();</code> Rollback Transaction <code>ROLLBACK;</code> <code>ROLLBACK;</code> <code>session.abortTransaction(); session.endSession();</code>"},{"location":"blog/database-management-cli-equivalence-in-mysql-postgresql-and-mongodb/#importexport-data","title":"Import/Export Data","text":"Task MySQL CLI (<code>mysql</code>) PostgreSQL CLI (<code>psql</code>) MongoDB CLI (<code>mongosh</code>) Import Data <code>LOAD DATA INFILE 'file.csv' INTO TABLE table_name;</code> <code>\\COPY table_name FROM 'file.csv' DELIMITER ',' CSV;</code> <code>mongoimport --db database --collection collection_name --file file.json</code> Export Data <code>SELECT * FROM table_name INTO OUTFILE 'file.csv';</code> <code>\\COPY (SELECT * FROM table_name) TO 'file.csv' DELIMITER ',' CSV;</code> <code>mongoexport --db database --collection collection_name --out file.json</code>"},{"location":"blog/database-management-cli-equivalence-in-mysql-postgresql-and-mongodb/#conclusion","title":"Conclusion","text":"<p>This guide provides a comparison of the most commonly used database management systems' command-line interfaces: MySQL, PostgreSQL, and MongoDB. By understanding these equivalences, developers and administrators can efficiently manage and query databases across different platforms.</p> <p>Whether you're working with relational databases like MySQL and PostgreSQL or a document-based NoSQL database like MongoDB, having a quick reference for CLI commands can enhance your productivity and streamline your workflow.</p>"},{"location":"blog/guide-to-applying-query-on-you-mongodb-atlas-hosted-database-from-command-line/","title":"Guide to Applying query on you mongodb atlas hosted database from command line","text":""},{"location":"blog/guide-to-applying-query-on-you-mongodb-atlas-hosted-database-from-command-line/#introduction","title":"Introduction","text":"<p>Often when you're using a database in a dev project, you want to access it quickly to check for modifications. When you're working with mysql database, you have a client that can help you with that. But what to do when you're using mongo db ? In this tutorial, i present how to access, from command line, your databased hosted with mongo db atlas. Then i showcase basic still important query examples. It should work also for those hosted locally.</p>"},{"location":"blog/guide-to-applying-query-on-you-mongodb-atlas-hosted-database-from-command-line/#prerequistes","title":"Prerequistes","text":"<ul> <li>Node js</li> <li>a mongo db database, offline or locally served</li> </ul>"},{"location":"blog/guide-to-applying-query-on-you-mongodb-atlas-hosted-database-from-command-line/#download-and-install-mongosh","title":"Download and Install Mongosh","text":"<p>You can download and install <code>mongosh</code> from the MongoDB website or using package managers like npm or yarn. Make sure you have Node.js installed on your system before proceeding.</p> <p>Personally, using windows, i've downloaded it (and installed the setup) from the website, put the bin file (containing mongosh.exe) into environment variables and read a bit of the docu. However, using ubuntu, i've tested both approch, installing using <code>apt-get</code> and using <code>nodejs</code>.</p> <p>Let's use the Package Manage option, as it is a more straightforward approach</p> <code>Using npm (Node Package Manager)</code> <code>Using yarn (Package Manager)</code> <pre><code>npx mongosh --version\n</code></pre> <pre><code>yarn dlx mongosh --version\n</code></pre> <p>This command will install mongosh it you doesn't have it.</p> <p>For more details on installing <code>mongosh</code>, refer to the MongoDB documentation <sup>1</sup>.</p> <p>I figured later on i can install mongodb database tools in a Dockerfile like proposed here</p>"},{"location":"blog/guide-to-applying-query-on-you-mongodb-atlas-hosted-database-from-command-line/#access-a-dababase-from-cli","title":"Access a dababase from cli","text":"<p>I assume you have a MongoDB deployment to connect to. You can use a free cloud-hosted deployment like MongoDB Atlas or run a local MongoDB deployment.</p> <p>Connect to your MongoDB deployment using mongosh by running the command <sup>2</sup></p> <pre><code>mongosh \"mongodb+srv://&lt;username&gt;:&lt;password&gt;@&lt;cluster-address&gt;/&lt;database-name&gt;\"\n</code></pre> <p>Replace <code>&lt;username&gt;</code>, <code>&lt;password&gt;</code>, <code>&lt;cluster-address&gt;</code>, and <code>&lt;database-name&gt;</code> with your own values.</p> <p>You can find more information on how to install and use mongosh in the official MongoDB documentation <sup>1</sup> <sup>2</sup>.</p>"},{"location":"blog/guide-to-applying-query-on-you-mongodb-atlas-hosted-database-from-command-line/#get-one-record-by-searching-an-attribute","title":"Get one record by searching an attribute","text":"<p>Get user by id</p> <pre><code>db.users.find({\"_id\":ObjectId(\"&lt;value&gt;\")})\n</code></pre> <p>Get user by telephone</p> <pre><code>db.users.find({telephone:\"&lt;telephone&gt;\"})\n</code></pre>"},{"location":"blog/guide-to-applying-query-on-you-mongodb-atlas-hosted-database-from-command-line/#get-all-record-on-constraint","title":"Get all record on constraint","text":"<p>Get all restaurants</p> <pre><code>db.restaurants.find()\n</code></pre> <p>Get all users whose telephone contains 210</p> <pre><code>db.users.find({ telephone: { $regex: '210' }})\n</code></pre> <p>Get the n latest</p> <pre><code>db.purchases.find({}).sort({_id:-1}).limit(1)\n</code></pre> <p>Get the list of _id for all articles</p> <pre><code>db.articles.find({}, { _id: 1 }).toArray().map((doc) =&gt; doc._id)\n</code></pre> <p>Get users whose prenoms contain <code>abc</code>, case insensitive</p> <pre><code>db.users.find({ prenoms: { $regex: \"abc\", $options: 'i' } });\n</code></pre>"},{"location":"blog/guide-to-applying-query-on-you-mongodb-atlas-hosted-database-from-command-line/#find-with-cross-tables-constraints","title":"Find with cross tables constraints","text":"<p>Get all restaurants without an owner</p> <pre><code>db.restaurants.aggregate([\n  { $lookup: { from: \"users\", localField: \"own_by\", foreignField:\"_id\", as: \"owner\" } },\n  { $match: { owner: { $size: 0 } /* Filter where \"owner\" array is empty, meaning no matching user found*/ } }\n]);\n</code></pre> <p>Get all users whose _id is present in the restaurants table and are of type \"RESTAU\"</p> <pre><code>db.users.aggregate([\n  { $lookup: { from: \"restaurants\", localField: \"_id\", foreignField: \"own_by\", as: \"restau\" }},\n  { $match: { type: \"RESTAU\" }}\n]);\n</code></pre> <p>Get users with at least one restaurant and apply a modification</p> <pre><code>db.users.aggregate([\n  { $lookup: { from: \"restaurants\", localField: \"_id\", foreignField: \"own_by\", as: \"restaurants\" } },\n  { $match: { restaurants: { $exists: true, $not: { $size: 0 } } /* Users with at least one restaurant*/ } },\n  { $set: { key: \"value\" } }\n]);\n</code></pre>"},{"location":"blog/guide-to-applying-query-on-you-mongodb-atlas-hosted-database-from-command-line/#update-rows","title":"Update rows","text":"<p>Update the user with a specific _id</p> <pre><code>db.users.updateOne({\"_id\":ObjectId(\"&lt;value&gt;\")}, {$set : {\"nom\":\"new name\"}})\n</code></pre> <p>Update the user with a specific telephone</p> <pre><code>db.users.updateOne({telephone:\"&lt;value&gt;\"}, {$set: {\"notif_data.notif_token\":\"&lt;value&gt;\"}})\n</code></pre> <p>Update the password of all users with type=\"RESTAU</p> <pre><code>db.users.updateMany({\"type\":\"RESTAU\"}, {\"$set\":{\"password\":\"&lt;value&gt;\"}})\n</code></pre>"},{"location":"blog/guide-to-applying-query-on-you-mongodb-atlas-hosted-database-from-command-line/#other-commands","title":"Other Commands","text":"<p>See all collections</p> <pre><code>show tables\n</code></pre> <p>See all databases</p> <pre><code>show dbs\n</code></pre> <p>Switch to the database <code>BiWag</code></p> <pre><code>use BiWag\n</code></pre> <p>See help</p> <pre><code>help\n</code></pre>"},{"location":"blog/guide-to-applying-query-on-you-mongodb-atlas-hosted-database-from-command-line/#conclusion","title":"Conclusion","text":"<p>Note: This document contains example commands and use cases for the <code>mongosh</code> shell in MongoDB. Always be cautious when performing any updates or deletions on your database and ensure you have proper backups and permissions.</p> <ol> <li> <p>https://www.mongodb.com/docs/mongodb-shell/install/ \u21a9\u21a9</p> </li> <li> <p>https://www.mongodb.com/docs/mongodb-shell/connect/ \u21a9\u21a9</p> </li> </ol>"},{"location":"blog/guide-to-installing-mysql-and-connecting-to-databases/","title":"Guide to Installing MySQL and Connecting to Databases","text":""},{"location":"blog/guide-to-installing-mysql-and-connecting-to-databases/#introduction","title":"Introduction","text":"<p>MySQL is a popular relational database management system used for storing and managing data. To get started, you'll need to install MySQL, set it up, and then connect to databases. Here's a comprehensive guide to help you through the process.</p>"},{"location":"blog/guide-to-installing-mysql-and-connecting-to-databases/#installation-process","title":"Installation Process","text":"<p>To install MySQL, follow these steps:</p> <ul> <li>update the package lists</li> </ul> <pre><code>sudo apt-get update\n</code></pre> <ul> <li>install MySQL</li> </ul> <pre><code>sudo apt-get install mysql-server\n</code></pre> <p>or use yum</p> <pre><code>yum install &lt;package-name&gt;\nor \nsudo yum install mysql-server\n</code></pre> <p>you will be prompted to set a password for the MySQL root user. Make sure to choose a strong password and remember it, as you will need it to access MySQL.</p> <ul> <li>if you haven't been prompted the password,</li> <li> <p>do this</p> <pre><code>sudo mysql_secure_installation\n</code></pre> </li> <li> <p>or connect later on using your ssh details (username, password): <code>mysql -u [username] -p</code></p> </li> <li> <p>start the MySQL service</p> </li> </ul> <pre><code>sudo service mysql start\nor\nsudo systemctl start mysqld\n</code></pre> <ul> <li>check if MySQL is running,</li> </ul> <pre><code>sudo service mysql status\n</code></pre> <p>If MySQL is running, you should see a message that says \"Active: active (running)\".</p>"},{"location":"blog/guide-to-installing-mysql-and-connecting-to-databases/#testing-the-mysql-connection","title":"Testing the MySQL Connection","text":"<p>Here are a few commands you can use to test your MySQL connection:</p> <pre><code>mysql -u [username] -p -h [hostname] -P [port]\n</code></pre> <ul> <li> <p>Replace [username] with your database username, [hostname] with your database hostname, [port] with your database port number, and leave out the brackets.</p> </li> <li> <p>For example, if your database username is \"myuser\", your database hostname is \"db.example.com\", and your database port number is 3306, the command would look like this:</p> </li> </ul> <pre><code>mysql -u myuser -p -h db.example.com -P 3306\n</code></pre> <ul> <li> <p>Press Enter and then enter your database password when prompted. If the connection is successful, you'll see a prompt that looks like this:</p> </li> <li> <p>if that don't work, test that command with your ssh [username] and [password]</p> </li> </ul> <pre><code>mysql -u [username] -p\n</code></pre> <p>you will see this</p> <pre><code>mysql&gt;\n</code></pre> <p>This means you're now connected to your MySQL server.</p> <ul> <li>To test that you can retrieve data from your database, enter the following command:</li> </ul> <pre><code>use [databasename];\nselect * from [tablename];\n</code></pre> <ul> <li> <p>Replace [databasename] with the name of your database and [tablename] with the name of a table in your database. This will select all rows from the specified table.</p> </li> <li> <p>If the command returns data from your database, then your connection is working properly.</p> </li> </ul>"},{"location":"blog/guide-to-installing-mysql-and-connecting-to-databases/#essential-actions","title":"Essential Actions","text":"<p>Here are some quick commands and actions you can perform in MySQL:</p> <ul> <li>show all the databases</li> </ul> <pre><code>SHOW DATABASES;\n</code></pre> <ul> <li>create a new database</li> </ul> <pre><code>CREATE DATABASE mydatabase;\n</code></pre> <ul> <li>use this database:</li> </ul> <pre><code>USE mydatabase;\n</code></pre> <ul> <li>create a new table:</li> </ul> <pre><code>CREATE TABLE mytable (id INT, name VARCHAR(20));\n</code></pre> <ul> <li>insert data into this table:</li> </ul> <pre><code>INSERT INTO mytable VALUES (1, 'John'), (2, 'Jane');\n</code></pre> <ul> <li>query the data:</li> </ul> <pre><code>SELECT * FROM mytable;\n</code></pre> <p>This will display the data you inserted into the table.</p> <ul> <li>To exit the MySQL shell:</li> </ul> <pre><code>exit\n</code></pre>"},{"location":"blog/guide-to-installing-mysql-and-connecting-to-databases/#create-a-new-mysql-user-account","title":"Create a new MySQL user account","text":"<pre><code>sudo mysql -u root -p\n</code></pre> <pre><code>CREATE USER 'yourusername'@'localhost' IDENTIFIED BY 'yourpassword';\n\nGRANT ALL PRIVILEGES ON *.* TO 'yourusername'@'localhost' WITH GRANT OPTION;\n\nFLUSH PRIVILEGES;\n</code></pre> <p>Replace yourusername and yourpassword with the desired username and password for your MySQL user account.</p>"},{"location":"blog/guide-to-installing-mysql-and-connecting-to-databases/#connecting-to-an-online-mysql-database","title":"Connecting to an Online MySQL Database","text":"<pre><code># Extracting details from the connection string\nusername=\"doadmin\"\npassword=\"AVNS_7wyTjplB7LVpwf3VKKf\"\nhostname=\"db-mysql-metalandapi-do-user-12655475-0.b.db.ondigitalocean.com\"\nport=\"25060\"\ndatabase_name=\"defaultdb\"\n\n# Constructing the MySQL CLI command\nmysql -u $username -p$password -h $hostname -P $port $database_name\n</code></pre>"},{"location":"blog/guide-to-installing-mysql-and-connecting-to-databases/#deployment-and-integration","title":"Deployment and Integration","text":"<p>Configure your web application to utilize the MySQL database. Ensure your web server knows:</p> <ul> <li><code>Hostname</code> (usually localhost if on the same machine)</li> <li><code>Port</code> (default is 3306 for MySQL)</li> <li><code>Username</code> and <code>password</code> created earlier Database name you established</li> </ul> <p>Employ programming languages like PHP or Python to interact with the MySQL database. Use Nginx as a reverse proxy to direct requests to your application server.</p>"},{"location":"blog/guide-to-installing-mysql-and-connecting-to-databases/#example-integrating-mysql-with-maven","title":"Example: Integrating MySQL with Maven","text":"<p>An example of integrating MySQL with a Maven project:</p> <ol> <li>Modify your Maven project to connect to the MySQL database. You can add the MySQL JDBC driver as a dependency in your project's <code>pom.xml</code> file, like this:</li> </ol> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;mysql&lt;/groupId&gt;\n    &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;\n    &lt;version&gt;8.0.25&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>Replace the version number with the latest version of the MySQL JDBC driver.</p> <ol> <li>Configure your application to use the MySQL database. You can add the necessary configuration properties to your application.properties file, like this:</li> </ol> <pre><code>spring.datasource.url=jdbc:mysql://localhost:3306/your_database_name\nspring.datasource.username=root\nspring.datasource.password=your_mysql_password\nspring.jpa.hibernate.ddl-auto=update\n</code></pre> <p>Replace your_database_name with the name of the database you created in step 2, and your_mysql_password with the password you set for the MySQL root user.</p> <ol> <li> <p>Build your Maven project and create an executable JAR file using the mvn package command.</p> </li> <li> <p>Start your application using the executable JAR file you created earlier. You can start it using the <code>java -jar &lt;jar-file-name&gt;</code> command.</p> </li> </ol> <p>Now your application should be up and running, connected to the MySQL database and loaded with the data from your SQL file. Nginx can then be used as a reverse proxy to serve your application to users.</p>"},{"location":"projects/","title":"My Projects","text":""},{"location":"projects/#data-science","title":"Data science","text":"<ul> <li>D\u00e9tection de bot dans dans les ench\u00e8res</li> <li>Search Engine for domain specific french users</li> <li>Streamlit App for Formula OCR using pix2tex</li> </ul>"},{"location":"projects/#software-development","title":"Software development","text":"<ul> <li>Flask based File Hosting (web app &amp; api &amp; python module &amp; cli app)</li> <li>Introducing Two New Packages for Streamlining File Conversions in Python</li> <li>Introducing the <code>lissajou</code> Package: Animate Stunning Lissajou Curves and Beyond</li> </ul>"},{"location":"projects/bot_detection_in_auction/","title":"D\u00e9tection de bot dans dans les ench\u00e8res","text":""},{"location":"projects/bot_detection_in_auction/#objectif-general","title":"Objectif g\u00e9n\u00e9ral","text":"<p>Notre objectif g\u00e9n\u00e9ral est d\\'\u00e9tudier un \u00ab\u00a0dataset\u00a0\u00bb issu d\\'une plateforme d\\'ench\u00e8res publicitaires pour pouvoir pr\u00e9dire si l'agent qui a \u00e9mis des ench\u00e8res est un humain ou un robot. A partir d'une analyse bien approfondie de diverses donn\u00e9es concernant les transactions effectu\u00e9es notamment les outils num\u00e9riques utilis\u00e9s, les temps de ces transactions et bien d'autres \u00ab\u00a0features\u00a0\u00bb, nous allons d\u00e9velopper un mod\u00e8le de classification capable de pr\u00e9dire la variable binaire \u00ab\u00a0outcome\u00a0\u00bb de telle sorte que 0 d\u00e9signe <code>humain</code> et 1, <code>robot</code>. En outre, nous visons \u00e0 travers ce projet \u00e0 minimiser le taux des faux n\u00e9gatifs (pr\u00e9dire que l'agent est un humain, alors qu'il est un robot) et donc augmenter comme m\u00e9trique le \u00ab\u00a0recall\u00a0\u00bb pour \u00e9viter toute sorte de fraude. Enfin, nous allons choisir au maximum 5 variables pour notre mod\u00e9lisation.</p>"},{"location":"projects/bot_detection_in_auction/#comprehension-des-donnees","title":"Compr\u00e9hension des donn\u00e9es","text":"<p>La base de donn\u00e9es fournie contient des informations sur les soumissionnaires de l'ench\u00e8re et sur l'ench\u00e8re. Les features donn\u00e9s sont expliqu\u00e9s ci-dessous\u00a0:</p> Feature Explication Bidder_id Un identifiant unique pour le soumissionnaire Bid_id Un identifiant unique de l'offre fait par le soumissionnaire Auction Un identifiant unique de l'ench\u00e8re (l'offre publique) Merchandise La cat\u00e9gorie du produit/offre Device Mod\u00e8le de t\u00e9l\u00e9phone d'un visiteur Time Temps \u00e0 lequel la transaction a \u00e9t\u00e9 faite pour l'ench\u00e8re Country Le pays auquel appartient l'IP IP Adresse IP de l'enrichisseur Url Le site \u00e0 partir duquel l'enrichisseur a \u00e9t\u00e9 r\u00e9f\u00e9r\u00e9 Payment_account Le compte \u00e0 partir duquel l'enrichisseur a pay\u00e9 Address L'adresse de l'enrichisseur Outcome 1 si robot, 0 si homme"},{"location":"projects/bot_detection_in_auction/#analyse-descriptive-et-selection-de-variables","title":"Analyse descriptive et s\u00e9lection de variables","text":""},{"location":"projects/bot_detection_in_auction/#some-constants","title":"some constants","text":"<pre><code>TARGET_COL = \"outcome\"\nREMOVE_EVIDENT_MERCHANDISE = False\nFILE_VERSION = \"v7\"\nPROD_INSTEAD_OF_SUM =  True\nADD_LEN_TO_GROUPBY = True\n#prod+nolen &lt; prod+len\n</code></pre>"},{"location":"projects/bot_detection_in_auction/#load-data","title":"Load data","text":"<pre><code>df = pd.read_csv(\"Projet_ML.csv\")\ndf.bidder_id.nunique() # 87\n</code></pre>"},{"location":"projects/bot_detection_in_auction/#preview","title":"Preview","text":"<p>Voici un aper\u00e7u de la dataset\u00a0:</p> &gt;&gt;&gt; df bidder_id bid_id auction merchandise device time country ip url payment_account address outcome 001068c415025a009fee375a12cff4fcnht8y 7179832 4ifac jewelry phone561 5.140996e-308 bn 139.226.147.115 vasstdc27m7nks3 a3d2de7675556553a5f08e4c88d2c228iiasc a3d2de7675556553a5f08e4c88d2c2282aj35 0 0030a2dd87ad2733e0873062e4f83954mkj86 6805028 obbny mobile phone313 5.139226e-308 ir 21.67.17.162 vnw40k8zzokijsv a3d2de7675556553a5f08e4c88d2c228jem8t f3bc67b04b43c3cebd1db5ed4941874c9br67 0 00a0517965f18610417ee784a05f494d4dw6e 2501797 l3o6q books and music phone451 5.067829e-308 bh 103.165.41.136 kk7rxe25ehseyci 52743ba515e9c1279ac76e19f00c0b001p3pm 7578f951008bd0b64528bf81b8578d5djy0uy 0 00a0517965f18610417ee784a05f494d4dw6e 2724778 du967 books and music phone117 5.068704e-308 tr 239.250.228.152 iu2iu3k137vakme 52743ba515e9c1279ac76e19f00c0b001p3pm 7578f951008bd0b64528bf81b8578d5djy0uy 0 00a0517965f18610417ee784a05f494d4dw6e 2742648 wx3kf books and music phone16 5.068805e-308 in 255.108.248.101 u85yj2e7owkz6xp 52743ba515e9c1279ac76e19f00c0b001p3pm 7578f951008bd0b64528bf81b8578d5djy0uy 0 ... ... ... ... ... ... ... ... ... ... ... ... 0ad17aa9111f657d71cd3005599afc24fd44y 1411172 toxfq mobile phone1036 5.201503e-308 in 186.94.48.203 vasstdc27m7nks3 22cdb26663f071c00de61cc2dcde7b556rido db147bf6056d00428b1bbf250c6e97594ewjy 1 0ad17aa9111f657d71cd3005599afc24fd44y 1411587 ucb4u mobile phone127 5.201506e-308 in 119.27.26.126 vasstdc27m7nks3 22cdb26663f071c00de61cc2dcde7b556rido db147bf6056d00428b1bbf250c6e97594ewjy 1 0ad17aa9111f657d71cd3005599afc24fd44y 1411727 sg8yd mobile phone383 5.201507e-308 in 243.25.54.63 yweo7wfejrgbi2d 22cdb26663f071c00de61cc2dcde7b556rido db147bf6056d00428b1bbf250c6e97594ewjy 1 0ad17aa9111f657d71cd3005599afc24fd44y 1411877 toaj7 mobile phone26 5.201508e-308 in 17.66.120.232 4dd8ei0o5oqsua3 22cdb26663f071c00de61cc2dcde7b556rido db147bf6056d00428b1bbf250c6e97594ewjy 1 0ad17aa9111f657d71cd3005599afc24fd44y 1412085 07axb mobile phone25 5.201509e-308 in 64.30.57.156 8zdkeqk4yby6lz2 22cdb26663f071c00de61cc2dcde7b556rido db147bf6056d00428b1bbf250c6e97594ewjy 1"},{"location":"projects/bot_detection_in_auction/#variable-a-predire","title":"variable \u00e0 pr\u00e9dire","text":"<p><code>&gt;&gt;&gt; df[TARGET_COL].value_counts()</code></p> <pre><code>0    90877\n1     9123\nName: outcome, dtype: int64\n</code></pre> <p>c'est une classification binaire</p>"},{"location":"projects/bot_detection_in_auction/#description-des-champs-numeriques","title":"Description des champs num\u00e9riques","text":"<p><code>&gt;&gt;&gt; df.info()</code></p> <pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 100000 entries, 0 to 99999\nData columns (total 12 columns):\n\n## Column           Non-Null Count   Dtype  \n\n---  ------           --------------   -----  \n0   bidder_id        100000 non-null  object\n1   bid_id           100000 non-null  int64  \n2   auction          100000 non-null  object\n3   merchandise      100000 non-null  object\n4   device           100000 non-null  object\n5   time             100000 non-null  float64\n6   country          99816 non-null   object\n7   ip               100000 non-null  object\n8   url              100000 non-null  object\n9   payment_account  100000 non-null  object\n10  address          100000 non-null  object\n11  outcome          100000 non-null  int64  \ndtypes: float64(1), int64(2), object(9)\nmemory usage: 9.2+ MB\n</code></pre> <p>On a pr\u00e8s de 200 cellules vides dans country. on verra \u00e7a apr\u00e8s</p>"},{"location":"projects/bot_detection_in_auction/#types-et-autres-informations","title":"Types et autres informations","text":"<p>Le dataset contient 12 colonnes et 100.000 lignes.</p>"},{"location":"projects/bot_detection_in_auction/#description","title":"Description","text":"<p>L'image ci-dessous pr\u00e9sente plusieurs caract\u00e9ristiques de chaque champ de la table</p> <ul> <li> <p>dtype\u00a0: type de la variable (int64 pour les entiers, float64 pour les nombres, object pour les champs textes ou non-identifi\u00e9s)</p> </li> <li> <p>nunique\u00a0: nombre de valeurs uniques que prends cette variable</p> </li> <li> <p>nunique(%)\u00a0: proportion des valeurs uniques que prends cette variable par rapport au nombre de lignes dans la table</p> </li> <li> <p>nunique_per_bid&gt;1(%)\u00a0: nombre de \u00ab\u00a0bidder_id\u00a0\u00bb qui pr\u00e9sentes plusieurs valeurs diff\u00e9rentes pour cette variable.</p> </li> <li> <p>is_cat: 1 si la variable peut \u00eatre consid\u00e9r\u00e9e categorielle (ici, moins de 10 valeurs uniques) 0 sinon</p> </li> </ul> <code>def get_cols_info</code> <pre><code>from math import ceil\n\ndef get_cols_info(df, index_col=None):\n  print(\"&gt;&gt;&gt; df.shape= \", df.shape)\n  print(\"\\n&gt;&gt;&gt; df.info= \")\n  df.info()\n  dd = {\"col\":[],\"dtype\":[],\"nunique\":[],\"nunique(%)\":[],\"nunique_per_bid&gt;1(%)\":[],\"is_cat\":[]}\n  for elt in df.columns:\n  dd[\"col\"].append(elt)\n  dd[\"nunique\"].append(df[elt].nunique())\n  dd[\"nunique(%)\"].append(0.1*ceil(10*100*df[elt].nunique()/len(df)))\n  dd[\"dtype\"].append(df[elt].dtype)\n  dd[\"is_cat\"].append(int(df[elt].nunique()&lt;10))\n  if index_col: dd[\"nunique_per_bid&gt;1(%)\"].append(0.1*ceil(10*100*(df.groupby(index_col)[elt].nunique()&gt;1).sum()/df[index_col].nunique()))\n  else: dd[\"nunique_per_bid&gt;1(%)\"].append('')\n  list_indx = dd[\"col\"]\n  del dd[\"col\"]\n  print(\"\\n&gt;&gt;&gt; df.more_info= \")\n  print(pd.DataFrame(dd, index=list_indx).sort_values(by=['nunique']))\n  print(\"\\n&gt;&gt;&gt; df.describe= \")\n  print(df.describe())\n\nget_cols_info(df, \"bidder_id\")\n</code></pre> <code>&gt;&gt;&gt; get_cols_info(df, \"bidder_id\")</code> <pre><code>  &gt;&gt;&gt; df.shape=  (100000, 12)\n\n  &gt;&gt;&gt; df.info= \n  &lt;class 'pandas.core.frame.DataFrame'&gt;\n  RangeIndex: 100000 entries, 0 to 99999\n  Data columns (total 12 columns):\n  ##   Column           Non-Null Count   Dtype  \n  ---  ------           --------------   -----  \n  0   bidder_id        100000 non-null  object \n  1   bid_id           100000 non-null  int64  \n  2   auction          100000 non-null  object \n  3   merchandise      100000 non-null  object \n  4   device           100000 non-null  object \n  5   time             100000 non-null  float64\n  6   country          100000 non-null  object \n  7   ip               100000 non-null  object \n  8   url              100000 non-null  object \n  9   payment_account  100000 non-null  object \n  10  address          100000 non-null  object \n  11  outcome          100000 non-null  int64  \n  dtypes: float64(1), int64(2), object(9)\n  memory usage: 9.2+ MB\n\n  &gt;&gt;&gt; df.more_info= \n  dtype  nunique  nunique(%)  nunique_per_bid&gt;1(%)  is_cat\n  outcome            int64        2         0.1                   0.0       1\n  merchandise       object        6         0.1                   0.0       1\n  bidder_id         object       87         0.1                   0.0       0\n  payment_account   object       87         0.1                   0.0       0\n  address           object       87         0.1                   0.0       0\n  country           object      175         0.2                  70.2       0\n  device            object     1871         1.9                  80.5       0\n  auction           object     3438         3.5                  79.4       0\n  url               object    21951        22.0                  72.5       0\n  ip                object    35083        35.1                  84.0       0\n  time             float64    92385        92.4                  85.1       0\n  bid_id             int64   100000       100.0                  85.1       0\n\n  &gt;&gt;&gt; df.describe= \n  bid_id           time        outcome\n  count  1.000000e+05  100000.000000  100000.000000\n  mean   3.697622e+06       0.543571       0.091230\n  std    2.380217e+06       0.362483       0.287937\n  min    8.900000e+01       0.000000       0.000000\n  25%    1.463762e+06       0.085364       0.000000\n  50%    3.660968e+06       0.514295       0.000000\n  75%    5.881387e+06       0.935483       0.000000\n  max    7.656326e+06       1.000000       1.000000\n</code></pre> <p>on remarque le bid_id est un identifiant unique pour chaque bidder_id qui repr\u00e9sente l'action de l'offre men\u00e9e par chaque bidder</p> <p>on constate que time est n'est pas \u00e0 100% unique donc on peut avoir deux actions d'ench\u00e8re qui sont r\u00e9alis\u00e9 en m\u00eame instant</p>"},{"location":"projects/bot_detection_in_auction/#etude-de-la-variable-predictive","title":"Etude de la variable pr\u00e9dictive","text":"<pre><code>tg = \"outcome\" #(\"outcome\", \"&lt;lambda&gt;\")\nsss[tg] = sss[tg].values.astype(int)\nax = sns.histplot(x=tg, data=sss)\nadd_labels_to_histplot(ax, title=\"distribution of outcome\")\n</code></pre> <p>On remarque que sur 87 bidder_id, seuls 6 correspondent \u00e0 des robots. Ainsi, il s'agit d'un probl\u00e8me de classification binaire d\u00e9s\u00e9quilibr\u00e9.</p>"},{"location":"projects/bot_detection_in_auction/#selection-des-variables","title":"S\u00e9lection des variables","text":"<p>Les variables qui prennent plusieurs valeurs pour un m\u00eame joueur pourraient \u00eatre utilis\u00e9s afin d'\u00e9tudier la vari\u00e9t\u00e9 des outils utilis\u00e9s par le joueur. C'est le cas des variables (country, device, auction, url, ip). La variable bid_id est un identifiant unique de transaction (nunique=100%) et ne sera pas consid\u00e9r\u00e9.</p> <p>Les variables cat\u00e9gorielles, si pertinentes seront int\u00e9gr\u00e9es. C'est le cas de la variable (merchandise)</p> <p>Les variables avec un \u00ab\u00a0nunique_per_bid\u00a0\u00bb prenant la valeur nulle, n'apportent pas d'informations et seront rejet\u00e9es s'il n'y a pas d'extraction d'informations possibles. C'est le cas pour les variables (payment_account et address)</p> Variable Pertinence Descriptions \u00c0 Rejeter Outcome (status du joueur) - Il s'agit de la variable \u00e0 pr\u00e9dire Variable cat\u00e9gorielle (0 ou 1). Chaque joueur a un seul status. Cette information est v\u00e9rifi\u00e9e sur toute la table. NON Merchandise (type de produit achet\u00e9s) - Les robots ou les humains pourraient avoir des tendances vers des produits particuliers. Variable cat\u00e9gorielle (6 cat\u00e9gories) NON bidder_id (identifiant du joueur) - Il s'agit de l'object de la pr\u00e9diction. Identifiant. Mais il n'apporte pas d'informations suppl\u00e9mentaires. NON PAYMENT_ACCOUNT ET ADRESSE - Ces variables n'apportent pas d'informations suppl\u00e9mentaires (unique_per_bid = 0). Identifiants sans possibilit\u00e9 d'extraction d'informations. OUI COUNTRY - Les joueurs de certains pays pourraient avoir plus tendance \u00e0 utiliser ou non des robots. Variable texte (identifiant du pays). NON device (Appareil utilis\u00e9), AUCTION, URL, IP - Les robots ou humains pourraient avoir tendance \u00e0 utiliser certains appareils (pc vs mobile vs web ...) Variable texte (nom du device). NON BID_ID - Le bid_id est unique sur les lignes et ne contient pas de features \u00e0 extraire. Identifiant ne contenant pas de features \u00e0 extraire. OUI"},{"location":"projects/bot_detection_in_auction/#valeurs-nulles","title":"Valeurs nulles","text":"<p>On affiche le nombre de lignes vides par colonne</p> <p><code>&gt;&gt;&gt; df.isnull().sum()</code></p> <pre><code>bidder_id            0\nbid_id               0\nauction              0\nmerchandise          0\ndevice               0\ntime                 0\ncountry            184\nip                   0\nurl                  0\npayment_account      0\naddress              0\noutcome              0\ndtype: int64\n</code></pre> <p>Il n'y a de cellules vides que dans country</p>"},{"location":"projects/bot_detection_in_auction/#valeurs-dupliquees","title":"Valeurs dupliqu\u00e9es\u00a0?","text":"<p>Il n'y a pas de lignes dupliqu\u00e9es dans la table. Mais si on enl\u00e8ve le champs bid_id, il y a deux lignes dupliqu\u00e9es. On supprime ces deux lignes puisque ce sont des transactions qui se r\u00e9p\u00e8tent.</p>"},{"location":"projects/bot_detection_in_auction/#feature-engineering-visualisation-etou-test-des-hypotheses","title":"Feature Engineering, Visualisation et/ou Test des Hypoth\u00e8ses","text":"<p>Afin d'en savoir plus sur le pouvoir de s\u00e9parabilit\u00e9 des variables par rapport \u00e0 la variable \u00ab\u00a0outcome\u00a0\u00bb, nous allons effectuer une s\u00e9rie d'analyses et de visualisations.</p> <p>Par ailleurs, nous ferons une agr\u00e9gation des donn\u00e9es selon le \u00ab\u00a0bidder_id\u00a0\u00bb qui identifie la nature (robot ou humain) des joueurs.</p>"},{"location":"projects/bot_detection_in_auction/#librairies-utiles","title":"Librairies utiles","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport scipy.stats as stats\nfrom statsmodels.api import Logit\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n</code></pre>"},{"location":"projects/bot_detection_in_auction/#null-values-country","title":"Null values (country)","text":"<p>Seule la variable country contient des valeurs nulles. Nous allons remplacer toutes les valeurs nulles par une constante nomm\u00e9e \u00ab\u00a0NO_COUNTRY\u00a0\u00bb</p> <pre><code>## proportion des valeurs nulles\ndf.country.isnull().sum()/len(df)\n</code></pre> <p>0.00184</p>"},{"location":"projects/bot_detection_in_auction/#transformation-ip","title":"Transformation (ip)","text":"<code>&gt;&gt;&gt; df.ip</code> <pre><code>0        139.226.147.115\n1           21.67.17.162\n2         103.165.41.136\n3        239.250.228.152\n4        255.108.248.101\n              ...       \n99995      186.94.48.203\n99996      119.27.26.126\n99997       243.25.54.63\n99998      17.66.120.232\n99999       64.30.57.156\nName: ip, Length: 100000, dtype: object\n</code></pre> <pre><code>## identify network instead of device\ndf[\"ip\"] = df.ip.apply(lambda x: '.'.join(x.split('.')[:2]))\n</code></pre> <code>&gt;&gt;&gt; df.ip</code> <pre><code>0        139.226\n1          21.67\n2        103.165\n3        239.250\n4        255.108\n          ...   \n99995     186.94\n99996     119.27\n99997     243.25\n99998      17.66\n99999      64.30\nName: ip, Length: 100000, dtype: object\n</code></pre> <code>&gt;&gt;&gt; df.groupby(\"ip\").agg({\"country\": lambda x: x.nunique()}).country.value_counts()</code> <pre><code>1     25426\n2      7918\n3      1469\n4       205\n5        44\n6         9\n8         4\n7         3\n10        2\n9         2\n13        1\nName: country, dtype: int64\n</code></pre>"},{"location":"projects/bot_detection_in_auction/#normalisation-time","title":"Normalisation (time)","text":"<p>La variable \u00ab\u00a0time\u00a0\u00bb prends des valeurs tr\u00e8s petites (de l'ordre de 10**(-308)). On appliquera une normalisation lin\u00e9aire et qui donc ne r\u00e9duit pas l'information de cette variable.</p> <code>&gt;&gt;&gt; df[\"time\"].describe()</code> <pre><code>count     1.000000e+05\nmean     5.143168e-308\nstd       0.000000e+00\nmin      5.067452e-308\n25%      5.079343e-308\n50%      5.139090e-308\n75%      5.197759e-308\nmax      5.206746e-308\nName: time, dtype: float64\n</code></pre> <pre><code>## normalize time\ndf[\"time\"] = (df.time - df.time.min())/(df.time.max() - df.time.min())\n</code></pre> <code>&gt;&gt;&gt; df[\"time\"].describe()</code> <pre><code>count    100000.000000\nmean          0.543571\nstd           0.362483\nmin           0.000000\n25%           0.085364\n50%           0.514295\n75%           0.935483\nmax           1.000000\nName: time, dtype: float64\n</code></pre>"},{"location":"projects/bot_detection_in_auction/#visualisation-merchandise","title":"Visualisation (Merchandise)","text":"<pre><code>## histogrammes des produits\nax = sns.histplot(x=\"merchandise\", data=df, color='b')\nadd_labels_to_histplot(ax, title=\"Distribution of merchandises\")\n</code></pre> <pre><code>## histogramme des produits par outcome\nax = sns.histplot(x=\"merchandise\", data=df, hue=\"outcome\", palette=[\"b\",\"orange\"])\nadd_labels_to_histplot(ax, title=\"Distribution of merchandises by outcome\")\n</code></pre> <pre><code>## liste des outcome par produit\ndef human(x): return (x==0).sum()\ndef bot(x): return (x==1).sum()\ndf.groupby(\"merchandise\").agg({\"outcome\": [human, bot] })\n</code></pre> Category Human Bot Merchandise Books and Music 227 0 Home Goods 13002 2093 Jewelry 25004 0 Mobile 26228 6853 Office Equipment 21 0 Sporting Goods 26395 177"},{"location":"projects/bot_detection_in_auction/#encoding-merchandise","title":"Encoding (Merchandise)","text":"<p><code>&gt;&gt;&gt; df.merchandise.unique()</code></p> <p>array(['jewelry', 'mobile', 'books and music', 'office equipment',        'sporting goods', 'home goods'], dtype=object)</p> <code>def one_hot_encoder</code> <pre><code>REMOVE_EVIDENT_MERCHANDISE = False\n\nif REMOVE_EVIDENT_MERCHANDISE : df = df[df.merchandise.apply(lambda x: x in [\"mobile\", \"sporting goods\", \"home goods\"])]\nelse: df[\"non_robot_merchandise\"] = df.merchandise.apply(lambda x: int(x in ['jewelry', 'books and music', 'office equipment']))\n\ndef one_hot_encoder_v2(data, col_name):\n  data = data.copy()\n  new_cols = []\n  for i, elt in enumerate(data[col_name].unique()):\n    new_cols.append(f\"{col_name}_{i+1}\")\n    data[new_cols[-1]] = data[col_name].apply(lambda x: int(x==elt))\n  del data[col_name]\n  return data, new_cols\n\ndf2, new_cols = one_hot_encoder_v2(df, \"merchandise\")\ndf2\n</code></pre> <code>&gt;&gt;&gt; df2</code> bidder_id auction device time country ip url outcome non_robot_merchandise merchandise_1 merchandise_2 merchandise_3 merchandise_4 merchandise_5 merchandise_6 0 001068c415025a009fee375a12cff4fcnht8y 4ifac phone561 0.527973 bn 139.226 vasstdc27m7nks3 0 1 1 0 0 0 0 0 1 0030a2dd87ad2733e0873062e4f83954mkj86 obbny phone313 0.515267 ir 21.67 vnw40k8zzokijsv 0 0 0 1 0 0 0 0 2 00a0517965f18610417ee784a05f494d4dw6e l3o6q phone451 0.002705 bh 103.165 kk7rxe25ehseyci 0 1 0 0 1 0 0 0 3 00a0517965f18610417ee784a05f494d4dw6e du967 phone117 0.008986 tr 239.250 iu2iu3k137vakme 0 1 0 0 1 0 0 0 4 00a0517965f18610417ee784a05f494d4dw6e wx3kf phone16 0.009710 in 255.108 u85yj2e7owkz6xp 0 1 0 0 1 0 0 0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 99995 0ad17aa9111f657d71cd3005599afc24fd44y toxfq phone1036 0.962363 in 186.94 vasstdc27m7nks3 1 0 1 0 0 0 0 0 99996 0ad17aa9111f657d71cd3005599afc24fd44y ucb4u phone127 0.962380 in 119.27 vasstdc27m7nks3 1 0 1 0 0 0 0 0 99997 0ad17aa9111f657d71cd3005599afc24fd44y sg8yd phone383 0.962386 in 243.25 yweo7wfejrgbi2d 1 0 1 0 0 0 0 0 99998 0ad17aa9111f657d71cd3005599afc24fd44y toaj7 phone26 0.962393 in 17.66 4dd8ei0o5oqsua3 1 0 1 0 0 0 0 0 99999 0ad17aa9111f657d71cd3005599afc24fd44y 07axb phone25 0.962401 in 64.30 8zdkeqk4yby6lz2 1 0 1 0 0 0 0 0"},{"location":"projects/bot_detection_in_auction/#country","title":"country","text":"<pre><code>#sns.barplot(x=\"country\", y=\"outcome\", data=df)\n</code></pre> <p>no unique on merchandise because of unicite/bidder_id</p>"},{"location":"projects/bot_detection_in_auction/#groupage-par-bidder_id","title":"Groupage par bidder_id","text":"<p>Les variables actuelles</p> <p><code>&gt;&gt;&gt; df.columns</code></p> <p>Index(['bidder_id', 'auction', 'device', 'time', 'country', 'ip', 'url',        'outcome', 'non_robot_merchandise', 'merchandise_1', 'merchandise_2',        'merchandise_3', 'merchandise_4', 'merchandise_5', 'merchandise_6'],       dtype='object')</p> <p>Les nouvelles variables\u00a0:</p> Champs Descriptif nb_device Nombre d'appareils uniques utilis\u00e9s par bidder_id nb_auction Nombre d'ench\u00e8res auxquelles bidder_id a particip\u00e9 nb_ip Nombre d'adresses IP (identifiant de r\u00e9seau) uniques utilis\u00e9es par bidder_id nb_url Nombre d'URLs utilis\u00e9es par le bidder_id nb_country Nombre de pays uniques identifi\u00e9s par bidder_id outcome 1 si robot, 0 si homme non_robot_merchandise Cat\u00e9gories non choisies par les robots time Moyenne des \u00e9carts de temps pour chaque bidder_id nb_bid Nombre de bids effectu\u00e9es par bidder_id nb_merchandise_1 Nombre de \"jewelry\" uniques utilis\u00e9s par bidder_id nb_merchandise_2 Nombre de \"mobile\" uniques utilis\u00e9s par bidder_id nb_merchandise_3 Nombre de \"books and music\" uniques utilis\u00e9s par bidder_id nb_merchandise_4 Nombre de \"office equipment\" uniques utilis\u00e9s par bidder_id nb_merchandise_5 Nombre de \"sporting goods\" uniques utilis\u00e9s par bidder_id nb_merchandise_6 Nombre de \"home goods\" uniques utilis\u00e9s par bidder_id"},{"location":"projects/bot_detection_in_auction/#groupage-par-bidder_id-and-time","title":"Groupage par bidder_id and time","text":"<p>Pour chaque bidder_id, \u00e0 chaque instant donn\u00e9, on identifie le nombre de fois que l'utilisateur a utilis\u00e9 simultan\u00e9ment certaines ressources</p> <ul> <li> <p>adresse ip\u00a0: on identifie le nombre d'adresses ip utilis\u00e9es \u00e0 cet instant par cette personne</p> </li> <li> <p>auction\u00a0: le nombre d'ench\u00e8res o\u00f9 il \u00e9tait connect\u00e9 simultan\u00e9ment</p> </li> <li> <p>device\u00a0: le nombre d'appareils diff\u00e9rents qu'il a utilis\u00e9 simultan\u00e9ment</p> </li> <li> <p>url\u00a0: le nombre d'urls diff\u00e9rents qui ont \u00e9t\u00e9 utilis\u00e9s simultan\u00e9ment par l'enrichisseur</p> </li> <li> <p>country\u00a0: le nombre de pays depuis lesquels l'enrichisseur a fait simultan\u00e9ment la transaction .</p> </li> </ul> <p>Ensuite, pour chaque bidder_id et chaque temps, on somme ces 5 quantit\u00e9s.</p> <p>Ainsi, pour chaque bidder_id, on a une s\u00e9rie de valeurs sur laquelle on calcule des statistiques simples telles que la moyenne (my_agg_mean), la somme(my_agg_sum), l'\u00e9cart-type(my_agg_std) et le max(my_agg_max)</p> <code>def compute_groupby</code> <pre><code>def compute_groupby(filename):\n  dd = df.groupby([\"bidder_id\", \"time\"]).agg({\n              \"auction\":lambda x: x.nunique() - 1,\n              \"device\":lambda x: x.nunique() - 1,\n              \"country\":lambda x: x.nunique() - 1,\n              \"ip\":lambda x: x.nunique() - 1,\n              \"url\":lambda x: x.nunique() - 1,\n              \"outcome\":lambda x: x.unique()[0]\n              })\n  cls_ =list(set(dd.columns)-{'outcome','bidder_id','time'})\n  dd[\"my_agg\"] = dd[cls_].sum(axis=1)\n  dd2 = dd.reset_index()\n  dd_min_per_bidder_id = {bidder_id: dd2[dd2.bidder_id==bidder_id].time.min() for bidder_id in dd2.bidder_id.unique()}\n\n  def modif_time(x):\n    x[\"time\"] = x[\"time\"] - dd_min_per_bidder_id[x[\"bidder_id\"]]\n    return x\n\n  dd2 = dd2.apply(modif_time, axis=1)\n  dd2.to_csv(filename)\n  return dd2\n</code></pre>"},{"location":"projects/bot_detection_in_auction/#normalisation-tous-les-features","title":"Normalisation (tous les features)","text":"<p>Chaque variable est normalis\u00e9e entre 0 et 1 \u00e0 l'aide d'un min-max-scaler</p> Preview <p></p> <p></p> <p></p>"},{"location":"projects/bot_detection_in_auction/#visualisation-histogramme","title":"Visualisation - Histogramme","text":"<p>Pour chaque variable, nous faisons un histogramme afin de comprendre l'explicabilit\u00e9 de nos variables explicatives par rapport \u00e0 notre variable expliqu\u00e9 \u00ab\u00a0outcome\u00a0\u00bb. Nous ajoutons \u00e0 ces diagrammes une estimation de la densit\u00e9 de chacune de ses variables.</p> <p>Comme nos donn\u00e9es sont d\u00e9s\u00e9quilibr\u00e9es par rapport \u00e0 la variable expliqu\u00e9, nous effectuons une augmentation de donn\u00e9es par duplication pour ces visualisations.</p>"},{"location":"projects/bot_detection_in_auction/#les-agregations-sur-bidder_id","title":"Les agr\u00e9gations sur bidder_id","text":"<code>Nb_ip</code> <code>Nb_auction</code> <code>Nb_device</code> <code>Nb_url</code> <code>Nb_country</code> <code>Nb_bid</code> <p>On s'attends \u00e0 ce que les robots effectuent plus d'op\u00e9rations que les humains, ce qui se v\u00e9rifie facilement dans ces diagrammes. En effet, on repr\u00e9sente les histogrammes des variables (nb_ip, nb_auction, nb_device, nb_url, nb_country, nb_bid) (bleu pour les humains et rouge pour les robots).</p> <p>D'abord, on remarque qu'il y a beaucoup de valeurs nulles, ce qui signifie que pour chaque ressource (ip, url, ...), l'usage multiple de ces ressources n'est pas courant. En plus, parmi ceux qui font un usage multiple de ces ressources, on note des robots mais aussi des humains pour certaines ressources comme l'url ou l'appareil. Ce qui est contre intuitif, c'est que les humains d\u00e9passent parfois les robots sur ces m\u00e9triques.</p>"},{"location":"projects/bot_detection_in_auction/#produits-achetes","title":"produits achet\u00e9s","text":"<p>Avec les variables suivantes, nous introduiront les produits achet\u00e9s ainsi que le temps qui est tr\u00e8s important.</p> <code>Nb_merchandise_1 &amp; 2</code> <code>Nb_merchandise_4 &amp; 6</code> <code>Nb_merchandise_3</code> <code>Nb_merchandise_5</code> <p> </p> <p> </p> <p> </p> <p> </p> <p>Ces visualisations montrent bien que les robots se s\u00e9parent des hommes quand l'utilisation des ressources augmente</p>"},{"location":"projects/bot_detection_in_auction/#temps","title":"temps","text":"<ul> <li>time</li> </ul> <p>Cette variable est pertinante car la visualisation confirme l'\u00e9vidence\u00a0: le temps entre les connections des robots sont beaucoup plus court que ceux des hommes</p> <ul> <li>my_agg (aggr\u00e9gation sur le bidder_id et le temps)</li> </ul> <p></p> <p>Les agr\u00e9gations effectu\u00e9es sur le temps et le bidder_id, sont pertinentes car les robots ont plus tendance \u00e0 se connecter plusieurs fois de fa\u00e7on simultan\u00e9e</p> <p>Nous avons fait des tests statistiques en plus de ces visualisations mais ils ne seront pr\u00e9sent\u00e9s que bri\u00e8vement dans ce rapport.</p>"},{"location":"projects/bot_detection_in_auction/#binarization-on-continuous-features","title":"Binarization (on continuous features)","text":"<p>L'objectif est d'exploiter le pouvoir s\u00e9paratif de nos variables afin de rendre les algorithmes de classification plus efficaces.</p> <p>Chaque feature est projet\u00e9e sur l'espace {0,1} en d\u00e9finissant un seuil (threshold). Les valeurs sup\u00e9rieures au seuil correspondent \u00e0 1, tandis que les valeurs inf\u00e9rieures ou \u00e9gales au seuil correspondent \u00e0 0.</p> <p>Ce seuil est d\u00e9termin\u00e9 pour chaque feature. Pour cela, on choisit un nombre limit\u00e9 de valeurs possibles entre 0 et1 (range (0, 1, 0.05)). Pour chaque valeur,</p> <p>On transforme la s\u00e9rie en variable cat\u00e9gorielle</p> <p></p> <p>On s\u00e9pare la s\u00e9rie en deux \u00e9chantillons (outcome=0 vs outcome=1). Puis, on r\u00e9alise un test de student pour les moyennes de deux \u00e9chantillons pour voir s'ils sont significativement diff\u00e9rents</p> <p></p> <p>On s\u00e9pare la s\u00e9rie en deux \u00e9chantillons selon les deux cat\u00e9gories. Puis, on calcule la proportion de robots dans chaque \u00e9chantillon. Ensuite, on compare ces proportions \u00e0 l'aide d'un z-test pour voir s'ils sont significativement diff\u00e9rents</p> <p></p> <p>Si ces deux tests donnent des r\u00e9sultats conclusifs (p-value\\&lt;0.05), on les retients</p> <p>On choisit le seuil qui passe les deux tests avec des p-valeurs minimales. Ce seuil est utilis\u00e9 pour transformer la variable continue en variable binaire (0 et 1)</p>"},{"location":"projects/bot_detection_in_auction/#resampling-techniques","title":"Resampling Techniques","text":"<p>On remarque que sur 87 bidder_id, seuls 6 correspondent \u00e0 des robots. Nous appliquons alors une m\u00e9thode de oversampling nomm\u00e9e SMOTE. Cette m\u00e9thode, ne duplique pas des lignes mais cr\u00e9\u00e9 de nouveaux points \u00e0 partir des anciens en trouvant des points au milieu des points existants.</p>"},{"location":"projects/bot_detection_in_auction/#strategie-dentrainement","title":"Strat\u00e9gie d'entrainement","text":""},{"location":"projects/bot_detection_in_auction/#split-train-test","title":"Split train test","text":"<p>Nous s\u00e9parons d'abord notre base de donn\u00e9es en deux parties. Une partie pour l'entrainement et une autre pour le test (50% du dataset).</p>"},{"location":"projects/bot_detection_in_auction/#models","title":"Models","text":"<p>Nous utilisons principalement des mod\u00e8les de classification disponibles sous la librairie sklearn. Il s'agit notamment</p> <ul> <li> <p>Mod\u00e8les lin\u00e9aires (R\u00e9gression logistique (sous sklearn ou statsmodels), Support vecteur Machine (SVC), Descente de gradient stochastique (SDG))</p> </li> <li> <p>Tree based (Arbre de d\u00e9cision, Random Forest)</p> </li> <li> <p>D'autres mod\u00e8les (KNN, LDA)</p> </li> <li> <p>Des mod\u00e8les ensemblistes (AdaBoost, Gradient Boosting)</p> </li> </ul> <p>Nous pr\u00e9senterons d'abord les r\u00e9sultats obtenus sur les algorithmes\u00a0: R\u00e9gression logistique, Support vecteur machine et random Forest.</p>"},{"location":"projects/bot_detection_in_auction/#choix-des-hyperparametres","title":"Choix des hyperparam\u00e8tres","text":"<p>Pour certains mod\u00e8les, nous utilisons une m\u00e9thode nomm\u00e9e Grid Search. Nous choisissions certains param\u00e8tres importants ainsi que des valeurs possibles. Cette m\u00e9thode am\u00e9liore optimise le loss du mod\u00e8le sur le training set en fonction des valeurs des hyperparam\u00e8tres donn\u00e9es.</p> Algorithm Parameters Objectives Regression Logistic - max_iter: [5, 100, 1000, 2000]  - penalty: [\"l1\", \"l2\", \"elasticnet\", \"none\"]  - C: [0.01, 0.1, 0.5, 1]  - solver: [\"lbfgs\", \"liblinear\"] - Varying the number of iterations  - Testing different regularizations with different weights  - Testing the \"liblinear\" solver suitable for small datasets SVM - gamma='scale'  - probability=True  - C=1  - kernel=\"linear\"  - class_weight=\"balanced\" - Utilizing a linear model Random Forest - max_depth: [1, 3, 5, 15, 25]  - max_leaf_nodes: [1, 3, 5, 15]  - n_estimators: [10, 50, 100]  - max_features: [\"sqrt\", \"log2\", None]  - criterion: [\"gini\", \"entropy\", \"log_loss\"] - Varying tree sizes but maintaining a maximum limit  - Testing different metrics, particularly the log-loss"},{"location":"projects/bot_detection_in_auction/#selection-de-feature","title":"Selection de feature?","text":"<p>Nous avons fait nos mod\u00e8les sur 19 variables et nous souhaitons tiliser 5 variables. Afin de s\u00e9lectionner les variables les plus pertinentes pour chaque mod\u00e8le, nous utilisons une m\u00e9thode qui \u00e9limine les variables les moins importantes de fa\u00e7on it\u00e9rative. Une impl\u00e9mentation est disponible sous sklearn \u00e0 travers la fonction RFE (recursive feature elimination)</p>"},{"location":"projects/bot_detection_in_auction/#metriques-de-validation","title":"M\u00e9triques de validation","text":"<p>Afin de valider nos mod\u00e8les, nous utilisons des m\u00e9triques adapt\u00e9es \u00e0 la classification. Il s'agit de</p> <ul> <li> <p>Matrice de confusion\u00a0: Elle montre le nombre de pr\u00e9dictions (bien class\u00e9 vs mal class\u00e9) en fonction des cat\u00e9gories</p> </li> <li> <p>Pr\u00e9cision\u00a0: Mesure la capacit\u00e9 du mod\u00e8le \u00e0 identifier tous les humains m\u00eame s'il classe mal des robots. Il r\u00e9duit le nombre de faux positifs</p> </li> <li> <p>recall\u00a0: Mesure la capacit\u00e9 du mod\u00e8le \u00e0 identifier tous les robots m\u00eame si il classe mal des humains. Il r\u00e9duit le nombre de faux n\u00e9gatifs</p> </li> </ul> <p></p> <ul> <li>f1\u00a0: elle combine la precision et le recall</li> </ul> <p></p> <ul> <li>Courbe ROC\u00a0: Calculer l\\'aire sous la courbe ROC \u00e0 partir des scores de pr\u00e9diction.</li> </ul> <p>Toutes ces m\u00e9triques sont disponibles sous sklearn</p>"},{"location":"projects/bot_detection_in_auction/#prediction","title":"Pr\u00e9diction","text":""},{"location":"projects/bot_detection_in_auction/#librairies-utiles-pour-la-prediction","title":"Librairies utiles pour la pr\u00e9diction","text":"<pre><code>from sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix,r2_score,accuracy_score,roc_auc_score,f1_score,precision_score,recall_score\n</code></pre>"},{"location":"projects/bot_detection_in_auction/#demarche","title":"D\u00e9marche","text":"<p>Les mod\u00e8les sont utilis\u00e9s dans une classe qui impl\u00e9mente la m\u00e9thode suivante</p> <code>def compute_all</code> <pre><code>def compute_all(self, model_name, important_cols:list=None, oversample:bool=None, test_size:float=None, remove_cols:list=None, feature_eng:bool=None):\n      print(f\"model loader: {self.description}\")\n      list_fct = {\n            \"logit\": self.logit_regression,\n            \"logistic\": logistic_regression_sklearn,\n            \"svc\": svc_classifier,\n            \"knn\":knn_classifier,\n            \"sdg\":sdg_classifier,\n            \"tree\":decision_tree_classifier,\n            \"lda\": lda_classifier,\n            \"forest\":random_forest_classifier,\n            \"ada\": ada_boost_classifier,\n            \"xgboost\": gradient_boosting_classifier\n            }\n\n      # transform, split and oversample\n      X_train, X_test, y_train, y_test = self.get_train_test(model_name=model_name, important_cols=important_cols, oversample=oversample, test_size=test_size, remove_cols=remove_cols, feature_eng=feature_eng)\n\n      # get model\n      my_fct = list_fct[model_name]\n      print(f\"{'':~^50}\\n{model_name:~^50}\\n{'':~^50}\")\n\n      # train\n      y_test_proba, y_train_proba, y_train_pred, y_test_pred, nb_params = my_fct(X_train, y_train, X_test, y_test)\n\n      # evaluate\n      train_res, test_res = show_results(y_train, y_train_pred, y_train_proba,y_test, y_test_pred, y_test_proba)\n      g = f\"{model_name}: nb_params = {nb_params}\"\n      print(f\"{g:~^50}\")\n</code></pre> <p>Cette m\u00e9thode utilise des algorithmes qui impl\u00e9mentent chaque une m\u00e9thode de classification avec comme valeur ajout\u00e9e</p> <ul> <li> <p>La s\u00e9lection des hyperparam\u00e8tres \u00e0 l'aide de GridSearchCV</p> </li> <li> <p>Le choix \u00e9ventuel d'un threshold plus optimal pour les m\u00e9thodes qui g\u00e9n\u00e9rent des probabilit\u00e9s</p> </li> <li> <p>La s\u00e9lection des variables avec RFE</p> </li> </ul> <p>Les r\u00e9sultats sont pr\u00e9sent\u00e9s avec cet algorithme</p> <code>def show_results</code> <pre><code>def return_dict_scores(y, y_pred, pred_proba):\n  return {\"acc\":round(accuracy_score(y_pred, y),3), \"f1\":round(f1_score(y_pred, y),3), \"pres\":round(precision_score(y_pred, y),3), \"rec\":round(recall_score(y_pred, y),3), \"roc\":round(roc_auc_score(y_pred, pred_proba),3)}\n\ndef bestThressholdForF1(y_true,y_pred_proba):\n    best_thresh = None\n    best_score = 0\n    for thresh in np.arange(0.1, 0.501, 0.01):\n        y_pred = np.array(y_pred_proba)&gt;thresh\n        score = f1_score(y_true, y_pred)\n        if score &gt; best_score:\n            best_thresh = thresh\n            best_score = score\n    return best_score , best_thresh, return_dict_scores(y_true, y_pred, y_pred_proba)\n\ndef print_metrics(y, y_pred,pred_proba):\n  print(\"- confusion_matrix\\n\",confusion_matrix(y, y_pred))\n  print(f\"- accuracy = {100*accuracy_score(y, y_pred):.2f}%\") #better -&gt;1 ##accuracy = nb_sucess/nb_sample\n  print(f\"- f1 = {100*f1_score(y, y_pred):.2f}%\") #better -&gt;1 ##f1 = 2 * (precision * recall) / (precision + recall)\n  print(f\"- roc(area under the curve) = {100*roc_auc_score(y, pred_proba):.2f}%\") #better -&gt;1 ##area under ROC and AUC\n  print(f\"- precision = {100*precision_score(y, y_pred):.2f}%\") #better-&gt;1 ##precision = tp / (tp + fp) where (tp=true_positive; fp:false_positive)\n  print(f\"- recall = {100*recall_score(y, y_pred):.2f}%\") #better-&gt;1 ##precision = tp / (tp + fn) where (tp=true_positive; fn:false_negative)\n  print(f\"- bestThressholdForF1 = {bestThressholdForF1(y,pred_proba)}\")\n  return return_dict_scores(y, y_pred, pred_proba)\n\ndef show_results(y_train, y_train_pred, y_train_proba,y_test, y_test_pred, y_test_proba ):\n  print(\"\\n&gt;&gt;&gt;&gt; metriques sur la base de donn\u00e9es d'entrainement\")\n  train_res = print_metrics(y_train,y_train_pred,y_train_proba)\n  print(\"\\n&gt;&gt;&gt;&gt; metriques sur la base de donn\u00e9es de test\")\n  test_res = print_metrics(y_test, y_test_pred,y_test_proba)\n  return train_res, test_res\n</code></pre>"},{"location":"projects/bot_detection_in_auction/#quelques-exemples","title":"Quelques exemples","text":"&gt;&gt;&gt; R\u00e9gression logistique <pre><code>def logistic_regression_sklearn(X_train, y_train, X_test, y_test):\n  parameters = {\"max_iter\":[5,100,1000,2000], \"penalty\":[\"l1\", \"l2\", \"elasticnet\", \"none\"], \"C\":[0.01, 0.1, 0.5, 1], \"solver\":[\"lbfgs\", \"liblinear\"]}\n  clf = GridSearchCV(LogisticRegression(random_state=0, class_weight=\"balanced\"), parameters).fit(X_train, y_train)\n  print(clf.best_params_)\n\n  #clf = LogisticRegression(random_state=0, max_iter=1000, class_weight=\"balanced\").fit(X_train, y_train)\n  y_train_pred = clf.predict(X_train)\n  y_test_pred = clf.predict(X_test)\n  y_train_proba = clf.predict_proba(X_train)[:, 1] #[proba({label=1}/row_data) for row_data in X_train]\n  y_test_proba = clf.predict_proba(X_test)[:, 1]\n  return y_test_proba, y_train_proba, y_train_pred, y_test_pred, len(clf.feature_names_in_)\n</code></pre> &gt;&gt;&gt; Knn <pre><code>def knn_classifier(X_train, y_train, X_test, y_test):\n  parameters = {\"n_neighbors\":[1,2,5,10,15,20]}\n  clf = GridSearchCV(KNeighborsClassifier(), parameters).fit(X_train, y_train)\n  print(clf.best_params_)\n  #clf = KNeighborsClassifier(n_neighbors=10).fit(X_train, y_train)\n  y_train_pred = clf.predict(X_train)\n  y_test_pred = clf.predict(X_test)\n  y_train_proba = clf.predict_proba(X_train)[:, 1] #[proba({label=1}/row_data) for row_data in X_train]\n  y_test_proba = clf.predict_proba(X_test)[:, 1]\n  return y_test_proba, y_train_proba, y_train_pred, y_test_pred, len(clf.feature_names_in_)\n</code></pre>"},{"location":"projects/bot_detection_in_auction/#algorithmes","title":"Algorithmes","text":""},{"location":"projects/bot_detection_in_auction/#configurations","title":"Configurations","text":"<ul> <li>Base de donn\u00e9es utilis\u00e9e\u00a0: Par d\u00e9faut, c'est une base de donn\u00e9es de 87 lignes qui contient tous les features qu'on a caucul\u00e9 auparavant</li> </ul> <ul> <li> <p>Oversampling\u00a0: par d\u00e9faut True. Dans ce cas, on utilise la m\u00e9thode SMOTE</p> </li> <li> <p>Test_size\u00a0: par d\u00e9faut 50%</p> </li> <li> <p>Feature_eng\u00a0: si on applique le binarizer aux donn\u00e9es avant l'entrainement. Oui par d\u00e9faut</p> </li> <li> <p>Important_cols\u00a0: la liste des colonnes \u00e0 s\u00e9lectionner\u00a0: par d\u00e9faut, toutes les colonnes sont utilis\u00e9es m\u00eame si on fera un filtre pendant le fine tuning du mod\u00e8le (RFE)</p> </li> </ul>"},{"location":"projects/bot_detection_in_auction/#resultats","title":"R\u00e9sultats","text":"<ul> <li>Algorithmique</li> </ul> <ul> <li>Dataset avec oversampling (SMOTE) appliqu\u00e9 juste au training set</li> </ul> <ul> <li>Dataset avec oversampling (Duplication) appliqu\u00e9 \u00e0 tout le dataset</li> </ul> <ul> <li>Dataset avec oversampling (SMOTE) appliqu\u00e9 \u00e0 tout le dataset</li> </ul> L\u00e9gende <ul> <li> <p>acc\u00a0: accuracy score</p> </li> <li> <p>f1\u00a0: f1 score</p> </li> <li> <p>pres\u00a0: precision score</p> </li> <li> <p>rec\u00a0: recall score</p> </li> <li> <p>roc\u00a0: area under the curve</p> </li> <li> <p>Ada: AdaBoost</p> </li> <li> <p>Forest: Random Forest</p> </li> <li> <p>Knn: K nearest neighbors</p> </li> <li> <p>Lda: Linear Discriminant analysis</p> </li> <li> <p>logistic: Logistic regression (sklearn)</p> </li> <li> <p>Logit: Logistic regression Analysis (statsmodels)</p> </li> <li> <p>Sgd: Descente de gradient stochastique</p> </li> <li> <p>Svc\u00a0: Support vecteur Machine</p> </li> <li> <p>Tree\u00a0: Arbre de decision</p> </li> <li> <p>Xgboost\u00a0: Gradient Bossting</p> </li> </ul>"},{"location":"projects/bot_detection_in_auction/#comparaison-des-modeles","title":"Comparaison des mod\u00e8les","text":"<p>Nous remarquons en g\u00e9n\u00e9ral, que nos mod\u00e8les sur des features limit\u00e9s ou non, donnent les r\u00e9sultats int\u00e9ressants</p> <ul> <li> <p>Des accuracy &gt; 0.9 sur tous nos datasets, ce qui n'est pas \u00e9tonnant vu qu'un tuning est fait sur les hyperparam\u00e8tres pendant et apr\u00e8s l'entrainement.</p> </li> <li> <p>Presque tous les recall des mod\u00e8les donnent un r\u00e9sultat de 100%  </p> </li> <li> <p>Pour les mod\u00e8les (random Forest, regression logistique, SVM et decision tree), nous avons s\u00e9lectionn\u00e9 4 param\u00e8tres et les r\u00e9sultats sont toujours aussi bons. Ces mod\u00e8les sont donc meilleurs que les autres car plus explicables notamment la regression logistique. Concernat les mod\u00e8les bas\u00e9s sur les arbres (random Forest et decision tree), les visualisations (qui seront montr\u00e9s en bas) montrent que les rabres sont tr\u00e8s lisibles (profondeur de 3 et largeur faible car limit\u00e9 dans le hyperparameter tuning)</p> </li> <li> <p>Concernant le choix de la m\u00e9thode de oversampling, nous pouvons, en comparer les 3 tables,</p> </li> <li> <p>remarquer que malgr\u00e9 de bon recall, la premi\u00e8re table montre des pr\u00e9cisions de moins de 50% sur le test alors que les pr\u00e9cisions sont sup\u00e9rieures \u00e0 90% sur le train, ce qui montre du overfitting. Cette premi\u00e8re table repr\u00e9sente le dataset o\u00f9 on a appliqu\u00e9 le oversampling juste sur le training set.</p> </li> <li> <p>Remarquer que la table 3 pr\u00e9sente en g\u00e9n\u00e9ral de meilleurs r\u00e9sultats que la table 2, ce qui montre que la m\u00e9thode SMOTE est plus explicative qu'une simple duplication des donn\u00e9es non repr\u00e9sentatives.</p> </li> </ul> <p>En conclusion, il est pr\u00e9f\u00e9rable d'utiliser des mod\u00e8les \u00e0 peu de variables car ils sont relativement aussi bons que les autres. Il est \u00e9galement pr\u00e9f\u00e9rable d'utiliser SMOTE comme m\u00e9thode d'oversampling et de l'appliquer \u00e0 tout le dataset avant l'entrainement</p>"},{"location":"projects/bot_detection_in_auction/#review","title":"Review","text":""},{"location":"projects/bot_detection_in_auction/#overfitting","title":"Overfitting","text":"<p>Il n'y a pas eu quelques ph\u00e9nom\u00e8nes d'overfitting pendant l'impl\u00e9mentation.</p> <p>Comme un tuning important a \u00e9t\u00e9 fait sur les hyperparam\u00e8tres et qu'on a un nombre assez limit\u00e9 de donn\u00e9es, l'overfitting est \u00e9vit\u00e9 en utilisant 50% de la base de donn\u00e9es pour le test. Pour compenser le nombre limit\u00e9 de donn\u00e9es en train et l'impact du d\u00e9s\u00e9quilibre des classes, nous appliquons la m\u00e9thode SMOTE sur notre base de donn\u00e9es. N\u00e9anmoins, m\u00eame quand on applique la m\u00e9thode SMOTE uniquement sur la base de donn\u00e9es d'entrainement, on a \u00e9galement de bons r\u00e9sultats</p>"},{"location":"projects/bot_detection_in_auction/#choix-techniques","title":"Choix techniques","text":"<ul> <li> <p>Dans la construction des features, nous avons calcul\u00e9 pour chaque bidder_id, des aggr\u00e9gations. Ce sont des sommes. On am\u00e9liore le pouvoir de s\u00e9parabilit\u00e9 en normalisant ces donn\u00e9es par rapport au nombre de bid auquels le bidder_id a particip\u00e9.</p> </li> <li> <p>Faire un oversampling sur toute le dataset ou juste sur le training set\u00a0: le choix est fait en fonction du mod\u00e8le</p> </li> </ul>"},{"location":"projects/bot_detection_in_auction/#explicabilite-des-resultats","title":"Explicabilit\u00e9 des r\u00e9sultats","text":""},{"location":"projects/bot_detection_in_auction/#regression-logistique","title":"R\u00e9gression Logistique","text":""},{"location":"projects/bot_detection_in_auction/#random-forest","title":"Random Forest","text":"<p>Ce mod\u00e8le utilise 5 arbres de d\u00e9cisions et 4 variables et obtient une precision de 87% et un recall de 85%. Les processus de d\u00e9cision est assez simple et intuitif. Par ailleurs, on peut avoir de meilleurs r\u00e9sultats en moyennant les calculs (par groupe de bidder_id) par le nombre de bid. On a une am\u00e9lioration des m\u00e9triques</p>"},{"location":"projects/bot_detection_in_auction/#random-forest-avec-moyenne-par-groupe","title":"Random Forest (avec moyenne par groupe)","text":"<p>Ce mod\u00e8le utilise 5 arbres de d\u00e9cisions et 4 variables</p> <ul> <li> <p>Nb_auction</p> </li> <li> <p>Nb_device</p> </li> <li> <p>Nb_country</p> </li> <li> <p>Nb_url</p> </li> </ul> <p>Ces variables ont \u00e9t\u00e9 moyenn\u00e9es par rapport au nombre de bid auxquels ils ont particip\u00e9. Ainsi, on remarque que conform\u00e9ment aux r\u00e9sultats obtenus dans l'\u00e9tape de feature engineering, les nombre d'appareils petits correspondent \u00e0 des humains\u00a0; les url et les ench\u00e8res et pays de petites valeurs correspondent \u00e0 des robots</p>"},{"location":"projects/bot_detection_in_auction/#support-vecteur-machine","title":"Support vecteur Machine","text":"<p>Avec 4 variables, on ne peut pas voir les s\u00e9parations lin\u00e9aires mais on n'a pas voulu appliquer des m\u00e9thodes suppl\u00e9mentaires de r\u00e9duction de dimension telles que PCA ou SVD. N\u00e9anmoins, le nombre limit\u00e9 de features et les bons r\u00e9sultats obtenus sur ce mod\u00e8le montre qu'on pourrait avec plus d'analyses expliquer les r\u00e9sultats.</p>"},{"location":"projects/bot_detection_in_auction/#decision-tree","title":"Decision Tree","text":"<p>Les r\u00e9sultats sont tr\u00e8s bons avec un processus de d\u00e9cision assez simple et explicable.</p>"},{"location":"projects/bot_detection_in_auction/#discussion","title":"Discussion","text":"<ul> <li> <p>Les mod\u00e8les donnent en g\u00e9n\u00e9ral de bons r\u00e9sultats avec la normalisation</p> </li> <li> <p>Nous avons utilis\u00e9 une table avec des unique bidder_id pour aggr\u00e9ger en \u00e9vitant les donn\u00e9es dupliqu\u00e9es, ce qui a r\u00e9duit le nombre de points de donn\u00e9es. Mais, ensuite, nous avons fait du oversampling. N\u00e9anmoins, nous avons utilis\u00e9 SMOTE, une m\u00e9thode de oversampling qui ne duplique pas simplement les donn\u00e9es. Mais unee base de donn\u00e9es de validation aurait pu \u00eatre utilis\u00e9e afin de s\u00e9parer totalement les m\u00e9thodes de feature engineering de l'\u00e9valuation des mod\u00e8les</p> </li> <li> <p>Comme il y a peu de points de donn\u00e9es, nous avons mis l'accent sur le feature engineering afin d'avoir des variables pertinentes, ce qui nous as permis d'avoir des r\u00e9sultats satisfaisants (precision, recall&gt;96%) sur un mod\u00e8le lin\u00e9aire tel que la r\u00e9gression logistique</p> </li> <li> <p>Des corr\u00e9lations entre les variables explicatives n'impactent pas les r\u00e9sultats gr\u00e2ce \u00e0 la s\u00e9lection de variables qui entre en compte pendant l'entrainement du mod\u00e8le.</p> </li> <li> <p>Architecture de d\u00e9ploiement\u00a0: A partir de notre mod\u00e8le, on prend le maximum d'informations sur un bidder puis, on fait du feature engineering avant d'utiliser les mod\u00e8les. Ainsi,</p> </li> <li> <p>Pendant l'exploration, on a remaqu\u00e9 que certains produits \u00e9taient achet\u00e9s que par des humains (dans les limites de ce datset). Ainsi, nous aurions pu enlever les lignes correspondantes car la pr\u00e9diction est \u00e9vidente. D'un autre point de vue, les garder peut apporter des informations de correlations sur les autres features.</p> </li> </ul>"},{"location":"projects/bot_detection_in_auction/#conclusion","title":"Conclusion","text":"<p>Ce projet de classification pr\u00e9sente des sp\u00e9cificit\u00e9s int\u00e9ressantes. D'une part, on a pu valider \u00e0 priori qu'on a sur les robots \u00e0 l'aide des donn\u00e9es qu'on avait m\u00eame s\\'il y a eu des paradoxes quelques fois qui peuvent \u00eatre li\u00e9s au nombre limit\u00e9 de donn\u00e9es ou \u00e0 des facteurs ext\u00e9rieurs. D'autre part, la base de donn\u00e9es brute contient beaucoup de champs texte. Ainsi, la transformation des donn\u00e9es est l'\u00e9tape cruciale de ce projet. En plus, la base de donn\u00e9es, d\u00e9s\u00e9quilibr\u00e9e, n\u00e9cessite d'utiliser des techniques particuli\u00e8res pour l'entrainement. Nous avons \u00e9galement adapt\u00e9 les m\u00e9triques de validation au probl\u00e8me de classification et avons obtenu des r\u00e9sultats satisfaisants (recall, pr\u00e9cision &gt; 90%) avec jusque 4 \u00e0 5 features maximum, ce qui montre le potentiel que le feature engineering peut apporter \u00e0 la mod\u00e9lisation et \u00e0 l'explicabilit\u00e9 des mod\u00e8les. Enfin, avec des mod\u00e8les simples qui proposent des recall parfaits, nous pouvons \u00eatre s\u00fbr de d\u00e9terminer les robots m\u00eame si certains (tr\u00e8s peu) d'humains sont mals pr\u00e9dits</p>"},{"location":"projects/file-hosting-app/","title":"Flask based File Hosting (web app & api & python module & cli app)","text":""},{"location":"projects/file-hosting-app/#flask-based-file-hosting-web-app-api-python-module-cli-app","title":"Flask based File Hosting (web app &amp; api &amp; python module &amp; cli app)","text":""},{"location":"projects/file-hosting-app/#introduction","title":"Introduction","text":"<p>I've implemented A simple Flask application designed for sharing files. The application can be hosted on a server so users can upload files and generate links for sharing.</p>"},{"location":"projects/file-hosting-app/#prerequistes","title":"Prerequistes","text":"<ul> <li>python &gt;=3.9</li> </ul>"},{"location":"projects/file-hosting-app/#installation","title":"Installation","text":"<ol> <li>Clone the repository:</li> </ol> <pre><code>git clone https://github.com/hermann-web/simple-file-hosting-with-flask.git\n</code></pre> <ol> <li>Create a virtual environment and activate it:</li> </ol> <pre><code>python3 -m venv env\nsource env/bin/activate\n</code></pre> <ol> <li>Install the required dependencies:</li> </ol> <pre><code>pip install -r requirements.txt\n</code></pre> <ol> <li>Start the Flask development server:</li> </ol> <pre><code>python app.py\n</code></pre> <ol> <li>Access the application by visiting http://localhost:5000 in your web browser.</li> </ol>"},{"location":"projects/file-hosting-app/#development-details","title":"Development Details","text":"<ul> <li>The application allows users to upload files and share them with others by providing a link to the file.</li> <li>To use the application, users need to clone the repository, create a virtual environment, install the required dependencies, and start the Flask development server.</li> <li>Once the server is running, users can access the application by visiting http://localhost:5000 in their web browser.</li> <li>The main page displays a list of shared files, and users can upload a file by clicking on \"Upload a File\" and selecting the file they want to share.</li> <li>The uploaded files will be listed on the main page for download.</li> <li>The application can be deployed on a remote server or cloud provider, such as AWS, Google Cloud, or Heroku.</li> <li>You can see the step by step implementation of the code</li> </ul>"},{"location":"projects/file-hosting-app/#usage","title":"Usage","text":""},{"location":"projects/file-hosting-app/#access-the-flask-web","title":"access the flask web","text":"<ul> <li>The main page displays a list of shared files.</li> <li>To upload a file, click on \"Upload a File\" and select the file you want to share.</li> <li>The uploaded files will be listed on the main page for download.</li> </ul>"},{"location":"projects/file-hosting-app/#access-the-app-through-an-api","title":"access the app through an api","text":"<ul> <li>you can access the api with the routes <code>http://localhost:5000/api/*</code></li> <li>The file cli_app/cli_app.py to access the api along with a context manager to handle sessions</li> <li>you can read the api documentation</li> </ul>"},{"location":"projects/file-hosting-app/#access-the-apps-api-with-a-cli-app","title":"access the app's api with a cli app","text":"<ul> <li>The file cli_app/sharefile.py provide a cli app to access the api context manager</li> <li>Using your cli, you can get list, upload and download files. The api will be called behind the hood by cli_app/cli_app.py</li> <li>you can read the api documentation</li> </ul>"},{"location":"projects/file-hosting-app/#deployment-guide","title":"Deployment Guide","text":"<p>To deploy the File Sharing App, follow these steps:</p> <ol> <li> <p>Choose a remote server or cloud provider to host your application. Popular options include AWS, Google Cloud, and Heroku.</p> </li> <li> <p>Set up an instance or virtual machine on your chosen server.</p> </li> <li> <p>Connect to your remote server.</p> </li> <li> <p>Install the required dependencies.</p> </li> <li> <p>Modify the Flask application's configuration to use a production-ready web server.</p> </li> <li> <p>Configure your domain or subdomain to point to the IP address of your remote server.</p> </li> <li> <p>Set up SSL/TLS certificates for secure HTTPS communication.</p> </li> <li> <p>Start the Flask application using the production-ready web server.</p> </li> <li> <p>Verify that your file sharing app is accessible.</p> </li> <li> <p>Monitor the deployed application for errors and performance issues.</p> </li> </ol> <p>Remember to follow best practices for securing your deployed application.</p>"},{"location":"projects/image-to-latex-formula/","title":"Streamlit App for Formula OCR using pix2tex","text":""},{"location":"projects/image-to-latex-formula/#introduction","title":"Introduction","text":"<p>I've implemented and deployed a Streamlit application designed for writing a LaTeX formula from an image containing that formula. The application is hosted on Hugging Face Spaces and github so users can upload images freely and copy LaTeX formulas.</p> <p>For the frontend, I use Streamlit, a framework that makes frontend for data projects easy. For the backend converter, I use a Python module named pix2tex that uses computer vision to generate the LaTeX formula.</p> <pre><code>from PIL import Image\nfrom pix2tex.cli import LatexOCR\n\nimg = Image.open('path/to/image.png')\nmodel = LatexOCR()\nprint(model(img))\n</code></pre> <p>You can access the application online at https://pix2tex.streamlit.app/ or https://huggingface.co/spaces/hermann-web/pix2tex.</p> <p></p> <p>You can read the following if you want to know the implementation details or clone the project.</p>"},{"location":"projects/image-to-latex-formula/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python &gt;=3.9</li> <li>torch &gt;=2.0.0</li> <li>Streamlit</li> <li>Pillow</li> <li>pix2tex</li> </ul>"},{"location":"projects/image-to-latex-formula/#installation","title":"Installation","text":"<ol> <li> <p>Clone the repository:</p> <pre><code>git clone https://huggingface.co/spaces/hermann-web/pix2tex\ncd pix2tex\n</code></pre> </li> <li> <p>Create a virtual environment and activate it:</p> </li> </ol> <code>For Linux &amp; Mac</code> <code>For Windows</code> <pre><code>python3 -m venv myenv\nsource myenv/bin/activate\n</code></pre> <pre><code>python -m venv myenv\n./myenv/Scripts/activate\n</code></pre> <ol> <li> <p>Install the required dependencies:</p> <pre><code>pip install -r requirements.txt\n</code></pre> </li> <li> <p>Start the Streamlit server:</p> <pre><code>streamlit run app.py\n</code></pre> </li> <li> <p>Access the application by visiting http://localhost:8501 in your web browser.</p> </li> </ol>"},{"location":"projects/image-to-latex-formula/#development-details","title":"Development Details","text":"<p>During the development of the Streamlit-based formula OCR app, several key components and decisions were made to ensure a smooth and efficient workflow.</p>"},{"location":"projects/image-to-latex-formula/#backend-with-pix2tex","title":"Backend with pix2tex","text":"<p>The core functionality of converting images into LaTeX formulas is powered by the <code>pix2tex</code> Python module. This module leverages computer vision techniques to perform LaTeX OCR on images. The following snippet illustrates how the image processing is handled:</p> <pre><code>from PIL import Image\nfrom pix2tex.cli import LatexOCR\n\ndef process_image(image):\n    # Perform LaTeX OCR on the image\n    img = Image.open(image)\n    model = LatexOCR()\n    latex_formula = model(img)\n    return latex_formula\n</code></pre> <p>The <code>LatexOCR</code> class from <code>pix2tex</code> is responsible for extracting LaTeX formulas from images, making the app suitable for mathematical content extraction.</p>"},{"location":"projects/image-to-latex-formula/#streamlit-for-frontend","title":"Streamlit for Frontend","text":"<p>The frontend of the application is built using Streamlit, a powerful Python library for creating web applications with minimal effort. Streamlit allowed for rapid prototyping and easy integration with the backend.</p> <p>The image is rendered with a streamlit function <code>st.latex</code>:</p> <pre><code>import streamlit as st\nst.latex(f\"\\n{latex_formula}\\n\")\n</code></pre>"},{"location":"projects/image-to-latex-formula/#usage","title":"Usage","text":"<p>Add information about how users can use the application, including any specific instructions or features.</p>"},{"location":"projects/image-to-latex-formula/#deployment-guide","title":"Deployment Guide","text":""},{"location":"projects/image-to-latex-formula/#with-streamlitio","title":"With streamlit.io","text":"<p>Deployment on streamlit.io is pretty fastforward <sup>1</sup>. You can put the application on a public repository. Then, Go to the website https://share.streamlit.io/deploy, signup with you github account for example, then choose the repository and your pyhton script.</p>"},{"location":"projects/image-to-latex-formula/#on-streamlit","title":"On streamlit","text":"<p>To deploy the app on Hugging Face, follow these steps:</p> <ol> <li> <p>Create a Hugging Face Account:</p> <ul> <li>If you don't have a Hugging Face account, create one on Hugging Face.</li> </ul> </li> <li> <p>Create a New Space:</p> <ul> <li>Log in to your Hugging Face account.</li> <li>Navigate to the Spaces page.</li> <li>Click on \"New Space\" and give it a name, e.g., \"FileHostingApp.\"</li> </ul> </li> <li> <p>Upload Your Code:</p> <ul> <li>Inside your local <code>pix2tex</code> directory, create a <code>.zip</code> archive of your code.</li> <li>Upload the <code>.zip</code> archive to your newly created Space.</li> </ul> </li> <li> <p>Set Up Environment Variables:</p> <ul> <li>In your Hugging Face Space, go to \"Settings.\"</li> <li>Under the \"Environment Variables\" section, add the necessary variables, such as any API keys or configuration parameters.</li> </ul> </li> <li> <p>Deploy the App:</p> <ul> <li>Go back to the \"Overview\" tab in your Space.</li> <li>Click on \"Deploy Model\" and follow the instructions to deploy your Streamlit app.</li> </ul> </li> <li> <p>Access Your Deployed App:</p> <ul> <li>Once deployed, you can access your app through the provided link.</li> </ul> </li> </ol> <ol> <li> <p>Streamlit documentation: Create an app \u21a9</p> </li> </ol>"},{"location":"projects/introducing-lissajou-for-animated-plots/","title":"Introducing the lissajou Package: Animate Stunning Lissajou Curves and Beyond","text":"<p>Lissajou curves have always held a certain fascination for me. Their intricate, ever-changing patterns hold a beauty that's both mathematical and artistic. So, one day, I decided to embark on a journey to bring these curves to life through animation.</p> <p>A Look Back: The Spark of Inspiration:</p> <p>It all started with a simple Wikipedia search. I was particularly drawn to the idea of closed sinusoids, those graceful loops that seemed to dance across the screen. As an educator who enjoys helping students visualize complex concepts, I saw an opportunity to explore the animation of functions changing over time, something I hadn't delved into deeply before.</p> <p>Finding a solution for plotting functions dynamically with matplotlib wasn't too challenging. I stumbled upon some helpful code online online that dealt with animating images. However, I wanted to adapt it to my specific needs. While the original code relied on global variables to maintain persistence across animation frames, I opted for a more structured approach using classes. This allowed for cleaner code organization and easier abstraction for generating various figures.</p> <p>The Exploration Begins: From Simple to Stunning:</p> <p>Through experimentation, I explored various shapes like sinusoids, ellipses, and circles. Adjusting the animation speed proved to be a crucial aspect. Sometimes, the dynamic view offered an exhilarating glimpse into the evolution of the curves. Other times, a static image, capturing the essence of a closed sinusoid, held its own charm.</p> <p>The Unexpected Delight: A Dynamic Ratio:</p> <p>The real magic happened when I decided to experiment with a dynamic ratio for the Lissajou curve parameters. Introducing <code>cos(t/100)</code> into the equation for the ratio <code>a/b</code> created a truly mesmerizing effect. The curves seemed to pulsate and transform with an added layer of complexity. This unexpected discovery was a highlight of the entire project for me.</p> <p>Sharing the Joy: Invitation to Play:</p> <p>I encourage you to explore the code and animations yourself. The beauty of open-source tools like <code>matplotlib</code> and the power of Python made this project a rewarding experience.</p> <p>The good news? You don't need to be a coding expert to create stunning animations! Here's how to get started with the Lissajou Animation Framework:</p> <ol> <li> <p>Installation: Open your terminal (command prompt) and type: <code>pip install lissajou</code></p> </li> <li> <p>Import the Library: In your Python script, type:</p> </li> </ol> <pre><code>from lissajou.anim import GenericLissajou\n</code></pre> <ol> <li>Create an Animation: Now, create an animation object:</li> </ol> <pre><code>animation = GenericLissajou()\n</code></pre> <ol> <li>Run the Animation: Simply call the <code>show()</code> method to display your animation:</li> </ol> <pre><code>animation.show()\n</code></pre> <p>That's it! This code will generate a basic Lissajou curve animation.</p> <p>And like in a restaurant, you can update the parameters of the lissajou function as well as the visualiation configurations</p> <p>Also, like in a restaurant, you have more a variety of proposotions: Instead of the  Generic Lissajou Curve (<code>GenericLissajou</code>), you can also test out Lissajou Curve with Varying Amplitude, with Sinusoidal Amplitude Lissajou Curve, with Fixed Ratio, Ellipse Animation and more.</p> <p>You will find more informations on the documentation online</p> <p>Beyond the Basics: Explore the Full Potential:</p> <p>The Lissajou Animation Framework is much more than just Lissajou curves! It offers a powerful toolkit for creating diverse animations:</p> <ul> <li>Image Animation: Bring static images to life by dynamically changing their pixel values.</li> <li>2D Parametric Animation: Create animations based on mathematical equations, allowing for endless possibilities.</li> <li>3D Parametric Animation (Optional Libraries Needed): Take your animations to a new dimension by generating and visualizing 3D shapes in motion.</li> </ul> <p>The framework also includes pre-implemented animations for various Lissajou curve types, saving you time and effort.</p> <p>Dive Deeper:</p> <p>For a comprehensive look at the framework's capabilities and advanced usage, check out the pypi project including full documentation. But don't be afraid to experiment and explore on your own! The world of animation awaits.</p> <p>Let's create something beautiful together!</p>"},{"location":"projects/introducing-two-new-packages-for-streamlining-file-conversions-in-python/","title":"Introducing Two New Packages for Streamlining File Conversions in Python","text":"<p>In the realm of data processing and manipulation, efficient handling of file conversions is often a crucial requirement. Python, with its rich ecosystem of libraries and frameworks, provides powerful tools for tackling such tasks.</p> <p>I've developed two packages with the intention of offering a robust file conversion design pattern separated from specific implementation details. While many existing solutions focus on providing direct conversion implementations, these packages provide a structured approach that facilitates easy extension, customization, and integration into various projects.</p> <p>In this blog post, we'll explore how to unlock the magic of data manipulation by streamlining file conversion operations using two powerful packages: <code>opencf-core</code> and <code>opencf</code>.</p>"},{"location":"projects/introducing-two-new-packages-for-streamlining-file-conversions-in-python/#introducing-opencf-core","title":"Introducing <code>opencf-core</code>","text":"<p>The <code>opencf-core</code> package serves as a solid foundation for building robust file conversion utilities in Python. It offers a modular and extensible architecture designed to simplify the process of reading from and writing to various file formats. Let's delve into some of its key features:</p>"},{"location":"projects/introducing-two-new-packages-for-streamlining-file-conversions-in-python/#modular-inputoutput-handlers","title":"Modular Input/Output Handlers","text":"<p>The framework defines abstract base classes for file readers and writers, allowing for easy extension and customization. This modular approach ensures flexibility in handling different types of input and output files.</p>"},{"location":"projects/introducing-two-new-packages-for-streamlining-file-conversions-in-python/#support-for-various-file-formats","title":"Support for Various File Formats","text":"<p>With built-in support for common file formats such as text, CSV, JSON, XML, Excel, and image files, the framework caters to diverse conversion needs out of the box.</p>"},{"location":"projects/introducing-two-new-packages-for-streamlining-file-conversions-in-python/#mime-type-detection","title":"MIME Type Detection","text":"<p>The inclusion of a MIME type guesser utility enables automatic detection of file MIME types based on content, facilitating seamless conversion operations.</p>"},{"location":"projects/introducing-two-new-packages-for-streamlining-file-conversions-in-python/#exception-handling","title":"Exception Handling","text":"<p>Custom exceptions are implemented to handle errors related to unsupported file types, empty suffixes, file not found, and mismatches between file types, ensuring robust error management during conversions.</p>"},{"location":"projects/introducing-two-new-packages-for-streamlining-file-conversions-in-python/#base-converter-class","title":"Base Converter Class","text":"<p>The framework provides an abstract base class for implementing specific file converters, offering a standardized interface for conversion operations.</p>"},{"location":"projects/introducing-two-new-packages-for-streamlining-file-conversions-in-python/#resolved-input-file-representation","title":"Resolved Input File Representation","text":"<p>A class is introduced to represent input files with resolved file types, ensuring consistency and correctness in conversion tasks.</p> <p>To start using <code>opencf-core</code>, you can install it using pip from the PyPI repository:</p> <pre><code>pip install opencf-core\n</code></pre> <p>To illustrate the usage of <code>opencf-core</code>, let's consider an example of converting a CSV file to JSON:</p> <pre><code>from opencf-core.io_handler import CSVReader, JSONWriter\nfrom opencf-core.base_converter import BaseConverter, ResolvedInputFile\nfrom opencf-core.filetypes import FileType\n\nclass CSVToJSONConverter(BaseConverter):\n    file_reader = CSVReader()\n    file_writer = JSONWriter()\n\n    @classmethod\n    def _get_supported_input_type(cls) -&gt; FileType:\n        return FileType.CSV\n\n    @classmethod\n    def _get_supported_output_type(cls) -&gt; FileType:\n        return FileType.JSON\n\n    def _convert(self, input_path: Path, output_path: Path):\n        # Implement conversion logic from CSV to JSON\n        pass\n\n# Usage\ninput_file_path = \"input.csv\"\noutput_file_path = \"output.json\"\ninput_file = ResolvedInputFile(input_file_path)\noutput_file = ResolvedInputFile(output_file_path)\nconverter = CSVToJSONConverter(input_file, output_file)\nconverter.convert()\n</code></pre>"},{"location":"projects/introducing-two-new-packages-for-streamlining-file-conversions-in-python/#exploring-opencf","title":"Exploring <code>opencf</code>","text":"<p>Building upon the <code>opencf-core</code>, the <code>opencf</code> package offers a collection of Python scripts tailored for common file format conversions. These scripts leverage the capabilities of the framework to provide convenient solutions for handling various data transformations. Let's take a closer look:</p>"},{"location":"projects/introducing-two-new-packages-for-streamlining-file-conversions-in-python/#extensive-conversion-support","title":"Extensive Conversion Support","text":"<p>The package includes scripts for converting between a wide range of file formats, including text, XML, JSON, CSV, and Excel. This broad support caters to diverse conversion needs across different domains.</p>"},{"location":"projects/introducing-two-new-packages-for-streamlining-file-conversions-in-python/#integration-with-opencf-core","title":"Integration with <code>opencf-core</code>","text":"<p>Utilizing classes from the <code>opencf-core</code> package for file I/O operations, MIME type detection, and exception handling ensures consistency and reliability in conversion tasks.</p>"},{"location":"projects/introducing-two-new-packages-for-streamlining-file-conversions-in-python/#command-line-interface","title":"Command-Line Interface","text":"<p>Each conversion script is equipped with a command-line interface, allowing users to specify input and output file paths, as well as input and output file types, for seamless execution of conversion tasks.</p>"},{"location":"projects/introducing-two-new-packages-for-streamlining-file-conversions-in-python/#extensibility","title":"Extensibility","text":"<p>The modular converter classes provided by <code>opencf</code> make it easy to add support for additional file formats or customize existing conversion functionalities as per project requirements.</p> <p>To start using <code>opencf</code>, you can clone the repository and set up the environment as follows:</p> <pre><code>git clone https://github.com/Hermann-web/file-converter\ncd file-converter/openconv-core\npython -m venv venv\nsource venv/bin/activate  # for Unix/Linux\n.\\venv\\Scripts\\activate   # for Windows\npip install -r requirements.txt\n</code></pre> <p>To demonstrate the usage of <code>opencf</code>, let's consider an example of converting an XML file to JSON using the provided CLI:</p> <pre><code>openconv input.xml -t XML -o output.json -ot JSON\n</code></pre>"},{"location":"projects/introducing-two-new-packages-for-streamlining-file-conversions-in-python/#upcoming-enhancements","title":"Upcoming Enhancements","text":"<p>In future iterations of <code>opencf</code>, we plan to extend the conversion methods along with reader and writer classes. Additionally, contributions to ameliorate <code>opencf-core</code> are welcome. However, it's worth noting that the <code>opencf-core</code> repository can still be reused independently. For example, it can be utilized to create custom versions of <code>opencf</code> or other file conversion utilities.</p>"},{"location":"projects/introducing-two-new-packages-for-streamlining-file-conversions-in-python/#conclusion","title":"Conclusion","text":"<p>Efficient handling of file conversion tasks is essential in various data-centric applications. With the <code>opencf-core</code> and <code>opencf</code> packages, Python developers have powerful tools at their disposal to tackle such challenges effectively. Whether you're building custom conversion utilities or integrating conversion functionalities into larger projects, these packages provide a solid foundation for streamlining your workflow.</p> <p>To begin leveraging the capabilities of these packages, simply install them along with their dependencies using your preferred package manager. You can then explore the provided examples and documentation to kickstart your file conversion endeavors.</p> <p>Start exploring the world of file conversion with Python today and unlock new possibilities in data processing and manipulation!</p> <p>For more information and detailed usage instructions, please refer to the documentation and README files available in the respective package repositories:</p> <ul> <li><code>opencf-core</code></li> <li><code>opencf</code></li> </ul> <p>Happy coding!</p>"},{"location":"projects/search-engine-for-domain-specific-french-users/","title":"Search Engine for domain specific french users","text":"<p>This tutorial is a work i've made as part of my internship at Safran. The objective is to create a search engine that take as input, a user query and return a shortlist of stored documents highly relevant to the search query. Parsing both the user query and the document in a same comparison space is a must. We use a projection in a vector space after common NLP tasks as tokenisation, stop word removal, lemmatization correction. Finally, unlike english as a standard for common NLP projects, we use tools that worked very well with french texts.</p> <p>In this post, we will be building a semantic documents search engine</p>"},{"location":"projects/search-engine-for-domain-specific-french-users/#prerequistes","title":"Prerequistes","text":"<ul> <li>Python &gt;=3.7</li> <li>NLTK</li> <li>Pandas</li> <li>Scikit-learn</li> </ul>"},{"location":"projects/search-engine-for-domain-specific-french-users/#imports","title":"Imports","text":"<pre><code>import re, json\nimport unicodedata, string\nimport time\nimport operator\nimport numpy as np \nimport pandas as pd\nfrom collections import Counter\n</code></pre> <pre><code>from collections import defaultdict\nimport nltk \nfrom nltk.tokenize import word_tokenize\nfrom nltk import pos_tag\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import wordnet as wn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n</code></pre> <pre><code>nltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('stopwords')\n</code></pre>"},{"location":"projects/search-engine-for-domain-specific-french-users/#data","title":"data","text":"<p>Files used in the notebook are stored in the folder data of the repository</p>"},{"location":"projects/search-engine-for-domain-specific-french-users/#1-creer-les-keywords-a-partir-dune-phrase","title":"**1: Cr\u00e9er les keywords \u00e0 partir d'une phrase","text":"<p>en se basant sur les mots d'un dictionnaire et un corpus de texte en passant par la tokenization, la correction, la lemmatization et le removeStopWords**</p>"},{"location":"projects/search-engine-for-domain-specific-french-users/#preprocessing","title":"preprocessing","text":"<pre><code>def get_dico():\n    textdir = \"liste.de.mots.francais.frgut_.txt\"\n    try:DICO = open(textdir,'r',encoding=\"utf-8\").read()\n    except: DICO = open(textdir,'r').read()\n\n    return DICO\n\n\ndef remove_accents(input_str):\n    \"\"\"This method removes all diacritic marks from the given string\"\"\"\n    norm_txt = unicodedata.normalize('NFD', input_str)\n    shaved = ''.join(c for c in norm_txt if not unicodedata.combining(c))\n    return unicodedata.normalize('NFC', shaved)\n\ndef clean_sentence(texte):\n    # Replace diacritics\n    texte = remove_accents(texte)\n    # Lowercase the document\n    texte = texte.lower()\n    # Remove Mentions\n    texte = re.sub(r'@\\w+', '', texte)\n    # Remove punctuations\n    texte = re.sub(r'[%s]' % re.escape(string.punctuation), ' ', texte)\n    # Remove the doubled space\n    texte = re.sub(r'\\s{2,}', ' ', texte)\n    #remove whitespaces at the beginning and the end\n    texte = texte.strip()\n\n    return texte\n\n\ndef tokenize_sentence(texte):\n        #clean the sentence \n    texte = clean_sentence(texte)\n        #tokenize \n    liste_words = texte.split()\n        #return \n    return liste_words\n\ndef strip_apostrophe(liste_words):\n    get_radical = lambda word: word.split('\\'')[-1]\n    return list(map(get_radical,liste_words))\n\ndef pre_process(sentence):\n    #remove '_' from the sentence \n    sentence = sentence.replace('_','')\n\n    #get words fro the sentence \n    liste_words = tokenize_sentence(sentence)\n    #cut out 1 or 2 letters ones \n    liste_words = [elt for elt in liste_words if len(elt)&gt;2]\n    #prendre le radical apr\u00e8s l'apostrophe\n    liste_words = strip_apostrophe(liste_words)\n    print('\\nsentence to words : ',liste_words)\n    return liste_words\n</code></pre>"},{"location":"projects/search-engine-for-domain-specific-french-users/#correction-des-mots","title":"correction des mots","text":"<pre><code>def edits1(word):\n    \"All edits that are one edit away from `word`.\"\n    letters    = 'abcdefghijklmnopqrstuvwxyz'\n    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n    deletes    = [L + R[1:]               for L, R in splits if R]\n    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)&gt;1]\n    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n    inserts    = [L + c + R               for L, R in splits for c in letters]\n    return set(deletes + transposes + replaces + inserts)\n\ndef edits2(word): \n    \"All edits that are two edits away from `word`.\"\n    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n\ndef known(words): \n    \"The subset of `words` that appear in the dictionary of WORDS.\"\n    return set(w for w in words if w in WORDS)\n\ndef candidates(word): \n    \"Generate possible spelling corrections for word.\"\n    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n\n\n\n\ndef DICO_ET_CORRECTEUR():\n    \"cette fonction retourne la liste des mots de dictionnaire\"\n    DICO = get_dico()\n    WORDS = Counter(pre_process(DICO)) #Counter prends un str et retourne une sorte de liste enrichie\n    \"correction des mots \"\n    N = sum(WORDS.values())\n    P = lambda word: WORDS[word] / N #\"Probability of `word`.\"\n\n    correction = lambda word: max(candidates(word), key=P) #\"Most probable\n    return WORDS,correction\n\nWORDS,CORRECTION = DICO_ET_CORRECTEUR()\n</code></pre>"},{"location":"projects/search-engine-for-domain-specific-french-users/#stopwords-et-stemmingpremier-exemple","title":"stopwords et stemming(premier exemple)","text":"<pre><code>## stopwords #//https://www.ranks.nl/stopwords/french\nwith open('stp_words_.txt','r') as f:\n    STOPWORDS = f.read()\n\n## bdd de stemmer\nwith open(\"sample_.json\",'r',encoding='cp1252') as json_file:\n    #json_file.seek(0)\n    LISTE = json.load(json_file)\nmy_stemmer = lambda word: LISTE[word] if word in LISTE else word\n</code></pre>"},{"location":"projects/search-engine-for-domain-specific-french-users/#fonction-sentence_to_correct_words","title":"fonction: SENTENCE_TO_CORRECT_WORDS","text":"<pre><code>def SENTENCE_TO_CORRECT_WORDS(sentence):\n    \"cette fonction retourne la liste des mots du user\"\n    print('\\n------------pre_process--------\\n')\n    liste_words = pre_process(sentence)\n    print(liste_words)\n    print('\\n------------correction--------\\n')\n    liste_words = list(map(CORRECTION,liste_words))\n    print(liste_words)\n    print('\\n------------stemming--------\\n')\n    liste_words = list(map(my_stemmer,liste_words))\n    print(liste_words)\n    print('\\n------------remove stop-words--------\\n')\n    liste_words = [elt for elt in liste_words if elt not in STOPWORDS]\n    print(liste_words)\n    print('\\n-------------------------------------\\n')\n    return liste_words\n</code></pre> SENTENCE_TO_CORRECT_WORDS usage <pre><code>SENTENCE_TO_CORRECT_WORDS('La PR reste au statut \u00ab\\xa0Approuve(e)\\xa0\u00bb et il n\u2019y a pas de commande\\\"\\'')\n</code></pre> <pre><code>------------pre_process--------\n['reste', 'statut', 'approuve', 'n\u2019y', 'pas', 'commande']\n\n------------correction--------\n['reste', 'statut', 'approuve', 'non', 'pas', 'commande']\n\n------------stemming--------\n['rester', 'statut', 'approuver', 'non', 'pas', 'commander']\n\n------------remove stop-words--------\n['rester', 'statut', 'approuver', 'commander']\n\n-------------------------------------\n['rester', 'statut', 'approuver', 'commander']\n</code></pre>"},{"location":"projects/search-engine-for-domain-specific-french-users/#create-dataset","title":"Create dataset","text":"Create dataset <pre><code>def open_file(textdir):\nfound = False\ntry:texte = open(textdir,'r',encoding=\"utf-8\").read();found=True\nexcept:pass\ntry: texte = open(textdir,'r').read();found=True \nexcept: pass\nif not found:\n    texte = open(textdir,'r',encoding='cp1252').read();found=True\nreturn  texte\ndef add_col(df_news,titre,keywords):\nreturn df_news.append(dict(zip(df_news.columns,[titre, keywords])), ignore_index=True)\n\nliste_pb = [elt for elt in open_file('liste_pb_.txt').split('\\n') if elt]\ndf_new = df_news.drop(df_news.index)\nfor i,titre in enumerate(liste_pb):\nkeywords = ','.join(SENTENCE_TO_CORRECT_WORDS(titre))\ndf_new = add_col(df_new,titre,keywords)\ndf_new.head()\n</code></pre> Output <pre><code>                    Subject                                    Clean_Keyword\n0  Message d'erreur : \"Le fournisseur ARIBA n'exi...  message,erreur,fournisseur,aria,exister\n1  Message d'erreur : \"Commande d\u2019article non aut...  message,erreur,commander,article,autoriser,oto\n2  Message d'erreur : \"Statut utilisateur FERM ac...  message,erreur,statut,utilisateur,actif,oto\n3  Message d'erreur : \"Statut systeme TCLO actif ...  message,erreur,statut,systeme,col,actif,nord\n4  Message d'erreur \"___ Cost center change could...  message,erreur,coat,centrer,changer,cold,affecter\n5  Messaeg d'erreur \"___ OTP change could not be ...  message,erreur,otp,changer,cold,affecter\n6  Messaeg d'erreur \"Entrez Centre de couts\"          message,erreur,entrer,centrer,cout\n7  Message d'erreur \"Indiquez une seule imputatio...  message,erreur,indiquer,imputation,statistique\n8  Message d'erreur \"Imputations CO ont des centr...  message,erreur,imputation,centrer,profit\n9  Message d'erreur \"Poste ___ Ordre ___ depassem...  message,erreur,poster,ordre,depassement,budget\n10  Message d'erreur \"Entrez une quantite de comma...  message,erreur,entrer,quantite,commander\n11  Message d'erreur \"Indiquez la quantite\"          message,erreur,indiquer,quantite\n12  Message d'erreur \"Le prix net doit etre superi...  message,erreur,prix,net,superieur\n...  ...  ...\n...  ...  ...\n...  ...  ...\n57  UO4-5 Commande | Envoi d'une commande manuelle  uo4,commander,envoi,commander,manuel\n58  UO5-4 Reception | Anomalie workflow  uo5,reception,anomalie,workflow\n59  UO5-1 Reception | Modification(s) de reception(s)  uo5,reception,modification,reception\n60  UO5-2 Reception | Annulation(s) de reception(s)  uo5,reception,annulation,reception\n61  UO5-3 Reception | Forcer la reception  uo5,reception,forcer,reception\n62  UO3-5 Demande d'achat | Demande de support cre...  uo3,demander,achat,demander,support,creation\n63  UO3-6 Demande d'achat | Demande de support mod...  uo3,demander,achat,demander,support,modification\n64  UO3-7 Demande d'achat | Demande de support ann...  uo3,demander,achat,demander,support,annulation\n65  UO4-2 Commande | Demande de support modificati...  uo4,commander,demander,support,modification,co...\n</code></pre>"},{"location":"projects/search-engine-for-domain-specific-french-users/#tokenize-and-stemmingsecond-exemple","title":"tokenize and stemming(second exemple)","text":"<pre><code># WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\ndef wordLemmatizer(data,colname):\n    tag_map = defaultdict(lambda : wn.NOUN)\n    tag_map['J'] = wn.ADJ\n    tag_map['V'] = wn.VERB\n    tag_map['R'] = wn.ADV\n    file_clean_k =pd.DataFrame()\n    for index,entry in enumerate(data):\n\n        # Declaring Empty List to store the words that follow the rules for this step\n        Final_words = []\n        # Initializing WordNetLemmatizer()\n        word_Lemmatized = WordNetLemmatizer()\n        # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n        for word, tag in pos_tag(entry):\n            # Below condition is to check for Stop words and consider only alphabets\n            if len(word)&gt;1 and word not in stopwords.words('french') and word.isalpha():\n                word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n                Final_words.append(word_Final)\n            # The final processed set of words for each iteration will be stored in 'text_final'\n                file_clean_k.loc[index,colname] = str(Final_words)\n                file_clean_k.loc[index,colname] = str(Final_words)\n                file_clean_k=file_clean_k.replace(to_replace =\"\\[.\", value = '', regex = True)\n                file_clean_k=file_clean_k.replace(to_replace =\"'\", value = '', regex = True)\n                file_clean_k=file_clean_k.replace(to_replace =\" \", value = '', regex = True)\n                file_clean_k=file_clean_k.replace(to_replace ='\\]', value = '', regex = True)\n\n    return file_clean_k\n\n\ndef wordLemmatizer_(sentence):\n    #prendre une phrase que retourner un str (les mots sont separes par des ,)\n    preprocessed_query = preprocessed_query = re.sub(\"\\W+\", \" \", sentence).strip()\n    tokens = word_tokenize(str(preprocessed_query))\n    q_df = pd.DataFrame(columns=['q_clean'])\n    idx = 0\n    colname = 'keyword_final'\n    q_df.loc[idx,'q_clean'] =tokens\n    print('\\n\\n---inputtoken');print(q_df.q_clean)\n    print('\\n\\n---outputlemma');print(wordLemmatizer(q_df.q_clean,colname).loc[idx,colname])\n    return wordLemmatizer(q_df.q_clean,colname).loc[idx,colname]\n</code></pre>"},{"location":"projects/search-engine-for-domain-specific-french-users/#2-trouver-la-meilleure-phrase-dans-une-liste-de-phrase","title":"2: trouver la meilleure phrase dans une liste de phrase","text":""},{"location":"projects/search-engine-for-domain-specific-french-users/#method-tf-idf","title":"method: TF-Idf","text":"<p>TfIdf stands for: Term Frequency Inverse Document Frequency</p> <p>In order to compare the user input to existing sentence in database, we will go throught two process</p> <ul> <li>Normalize database: Apply the pre-processing method to all sentences in the database. We then have, for each sentence, a list of keywords</li> <li>For each keyword kw for each sentence st, we compute,</li> <li>\\(frqc(word,sentence)\\) : occurrence of the keyword word in the sentence</li> <li>\\(doc\\_frqc(word)\\): number of sentences where the word appears</li> <li>\\(N\\) = Number of sentences</li> </ul> <p>\\(tf(wd,stc) =  \\frac {frqc(wd,stc)}{ \\sum_{stc} frqc(wd,stc) }\\)</p> <p>\\(idf(wd) =  \\log(\\frac{N}{doc\\_frqc(wd)})\\)</p> <p>\\(tfidf(wd,stc) = tf(wd,stc) *idf(wd)\\)</p> Example <ul> <li>st1: The computer is down</li> <li>st2: We need to change the computers</li> <li>st3: Changements have to be handle by the IT</li> </ul> <p>keywords per sentence</p> <pre><code>st1: [computer , down]\nst2: [need, change, computer]\nst3: [change, handle, IT]\n</code></pre> <p>vocabulary: [computer , down, need, change, handle, IT]</p> tf sentence1 sentence2 sentence3 computer \u00bd \u00bd 0 down 1 0 0 need 0 1 0 change 0 \u00bd \u00bd handle 0 0 1 IT 0 0 1 <p>idf values for the keywords</p> <p>N =number_of_sentences =  3</p> idf computer log(3/2) down log(3/1) need log(3/1) change log(3/2) handle log(3/1) IT log(3/1) <p>example for sentence 2: computing of the keywords tfidf values</p> \\[ tfidf('computer') = tf('computer', sentence2)*idf('computer') = 1/2 * log(3/2)\\\\ tfidf('down') = 0 * log(3/1)\\\\ tfidf('need') = 1 * log(3/1)\\\\ tfidf('change') = 1/2 * log(3/2)\\\\ tfidf('handle') = 0 * log(3/1)\\\\ tfidf('IT') = 0 * log(3/1)\\\\ \\] <p>vectorisation of the sentence 2</p> <pre><code>sentence2 &lt;==&gt; [ 0.5 * log(3 / 2), 0, 1 * log(3 / 2), 0.5 *  log(3 /2) , 0, 0]                                      \n</code></pre> <p>vectorisation of the sentences</p> \\[ sentence1 &lt;==&gt; [\\ 0.5*log(3/2),\\ log(3/1),\\ 0 ,\\ 0,\\ 0]\\\\ sentence2 &lt;==&gt; [\\ 0.5*log(3/2),\\ 0, 1*log(3/2),\\ 0.5* log(3/2) ,\\ 0,\\ 0]\\\\ sentence3 &lt;==&gt; [\\ 0, \\ 0, \\ 0,\\ 0.5 * 1*log(3/2),\\  log(3/1),\\ 1*log(3/1)] \\] <p>similarities between the user input and the sentences</p> <ul> <li>user input: The IT have replaced all of the computers</li> <li>keywords: [ 'IT', 'all',  'computer']</li> <li>keywords found in dictionnary: [ 'IT','computer']</li> <li>vectorization: [1,0,0,0,1]</li> </ul> <p>scores</p> \\[ sentence1: tfidf(sentence1)*vector =  [\\ 0.5*log(3/2),\\ log(3/1),\\ 0 ,\\ 0,\\ 0] *[1,0,0,0,1] =  0.5*log(3/2) \\\\ sentence1:0.5*log(3/2)\\\\ sentence2:  0.5*log(3/2)\\\\ sentence3: log(3/1) \\]"},{"location":"projects/search-engine-for-domain-specific-french-users/#fonction-cosine_similarity_t","title":"fonction: cosine_similarity_T","text":"<pre><code>def init(df_news):\n  ##  Create Vocabulary\n  vocabulary = set()\n  for doc in df_news.Clean_Keyword:\n      vocabulary.update(doc.split(','))\n  vocabulary = list(vocabulary)# Intializating the tfIdf model\n  tfidf = TfidfVectorizer(vocabulary=vocabulary)# Fit the TfIdf model\n  tfidf.fit(df_news.Clean_Keyword)# Transform the TfIdf model\n  tfidf_tran=tfidf.transform(df_news.Clean_Keyword)\n  globals()['vocabulary'],globals()['tfidf'],globals()['tfidf_tran'] = vocabulary,tfidf,tfidf_tran\n</code></pre>"},{"location":"projects/search-engine-for-domain-specific-french-users/#create-a-vector-for-querysearch-keywords","title":"Create a vector for Query/search keywords","text":"<pre><code>def gen_vector_T(tokens,df_news,vocabulary,tfidf,tfidf_tran):\n  Q = np.zeros((len(vocabulary)))    \n  x= tfidf.transform(tokens)\n  #print(tokens[0].split(','))\n  #print(keywords)\n  for token in tokens[0].split(','):\n\n      try:\n          ind = vocabulary.index(token)\n          Q[ind]  = x[0, tfidf.vocabulary_[token]]\n          print(token,':',ind)\n      except:\n          print(token,':','not found')\n          pass\n  return Q\n</code></pre>"},{"location":"projects/search-engine-for-domain-specific-french-users/#cosine-similarity-function","title":"Cosine Similarity function","text":"<pre><code>def cosine_sim(a, b):\n    if not np.linalg.norm(a) and not np.linalg.norm(b): return -3\n    if not np.linalg.norm(a):return -1\n    if not np.linalg.norm(b):return -2\n    cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n    return cos_sim   \n\ndef cosine_similarity_T(k, query,df_news,vocabulary=None,tfidf=None,tfidf_tran=None,mine=True):\n    try:\n      vocabulary = globals()['vocabulary']\n      tfidf = globals()['tfidf']\n      tfidf_tran = globals()['tfidf_tran']\n    except:\n      print('up exception')\n      init(df_news)\n    q_df = pd.DataFrame(columns=['q_clean'])\n    if mine:q_df.loc[0,'q_clean'] =','.join(SENTENCE_TO_CORRECT_WORDS(query))\n    else:q_df.loc[0,'q_clean'] = wordLemmatizer_(query)\n\n\n    print('\\n\\n---q_df');print(q_df)\n\n    print('\\n\\n')\n    d_cosines = []\n    query_vector = gen_vector_T(q_df['q_clean'],df_news,vocabulary,tfidf,tfidf_tran )\n    for d in tfidf_tran.A:\n        d_cosines.append(cosine_sim(query_vector, d ))\n\n    out = np.array(d_cosines).argsort()[-k:][::-1]\n    #print(\"\")\n    d_cosines.sort()\n    a = pd.DataFrame()\n    for i,index in enumerate(out):\n        a.loc[i,'index'] = str(index)\n        a.loc[i,'Subject'] = df_news['Subject'][index]\n    for j,simScore in enumerate(d_cosines[-k:][::-1]):\n        a.loc[j,'Score'] = simScore\n    return a\n</code></pre>"},{"location":"projects/search-engine-for-domain-specific-french-users/#test-cosine_similarity_t","title":"Test: cosine_similarity_T","text":"<pre><code>def test(data,sentence,init_=False,mine=True):\n  if not init_:\n    deb = time.time();print('\\n\\n## ## ## ## ## #')\n    init(df_news)\n    print('\\n## ## ## ## ## #temps init: ', time.time()-deb)\n  deb = time.time();print('\\n\\n## ## ## ## ## #')\n  print(cosine_similarity_T(10, sentence,df_news))\n  print('\\n## ## ## ## ## #temps methode 1: ', time.time()-deb)\nsentence = 'Message d\\'erreur \\\"La qte livree est differente de la qte facturee ; fonction impossible\"'\nsentence = 'erreur de conversion'\nsentence = 'message d\\'erreur'\nsentence = \"groupe d'acheteurs non d\u00e9fini\"\nsentence = \"UO4\"\nsentence = \"le fournisseur MDM n'existe pas\"\ninit(df_new) \n\ncosine_similarity_T(10,sentence,df_new )\n</code></pre> output <pre><code>------------pre_process--------\n['fournisseur', 'mdm', 'existe', 'pas']\n\n------------correction--------\n['fournisseur', 'mdm', 'existe', 'pas']\n\n------------stemming--------\n['fournisseur', 'mdm', 'exister', 'pas']\n\n------------remove stop-words--------\n['fournisseur', 'mdm', 'exister']\n\n-------------------------------------\n\n    index                   Subject                           Score\n0  19  Message d'erreur \"Le fournisseur MDM___ n\u2019exis...  0.781490\n1  0  Message d'erreur : \"Le fournisseur ARIBA n'exi...  0.600296\n2  20  Message d'erreur \"Le fournisseur MDM___ est bl...  0.587467\n3  14  Message d'erreur \"Le centre de profit __ n'exi...  0.236420\n4  33  Message d'erreur \"Il existe des factures pour ...  0.214371\n5  53  Message d'erreur \"Fournisseur non present dans...  0.142208\n6  18  Message d'erreur \"Validation ___ : le compte _...  0.000000\n7  30  Message d'erreur \"Renseigner correctement le d...  0.000000\n8  29  Message d'erreur \"Article ___ non gere dans la...  0.000000\n9  28  Message d'erreur \"Fonctions oblig. Suivantes n...  0.000000\n...  ...  ...\n</code></pre>"},{"location":"projects/search-engine-for-domain-specific-french-users/#conclusion-and-discussions","title":"Conclusion and Discussions","text":"<ul> <li>The system is very fast and reliable</li> <li>The shortlist, after tests is highly relevant to the user query with the best answer, appearing either first or second</li> <li>Some methods employed must be adapted for english or other languages different of french</li> </ul>"},{"location":"projects/js-course/","title":"The JavaScript Edge: Intermediate to Advanced Proficiency","text":"<ul> <li>JavaScript Fundamentals: A Beginner's Guide to Essential Concepts and Best Practices</li> <li>Intermediate JavaScript Concepts: Bridging Theory and Practice</li> </ul>"},{"location":"projects/js-course/javascript-concepts-intermediate/","title":"Intermediate JavaScript Concepts: Bridging Theory and Practice","text":""},{"location":"projects/js-course/javascript-concepts-intermediate/#introduction","title":"Introduction","text":"<p>Ready to take your JavaScript skills beyond the basics and delve into intermediate concepts?</p> <p>Embark on a journey into the realm of intermediate JavaScript, where theory meets practice and foundational knowledge evolves into practical application. This guide is tailored for developers seeking to expand their understanding and proficiency in JavaScript, bridging the gap between beginner and advanced levels.</p>"},{"location":"projects/js-course/javascript-concepts-intermediate/#overview","title":"Overview","text":"<p>In this document, we will cover:</p> <ul> <li>Intermediate data manipulation techniques for primitive and reference types</li> <li>Asynchronous programming patterns and their implementation in JavaScript</li> <li>Functional programming concepts and their relevance in modern JavaScript development</li> <li>Object-oriented programming principles applied to JavaScript</li> <li>Best practices for writing maintainable and scalable JavaScript code</li> <li>Testing methodologies and debugging strategies for intermediate-level applications</li> </ul> <p>By exploring these topics, you'll enhance your JavaScript proficiency and gain insights into the theoretical foundations and practical applications of intermediate-level concepts. Whether you're transitioning from beginner to intermediate or seeking to deepen your existing knowledge, this guide will equip you with the skills to tackle more complex JavaScript challenges with confidence.</p> <p>Ready to elevate your JavaScript skills to the next level? Let's dive in!</p>"},{"location":"projects/js-course/javascript-concepts-intermediate/#objects-beyond-simple-dictionaries","title":"Objects: Beyond Simple Dictionaries","text":"<p>In JavaScript, objects serve as fundamental data structures, akin to dictionaries in other programming languages. But what exactly are objects? Think of them as containers that can hold various pieces of related information. Each piece of information, called a property, consists of a key-value pair. For example, an object representing a book might have properties like \"title,\" \"author,\" and \"number of pages.\"</p> <p>In web development, JavaScript objects serve as fundamental data structures, allowing developers to represent real-world entities and their attributes. Let's consider an example of a book object from a fictional online bookstore:</p> <pre><code>const book = {\n    title: \"The Great Gatsby\",\n    author: \"F. Scott Fitzgerald\",\n    genre: \"Fiction\",\n    yearPublished: 1925,\n    price: 12.99,\n    available: true\n};\n\nconsole.log(book.title); // Output: The Great Gatsby\n</code></pre> <p>Understanding the syntax and manipulation of objects is crucial for dynamic web development. Objects allow us to interact with users, dynamically update web pages, and communicate with external services. In simpler terms, they enable the interactive and responsive behavior we see in modern websites.</p>"},{"location":"projects/js-course/javascript-concepts-intermediate/#integrated-development-environments-ides","title":"Integrated Development Environments (IDEs)","text":"<p>When writing JavaScript code, developers often rely on Integrated Development Environments (IDEs) like Visual Studio Code. An IDE is a software application that provides comprehensive tools for writing, testing, and debugging code. It offers features like code auto-completion, syntax highlighting, and integrated debugging tools, streamlining the development process.</p>"},{"location":"projects/js-course/javascript-concepts-intermediate/#servers-and-asynchronous-programming","title":"Servers and Asynchronous Programming","text":"<p>JavaScript's versatility extends beyond client-side scripting to server-side development. But what does \"server-side development\" mean? In web development, servers are computers that store and deliver web pages to users. Server-side JavaScript allows developers to write code that runs on these servers, enabling dynamic server-side interactions.</p> <p>Let's consider an example of asynchronous programming in a Node.js server application:</p> <pre><code>// Example of fetching data asynchronously from a server\napp.get('/api/books', async (req, res) =&gt; {\n    try {\n        const books = await Book.find(); // Asynchronous database query\n        res.json(books);\n    } catch (error) {\n        console.error(error);\n        res.status(500).json({ message: \"Server Error\" });\n    }\n});\n</code></pre> <p>One of the key features of JavaScript on the server-side is its support for asynchronous programming. Asynchronous programming allows tasks to be executed concurrently, without blocking the main execution thread. This means that while one task is being processed, other tasks can continue to run in the background. As a result, JavaScript applications can handle multiple operations simultaneously, enhancing responsiveness and efficiency.</p> <p>Asynchronous programming allows tasks to be executed concurrently, enhancing responsiveness and efficiency in JavaScript applications.</p>"},{"location":"projects/js-course/javascript-concepts-intermediate/#manipulating-values-primitives-vs-references","title":"Manipulating Values: Primitives vs. References","text":"<p>JavaScript distinguishes between primitive types, such as numbers and strings, and reference types, such as objects and arrays. But what's the difference between them?</p> <p>Primitive types represent simple data values and are stored directly in memory. When you assign a primitive value to a variable, you're essentially storing the value itself. For example, if you assign the number 42 to a variable, that variable will directly hold the value 42.</p> <p>Reference types, on the other hand, represent complex data structures and are stored as references in memory. When you assign a reference type to a variable, you're actually storing a reference to the memory location where the data is stored. This distinction becomes important when manipulating values, as changes to a reference type affect all variables that reference the same data.</p> <p>Let's illustrate the difference with an example:</p> <pre><code>let x = 10; // Primitive type (number)\nlet y = { name: 'John' }; // Reference type (object)\n\nlet a = x; // 'a' holds the value of 'x' (copying)\nlet b = y; // 'b' holds a reference to the same object as 'y'\n\na = 20; // Modifying 'a' does not affect 'x'\nb.name = 'Jane'; // Modifying 'b' affects the original object 'y'\n\nconsole.log(x, y); // Output: 10 { name: 'Jane' }\n</code></pre> <p>Understanding how JavaScript handles primitive and reference types is essential for effective value manipulation and data management.</p>"},{"location":"projects/js-course/javascript-concepts-intermediate/#functions-and-functional-programming","title":"Functions and Functional Programming","text":"<p>Functions play a central role in JavaScript, allowing developers to encapsulate and reuse code. But JavaScript also supports functional programming paradigms, which emphasize the use of functions as first-class citizens.</p> <p>In functional programming, functions are treated as values that can be passed around, assigned to variables, and returned from other functions.</p> <p>Let's see an example of a higher-order function:</p> <pre><code>// Example of a higher-order function\nfunction applyOperation(x, y, operation) {\n    return operation(x, y);\n}\n\nfunction add(x, y) {\n    return x + y;\n}\n\nconst result = applyOperation(5, 3, add);\nconsole.log(result); // Output: 8\n</code></pre> <p>This enables powerful programming techniques like higher-order functions, which take other functions as arguments or return them as results.</p> <p>Understanding functional programming concepts can lead to cleaner, more modular code that is easier to understand and maintain.</p>"},{"location":"projects/js-course/javascript-concepts-intermediate/#classes-and-object-oriented-programming","title":"Classes and Object-Oriented Programming","text":"<p>JavaScript supports object-oriented programming (OOP) through the use of classes and prototypes. But what exactly are classes and prototypes?</p> <p>Classes are blueprints for creating objects with similar properties and behaviors. They encapsulate data (in the form of properties) and behavior (in the form of methods) into a single entity. When you create an object from a class, you're essentially creating an instance of that class with its own set of properties and methods.</p> <p>Let's consider an example of creating objects using classes:</p> <pre><code>// Example of using classes for object creation\nclass Product {\n    constructor(name, price) {\n        this.name = name;\n        this.price = price;\n    }\n\n    display() {\n        console.log(`${this.name}: $${this.price}`);\n    }\n}\n\nconst product1 = new Product('Laptop', 999);\nproduct1.display(); // Output: Laptop: $999\n</code></pre> <p>Prototypes, on the other hand, are mechanisms for sharing behavior between objects. Every JavaScript object has a prototype, which serves as a template for the object's properties and methods. By leveraging prototypes, developers can create inheritance hierarchies and reuse code more efficiently.</p> <p>Classes provide a blueprint for creating objects with similar properties and behaviors, facilitating code organization and reusability.</p>"},{"location":"projects/js-course/javascript-concepts-intermediate/#clean-code-practices","title":"Clean Code Practices","text":"<p>Writing clean, readable, and maintainable code is essential for long-term project success. But what does \"clean code\" mean?</p> <p>Clean code adheres to principles like DRY (Don't Repeat Yourself) and separation of concerns. It is well-organized, with meaningful variable names, consistent formatting, and clear comments. By following clean code practices, developers can improve code quality, enhance collaboration, and reduce the likelihood of bugs.</p> <p>Adhering to clean code principles improves code quality, enhances collaboration, and reduces the likelihood of bugs.</p>"},{"location":"projects/js-course/javascript-concepts-intermediate/#testing-and-debugging-strategies","title":"Testing and Debugging Strategies","text":"<p>Robust testing and debugging practices are crucial for ensuring the reliability and stability of JavaScript applications. But what do these practices entail?</p> <p>Testing involves verifying that individual components of a system function correctly. This can include unit tests, which test individual functions or modules, as well as integration tests, which test the interaction between different components. Additionally, end-to-end tests simulate user interactions with the application to ensure that it behaves as expected.</p> Example of testing with Jest <pre><code>// Example of testing with Jest\nfunction sum(a, b) {\n    return a + b;\n}\n\nmodule.exports = sum;\n</code></pre> <p>Debugging, on the other hand, involves identifying and fixing errors in code. This can be done using tools like browser developer tools or integrated development environments (IDEs), which allow developers to inspect variables, set breakpoints, and step through code execution.</p> Example of debugging with Chrome DevTools <pre><code>// Example of debugging with Chrome DevTools\nfunction fetchData() {\n    return new Promise((resolve, reject) =&gt; {\n        setTimeout(() =&gt; {\n            resolve(\"Data fetched successfully!\");\n        }, 2000);\n    });\n}\n\nasync function getData() {\n    console.log(\"Fetching data...\");\n    const data = await fetchData();\n    console.log(data);\n}\n\ngetData(); // Debug in Chrome DevTools to analyze async behavior\n</code></pre> <p>Effective testing and debugging practices ensure the reliability and performance of JavaScript applications.</p>"},{"location":"projects/js-course/javascript-concepts-intermediate/#advanced-debugging-techniques","title":"Advanced Debugging Techniques","text":"<p>Mastering advanced debugging techniques can help developers diagnose and troubleshoot complex issues in JavaScript codebases. These techniques include setting conditional breakpoints, monitoring network activity, and profiling code performance.</p> <p>By leveraging debugging tools effectively, developers can identify bottlenecks, optimize code, and improve the overall quality of their applications.</p>"},{"location":"projects/js-course/javascript-concepts-intermediate/#conclusion","title":"Conclusion","text":"<p>As we explore these advanced JavaScript concepts, remember that practice and experimentation are key to mastering the language. By building projects, solving problems, and seeking feedback from peers, you'll continue to deepen your understanding and become a proficient JavaScript developer.</p>"},{"location":"projects/js-course/javascript-fundamentals-simplified/","title":"JavaScript Fundamentals: A Beginner's Guide to Essential Concepts and Best Practices","text":""},{"location":"projects/js-course/javascript-fundamentals-simplified/#introduction","title":"Introduction","text":"<p>Are you new to JavaScript or looking to refresh your understanding of its fundamental concepts?</p> <p>Dive into this beginner-friendly guide to explore the core principles and best practices in JavaScript!</p> <p>JavaScript (ES6), short for ECMAScript 6, is a major version of the JavaScript programming language. It introduces new features, syntax enhancements, and advanced programming concepts. This documentation is tailored for beginners and individuals seeking to recall the fundamentals of JavaScript. Let's embark on this learning journey together!</p>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#overview","title":"Overview","text":"<p>In this comprehensive guide, you will learn:</p> <ul> <li>Essential JavaScript data types, control structures, and functions</li> <li>Error handling techniques and best practices</li> <li>The use of classes, objects, arrays, and maps in JavaScript</li> <li>Comparison operators and logical operations</li> <li>Control structures including if/else, switch/case, for loops, and while loops</li> <li>Best practices for variable declaration and scope</li> <li>Tips for writing clean and maintainable JavaScript code</li> </ul> <p>Ready to elevate your JavaScript skills? Let's get started!</p>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#variables-and-constants","title":"Variables and Constants","text":""},{"location":"projects/js-course/javascript-fundamentals-simplified/#declaration-and-initialization","title":"Declaration and Initialization","text":"<ul> <li>Use <code>let</code> to declare variables whose value can change.</li> <li>Use <code>const</code> to declare constants whose value cannot change.</li> </ul> <p>Example:</p> <pre><code>let numberOfEpisodes = 9;\nconst pi = 3.14;\n</code></pre>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#data-types","title":"Data Types","text":"<ul> <li>Data types include <code>number</code>, <code>string</code>, <code>boolean</code>, <code>object</code>, <code>undefined</code>, <code>null</code>, etc.</li> <li>Use <code>typeof</code> to check the type of a variable.</li> </ul>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#numbers","title":"Numbers","text":"<p>In JavaScript, integers are represented as the <code>number</code> data type. You can declare a variable and assign it an integer value as follows:</p> <pre><code>let a = 15;\nconsole.log(typeof a);  // Displays \"number\"\n</code></pre>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#booleans","title":"Booleans","text":"<p>Booleans represent true or false values in JavaScript. You can declare a variable and assign it a boolean value like this:</p> <pre><code>let userIsSignedIn = true;\nconsole.log(typeof userIsSignedIn); // Displays \"boolean\"\n</code></pre>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#strings","title":"Strings","text":"<p>Strings are sequences of characters enclosed in single or double quotes. You can concatenate two strings using the <code>+</code> operator. Here's an example:</p> <pre><code>let nom = 'Ag';\nlet prenom = 'He';\nconsole.log(typeof nom); // Displays \"string\"\nconsole.log(nom + prenom); // Displays \"AgHe\"\n</code></pre>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#outputs","title":"Outputs","text":"<ul> <li>Use <code>console.log()</code> to display messages in the console.</li> <li>Use <code>alert()</code> to display messages in a dialog box.</li> </ul> <p>Example:</p> <pre><code>console.log(5);\nlet yyy = \"me\";\nconsole.log(\"retdy\" + \" \" + yyy);\nalert('some things'); // Display a message in a dialog box\n</code></pre>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#objects","title":"Objects","text":"<p>JavaScript objects behave similarly to dictionaries in Python.</p>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#syntax","title":"Syntax","text":"<p>Objects in JavaScript are declared using curly braces <code>{}</code> and consist of key-value pairs. Keys are always strings. Each value can be an instance of any other type.</p> <pre><code>let mybook = { \n    title: 'Allah is not obliged',\n    author: 'Un Mec',\n    numberOfPages: 200,\n    isAvailable: true\n};\n</code></pre>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#accessing-data","title":"Accessing Data","text":"<p>You can access data in an object using dot notation (<code>.</code>) or square bracket notation (<code>[]</code>). For example:</p>  using dot notation (<code>.</code>) using bracket notation (<code>[]</code>) <pre><code>let titre = mybook.title;\nlet auteur = mybook.author;\nlet isdisponible = mybook.isAvailable;\n</code></pre> <pre><code>let titre = mybook[\"title\"];\nlet auteur = mybook[\"author\"];\nlet isdisponible = mybook[\"isAvailable\"];\n</code></pre> <p>Note that the keys are case-sensitive. So <code>mybook.title</code> is not the same as <code>mybook.Title</code>.</p>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#classes","title":"Classes","text":"<p>In JavaScript, classes allow you to define blueprints for creating objects with properties and methods.</p>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#class-definition","title":"Class Definition","text":"<p>Classes are defined using the <code>class</code> keyword followed by the class name.</p> <pre><code>class Book {\n    constructor(title, author, numberOfPages) {\n        this.title = title;\n        this.author = author;\n        this.numberOfPages = numberOfPages;\n    }\n}\n</code></pre>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#creating-instances","title":"Creating Instances","text":"<p>You can create new instances of a class using the <code>new</code> keyword followed by the class name and passing the required parameters to the constructor.</p> <pre><code>let aBook = new Book(\"le moi int\u00e9rieur\", \"Hermann\", 250);\n</code></pre>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#methods","title":"Methods","text":"<p>Methods can be defined within the class using normal function syntax. Here's an example:</p> method example <pre><code>class BankAccount {\n    constructor(owner, balance) {\n        this.owner = owner;\n        this.balance = balance;\n    }\n\n    showBalance() {\n        console.log(\"Solde: \" + this.balance + \" EUR\");\n    }\n\n    deposit(amount) {\n        console.log(\"D\u00e9p\u00f4t de \" + amount + \" EUR\");\n        this.balance += amount;\n        this.showBalance();\n    }\n\n    withdraw(amount) {\n        if (amount &gt; this.balance) {\n            console.log(\"Retrait refus\u00e9 !\");\n        } else {\n            console.log(\"Retrait de \" + amount + \" EUR\");\n            this.balance -= amount;\n            this.showBalance();\n        }\n    }\n}\n</code></pre>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#instantiation-and-method-invocation","title":"Instantiation and Method Invocation","text":"<p>You can create an instance of a class and then call its methods as shown below:</p> <pre><code>const newAccount = new BankAccount(\"Will Alexander\", 500);\nnewAccount.showBalance(); // Prints \"Solde: 500 EUR\" to the console\n</code></pre> <p>This code creates a new bank account with an initial balance of 500 EUR and then displays its balance using the <code>showBalance</code> method.</p>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#arrays-and-maps","title":"Arrays and Maps","text":"<p>In JavaScript, arrays are used to store collections of elements. They are zero-indexed, meaning the index of the first element is 0.</p>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#array-definition","title":"Array Definition","text":"<p>Arrays can be defined without initialization, or directly initialized with elements.</p> <pre><code>// Definition without initialization\nlet myList;\n\n// Definition with direct initialization\nlet guests = [];\nlet invitedPeoples = [\"Sarah\", \"Jean-Pierre\", \"Claude\"];\n</code></pre>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#accessing-elements","title":"Accessing Elements","text":"<p>You can access elements in an array using square brackets and the index of the element. Remember that array indexing starts from 0.</p> <pre><code>let guest1 = invitedPeoples[0]; // Accesses the first element\nlet guest3 = invitedPeoples[2]; // Accesses the third element\n</code></pre>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#array-length","title":"Array Length","text":"<p>You can determine the length of an array using the <code>length</code> property.</p> <pre><code>console.log(invitedPeoples.length); // Prints the length of the array\n</code></pre>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#array-operations","title":"Array Operations","text":"<p>Arrays support various operations like adding elements, removing elements, and inserting elements.</p> <ul> <li>To add an element to the end of an array, you can use the <code>push</code> method.</li> <li>To remove the last element from an array, you can use the <code>pop</code> method.</li> <li>To add an element to the beginning of an array, you can use the <code>unshift</code> method.</li> </ul>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#maps","title":"Maps","text":"<p>Maps in JavaScript are similar to arrays but are unordered collections. They do not allow duplicates, and you can check if an element exists in a map.</p> <p>These are some basic operations you can perform with arrays in JavaScript.</p>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#comparison-operators","title":"Comparison Operators","text":"<p>Comparison operators in JavaScript allow you to compare values and determine the relationship between them. These operators are commonly used in conditional statements like <code>if</code> and loops.</p>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#common-comparison-operators","title":"Common Comparison Operators","text":"<ul> <li><code>&lt;</code>    Less than</li> <li><code>&lt;=</code>   Less than or equal to</li> <li><code>&gt;</code>    Greater than</li> <li><code>&gt;=</code>   Greater than or equal to</li> <li><code>==</code>   Equal to (checks value only)</li> <li><code>!=</code>   Not equal to (checks value only)</li> <li><code>===</code>  Equal to (checks value and type)</li> <li><code>!==</code>  Not equal to (checks value and type)</li> </ul>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#logical-operators","title":"Logical Operators","text":"<p>Logical operators allow you to combine multiple conditions in a single statement.</p> <ul> <li><code>&amp;&amp;</code>   Logical AND: Returns <code>true</code> if both conditions are true</li> <li><code>||</code>   Logical OR: Returns <code>true</code> if at least one condition is true</li> <li><code>!</code>    Logical NOT: Negates the result, returns <code>true</code> if the condition is false</li> </ul> <p>comparison evaluation</p> <pre><code>let a = 5;\nlet b = 10;\n\nif (a &lt; b) {\n    console.log(\"a is less than b\");\n} else {\n    console.log(\"a is greater than or equal to b\");\n}\n</code></pre> <p>In this example, the condition <code>a &lt; b</code> evaluates to <code>true</code>, so the message \"a is less than b\" will be logged to the console.</p>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#control-structures","title":"Control Structures","text":""},{"location":"projects/js-course/javascript-fundamentals-simplified/#ifelse","title":"If/else","text":"<p>In JavaScript, <code>if</code>, <code>else if</code>, and <code>else</code> statements are used to execute blocks of code based on conditions.</p>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#basic-structure","title":"Basic Structure","text":"<p>The basic structure of an <code>if</code> statement is as follows:</p> <pre><code>if (condition) {\n    // Code to execute if the condition is true\n} else {\n    // Code to execute if the condition is false\n}\n</code></pre> example of using an <code>if</code> statement <pre><code>let userLoggedIn = true;\n\nif (userLoggedIn) {\n    console.log(\"User logged in!\");\n} else {\n    console.log(\"Alert, intruder!\");\n}\n</code></pre> <p>In this example, if the <code>userLoggedIn</code> variable is <code>true</code>, the message \"User logged in!\" will be logged to the console; otherwise, \"Alert, intruder!\" will be logged.</p> multiple conditions with if / else if / else <p>You can also use <code>else if</code> statements to test multiple conditions. Here's an example:</p> <pre><code>if (numberOfGuests == numberOfSeats) {\n    // All seats are occupied\n} else if (numberOfGuests &lt; numberOfSeats) {\n    // Allow more guests\n} else {\n    // Do not allow new guests\n}\n</code></pre> <p>In this example, if the number of guests equals the number of seats, the first block of code will execute. If the number of guests is less than the number of seats, the second block of code will execute. Otherwise, the third block of code will execute.</p>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#conditions-evaluated-as-true-or-false","title":"Conditions Evaluated as True or False","text":"Conditions Evaluated as True Conditions Evaluated as False <p>Conditions that can be evaluated as true in an <code>if</code> statement include:</p> <ul> <li>Numbers that are not zero</li> <li>Strings that are not empty</li> <li>Boolean <code>true</code></li> <li>Objects (including arrays and functions)</li> </ul> <p>Conditions that are evaluated as false in an <code>if</code> statement include:</p> <ul> <li>Number <code>0</code></li> <li>Empty string <code>''</code></li> <li>Boolean <code>false</code></li> <li><code>null</code></li> <li><code>undefined</code></li> <li><code>NaN</code></li> </ul>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#switchcase","title":"Switch/Case","text":"<p>The <code>switch</code> statement in JavaScript allows you to execute different blocks of code based on different conditions. It's particularly useful when you have a single value that you want to compare to multiple possible variants.</p>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#basic-structure_1","title":"Basic Structure","text":"<p>The basic structure of a <code>switch</code> statement looks like this:</p> <pre><code>switch (expression) {\n    case value1:\n        // Code to execute if expression === value1\n        break;\n    case value2:\n        // Code to execute if expression === value2\n        break;\n    default:\n        // Code to execute if expression doesn't match any case\n}\n</code></pre> swich case example <pre><code>let guestType = \"star\";\nlet vipStatus;\n\nswitch (guestType) {\n    case \"artist\":\n        vipStatus = \"Normal\";\n        break;\n    case \"star\":\n        vipStatus = \"Important\";\n        break;\n    default:\n        vipStatus = \"None\";\n}\n\nconsole.log(\"VIP status:\", vipStatus);\n</code></pre> <p>In this example, if the <code>guestType</code> is \"artist\", the <code>vipStatus</code> will be set to \"Normal\". If the <code>guestType</code> is \"star\", the <code>vipStatus</code> will be set to \"Important\". Otherwise, the <code>vipStatus</code> will be set to \"None\".</p>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#handling-unknown-values","title":"Handling Unknown Values","text":"<p>The <code>default</code> case is used to handle values that don't match any of the specified cases. This is useful for providing a fallback option or handling unexpected input.</p> Importance of Break <p>It's important to include <code>break</code> statements after each case block to prevent fall-through behavior, where execution continues to the next case block regardless of whether the condition is met. Here's an example illustrating the importance of <code>break</code>:</p> <pre><code>let vipStatus = \"\";\nlet guest = {\n    name: \"Sarah Kate\",\n    age: 21,\n    ticket: true,\n    guestType: \"artist\"\n};\n\nswitch (guest.guestType) {\n    case \"artist\":\n        vipStatus = \"Normal\";\n    case \"star\":\n        vipStatus = \"Important\";\n        break;\n    case \"presidential\":\n        vipStatus = \"Mega-important\";\n        break;\n    default:\n        vipStatus = \"None\";\n}\n</code></pre> <p>In this example, the <code>vipStatus</code> variable is erroneously assigned \"Normal\" because the <code>break</code> statement is missing after the <code>\"artist\"</code> case. Without the <code>break</code>, execution falls through to the <code>\"star\"</code> case, causing <code>vipStatus</code> to be overwritten with \"Important\". To avoid this, ensure that each case block ends with a <code>break</code> statement.</p>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#for-loops","title":"For Loops","text":"<p>In JavaScript, <code>for</code> loops are used to iterate over elements in an array or perform a specific action a certain number of times.</p>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#basic-structure_2","title":"Basic Structure","text":"<p>The basic structure of a <code>for</code> loop is as follows:</p> <pre><code>for (initialization; condition; increment/decrement) {\n    // Code to execute for each iteration\n}\n</code></pre> <p>for loop: basic usage example</p> <pre><code>for (let i = 0; i &lt; numberOfPassengers; i++) {\n    console.log(\"Passenger boarded!\");\n}\n</code></pre>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#for-loop-on-arrays","title":"For Loop on Arrays","text":"<p>This section provides an overview of <code>for</code> loops in JavaScript, including examples of both <code>for...in</code> and <code>for...of</code> loops and their respective use cases</p> Example 1: Using <code>for...in</code> Loop <p>The <code>for...in</code> loop iterates over the enumerable properties of an object, such as the indices of an array. Here's an example:</p> <pre><code>const passengers = [\n    \"Will Alexander\",\n    \"Sarah Kate\",\n    \"Audrey Simon\",\n    \"Tao Perkington\"\n]\n\nfor (let i in passengers) {\n    console.log(\"Boarding passenger: \" + passengers[i]);\n}\n</code></pre> Example 2: Using <code>for...of</code> Loop <p>The <code>for...of</code> loop is used to iterate over iterable objects, such as arrays. It provides a more concise syntax compared to the <code>for...in</code> loop. Here's an example:</p> <pre><code>const passengers = [\n    \"Will Alexander\",\n    \"Sarah Kate\",\n    \"Audrey Simon\",\n    \"Tao Perkington\"\n]\n\nfor (let passenger of passengers) {\n    console.log(\"Boarding passenger: \" + passenger);\n}\n</code></pre> <p>In both examples, each passenger's name is logged to the console, indicating that they are boarding the vehicle or entering some other context.</p>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#while-loops","title":"While Loops","text":"<p>In JavaScript, <code>while</code> loops are used to execute a block of code repeatedly as long as a specified condition is true.</p>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#basic-structure_3","title":"Basic Structure","text":"<p>The basic structure of a <code>while</code> loop is as follows:</p> <pre><code>while (condition) {\n    // Code to execute as long as the condition is true\n}\n</code></pre> Example: Using a While Loop <p>Here's an example of using a <code>while</code> loop to repeatedly perform a task until a condition is no longer true:</p> <pre><code>let seatsLeft = 10;\nlet passengersStillToBoard = 8;\nlet passengersBoarded = 0;\n\nwhile (seatsLeft &gt; 0 &amp;&amp; passengersStillToBoard &gt; 0) {\n    passengersBoarded++; // A passenger boards\n    passengersStillToBoard--; // Decrease the number of passengers still to board\n    seatsLeft--; // Decrease the number of seats left\n}\n\nconsole.log(passengersBoarded); // Logs the total number of passengers boarded\n</code></pre> <p>In this example, the loop continues as long as there are seats available (<code>seatsLeft &gt; 0</code>) and passengers still to board (<code>passengersStillToBoard &gt; 0</code>). Each iteration of the loop represents a passenger boarding the vehicle or entering some other context.</p> <p>The loop terminates when either there are no more seats available or there are no more passengers to board.</p>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#error-handling","title":"Error Handling","text":"<p>In JavaScript, error handling is crucial for managing unexpected situations or errors that may occur during code execution. There are various types of errors, and JavaScript provides mechanisms like <code>try</code> and <code>catch</code> to handle them gracefully.</p> Types of Errors <ol> <li> <p>Syntax Errors: These errors occur when there is a mistake in the syntax of the code, such as missing semicolons <code>;</code>, brackets <code>{}</code>, or incorrect expressions.</p> </li> <li> <p>Logical Errors: Logical errors happen when the code executes but produces unexpected results due to incorrect logic or reasoning in the code.</p> </li> <li> <p>Runtime Errors: Runtime errors occur during the execution of the program, typically caused by factors such as incorrect user input, resource unavailability, or unexpected behavior of external dependencies.</p> </li> <li> <p>Reference Errors: Reference errors occur when trying to access variables or functions that are not declared or out of scope.</p> </li> <li> <p>Type Errors: Type errors occur when an operation is performed on a value of the wrong type, such as using a method on a non-object or passing incorrect arguments to a function.</p> </li> <li> <p>Range Errors: Range errors occur when trying to access an invalid index of an array or perform an invalid operation within a certain numeric range.</p> </li> </ol>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#exception-handling-with-trycatch","title":"Exception Handling with Try/Catch","text":"<p>JavaScript provides the <code>try</code> and <code>catch</code> blocks for handling exceptions and managing errors effectively.</p>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#basic-structure_4","title":"Basic Structure","text":"<pre><code>try {\n    // Code that may cause an error\n} catch (error) {\n    // Handling the error\n}\n</code></pre> <p>In this structure:</p> <ul> <li>The <code>try</code> block contains the code that might throw an error.</li> <li>If an error occurs within the <code>try</code> block, control is transferred to the <code>catch</code> block.</li> <li>The <code>catch</code> block is responsible for handling the error. It receives the error object as a parameter, which can be used to identify and respond to the error appropriately.</li> </ul> Example <pre><code>try {\n    // Attempting to execute code that may throw an error\n    let result = 10 / 0; // This will cause a division by zero error\n    console.log(result); // This line will not execute due to the error\n} catch (error) {\n    // Handling the error\n    console.error(\"An error occurred:\", error.message);\n}\n</code></pre> <p>In this example, if a division by zero error occurs within the <code>try</code> block, the control will be transferred to the <code>catch</code> block. The <code>catch</code> block then handles the error by logging a descriptive message to the console.</p> <p>Error handling with <code>try</code> and <code>catch</code> is an essential aspect of writing robust JavaScript code, ensuring that your applications can gracefully handle unexpected errors and provide a better user experience.</p>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#functions","title":"Functions","text":"<p>In JavaScript, functions are blocks of reusable code that can be invoked (called) to perform a specific task. They play a crucial role in organizing and structuring code, making it easier to manage and maintain.</p>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#basic-syntax","title":"Basic Syntax","text":"<p>Functions in JavaScript can be defined using different syntaxes, including arrow functions and traditional function declarations.</p> <code>Arrow Functions:</code> <code>Traditional Function Declarations:</code> <p>Arrow functions are a concise way to write functions in JavaScript. They are commonly used for short, single-expression functions.</p> <pre><code>const sum = (a, b) =&gt; {\n    return a + b;\n};\n</code></pre> <p>Traditional function declarations use the <code>function</code> keyword to define functions.</p> <pre><code>function sum(a, b) {\n    return a + b;\n}\n</code></pre>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#function-invocation","title":"Function Invocation","text":"<p>Once a function is defined, it can be invoked (called) by its name, followed by parentheses containing any arguments required by the function.</p>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#example","title":"Example","text":"<pre><code>const result = sum(3, 4);\nconsole.log(result); // Output: 7\n</code></pre> Example 1: Function to Calculate Sum <pre><code>const sum = (number1, number2) =&gt; {\n    const result = number1 + number2;\n    return result;\n}\n\nconsole.log(sum(4, 7)); // Output: 11\n</code></pre> <p>In this example, the <code>sum</code> function takes two parameters <code>number1</code> and <code>number2</code>, calculates their sum, and returns the result.</p> Example 2: Function to Calculate Average Rating <pre><code>const calculateAverageRating = (ratings) =&gt; {\n    if (ratings.length === 0) {\n        return 0;\n    }\n\n    let sum = 0;\n    for (let rating of ratings) {\n        sum += rating;\n    }\n\n    return sum / ratings.length;\n}\n\nconst tauRatings = [5, 4, 5, 5, 1, 2];\nconst colinRatings = [5, 5, 5, 4, 5];\n\nconst tauAverage = calculateAverageRating(tauRatings);\nconst colinAverage = calculateAverageRating(colinRatings);\n\nconsole.log(tauAverage); // Output: 3.6666666666666665\nconsole.log(colinAverage); // Output: 4.8\n</code></pre> <p>In this example, the <code>calculateAverageRating</code> function calculates the average rating based on the provided array of ratings.</p> <p>Additional Notes</p> <ul> <li>It's common to use arrow functions (<code>=&gt;</code>) for defining functions in modern JavaScript.</li> <li>Constants declared with <code>const</code> are used to define functions to prevent accidental reassignment.</li> <li>Functions can take parameters and return values, making them versatile and powerful tools for organizing code.</li> </ul> <p>Functions are essential for structuring JavaScript code and promoting code reuse and maintainability.</p>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#scope-of-variables","title":"Scope of Variables","text":"<p>In JavaScript, the scope of a variable determines where the variable is accessible within the code. Understanding variable scope is essential for writing maintainable and bug-free code.</p>"},{"location":"projects/js-course/javascript-fundamentals-simplified/#variable-declaration","title":"Variable Declaration","text":"<p>When declaring variables in JavaScript, it's recommended to use <code>let</code> or <code>const</code> to define variables.</p> <ul> <li> <p><code>let</code>: Variables declared with <code>let</code> have block scope, meaning they are only accessible within the block (enclosed by <code>{}</code>) in which they are defined, as well as any nested blocks (e.g., inside an <code>if</code> statement or loop) within that block.</p> </li> <li> <p><code>const</code>: Constants declared with <code>const</code> also have block scope and cannot be reassigned. They follow the same scoping rules as variables declared with <code>let</code>.</p> </li> <li> <p><code>var</code>: Variables declared with <code>var</code> have function scope. This means they are accessible throughout the entire function in which they are defined, regardless of block boundaries.</p> </li> </ul> Example <pre><code>{\n    let localVar = 'I am a local variable';\n    console.log(localVar); // Output: 'I am a local variable'\n}\n\nconsole.log(localVar); // Throws ReferenceError: localVar is not defined\n</code></pre> <p>In this example, the variable <code>localVar</code> is declared using <code>let</code> inside a block. It is accessible within that block but not outside of it. Attempting to access <code>localVar</code> outside the block results in a <code>ReferenceError</code>.</p> Avoid using <code>var</code> for variable declaration whenever possible <p>Using <code>let</code> and <code>const</code> for variable declaration helps prevent accidental variable hoisting and unintended side effects. It also promotes better code readability and maintenance by clearly defining the scope of variables.</p> <p>Avoid using <code>var</code> for variable declaration whenever possible, as it can lead to unexpected behavior due to its function scope and variable hoisting characteristics.</p> <p>Understanding variable scope is crucial for writing clean, predictable, and bug-free JavaScript code.</p> <p>Variables declared within a function are only accessible within that function, unless they are declared using the <code>var</code> keyword. Using <code>let</code> or <code>const</code> ensures that variables have block scope, making them accessible only within the block they are defined in.</p> <p>Best Practices</p> <ul> <li>Use meaningful variable names.</li> <li>Indent your code properly to make it readable.</li> <li>Comment your code to explain its functionality.</li> <li>Avoid ambiguous variable names.</li> </ul> <p>Additional Notes</p> <ul> <li>Remember to use semicolons to terminate statements.</li> <li>Use <code>{}</code> to define code blocks.</li> <li>Be mindful of the scope of variables when using <code>let</code>, <code>const</code>, and <code>var</code>.</li> <li>Always handle exceptions to prevent unexpected behavior.</li> <li>Utilize console methods such as <code>console.error()</code> for error messages.</li> </ul> Resources <p>Here are some useful resources to enhance your JavaScript skills and productivity:</p> <ul> <li> <p>ECMAScript Compatibility Table: A comprehensive table detailing the compatibility of various ECMAScript features across different JavaScript engines and environments.</p> </li> <li> <p>JS Bin: An online tool for quickly experimenting with and testing JavaScript code snippets. It provides a live-coding environment with a built-in console for immediate feedback.</p> </li> <li> <p>W3Schools JavaScript Tutorial: W3Schools offers a comprehensive and beginner-friendly JavaScript tutorial covering all fundamental concepts, syntax, and features of the language.</p> </li> <li> <p>OpenClassrooms JavaScript Course (English) or in French: OpenClassrooms provides interactive JavaScript courses suitable for beginners and intermediate learners. These courses cover topics ranging from basic syntax to advanced JavaScript programming techniques.</p> </li> </ul> <p>These resources offer valuable insights, tutorials, and tools to help you master JavaScript programming and become a proficient developer.</p>"},{"location":"projects/llm-course/tutoriel/","title":"Tutoriel","text":""},{"location":"projects/llm-course/tutoriel/#guide-perplexity-ai-tutoriels-pour-tous","title":"Guide Perplexity AI : Tutoriels pour Tous","text":"<p>Bienvenue dans ce guide pas \u00e0 pas pour utiliser Perplexity AI, pens\u00e9 pour tous : jeunes, moins jeunes, d\u00e9butants ou curieux. Chaque section propose des instructions simples, des ic\u00f4nes pour rep\u00e9rer les actions, et des indications pour ins\u00e9rer vos propres captures d\u2019\u00e9cran (pensez \u00e0 ajouter des balises <code>alt</code> pour l\u2019accessibilit\u00e9).</p>"},{"location":"projects/llm-course/tutoriel/#1-se-connecter-a-son-compte-perplexity","title":"1. Se connecter \u00e0 son compte Perplexity","text":"<p>Ic\u00f4ne \u00e0 utiliser :</p> <p></p>"},{"location":"projects/llm-course/tutoriel/#etapes-simples","title":"\u00c9tapes simples","text":"<ul> <li>Rendez-vous sur www.perplexity.ai depuis votre navigateur<sup>1</sup>.</li> <li>Entrez votre adresse e-mail et votre mot de passe, ou choisissez la connexion via Google<sup>1</sup>.</li> <li>Si vous avez oubli\u00e9 votre mot de passe, cliquez sur \u00ab Mot de passe oubli\u00e9 ? \u00bb et suivez les instructions re\u00e7ues par e-mail<sup>1</sup>.</li> <li>Apr\u00e8s connexion, vous acc\u00e9dez \u00e0 l\u2019interface principale.</li> </ul>"},{"location":"projects/llm-course/tutoriel/#2-discuter-avec-lia-et-comprendre-les-sources","title":"2. Discuter avec l\u2019IA et comprendre les sources","text":""},{"location":"projects/llm-course/tutoriel/#comment-discuter-avec-lia","title":"Comment discuter avec l\u2019IA","text":"<ul> <li>Tapez votre question dans la barre centrale, puis appuyez sur Entr\u00e9e ou cliquez sur l\u2019ic\u00f4ne fl\u00e8che<sup>2</sup><sup>3</sup>.</li> <li>L\u2019IA r\u00e9pond en quelques secondes, avec une r\u00e9ponse claire et facile \u00e0 lire<sup>2</sup><sup>3</sup>.</li> <li>Astuce : Posez vos questions comme \u00e0 une personne r\u00e9elle, et soyez pr\u00e9cis si possible<sup>3</sup>.</li> </ul> <p>![[resp.png]]</p>"},{"location":"projects/llm-course/tutoriel/#comprendre-les-sources","title":"Comprendre les sources","text":"<ul> <li>Sous chaque r\u00e9ponse, vous verrez des num\u00e9ros (ex : <sup>2</sup>, <sup>3</sup>, <sup>4</sup>) : ce sont les sources utilis\u00e9es par l\u2019IA<sup>3</sup>.</li> <li>Cliquez sur un num\u00e9ro pour afficher la source compl\u00e8te dans une nouvelle fen\u00eatre<sup>3</sup>.</li> </ul>"},{"location":"projects/llm-course/tutoriel/#3-retrouver-ses-anciennes-discussions","title":"3. Retrouver ses anciennes discussions","text":""},{"location":"projects/llm-course/tutoriel/#ou-retrouver-vos-conversations-passees","title":"O\u00f9 retrouver vos conversations pass\u00e9es","text":"<ul> <li>Sur la page principale, cherchez l'icone Accueil sur la gauche \ud83d\udd0d.</li> <li>Cherchez le menu Biblioth\u00e8que ou Historique sur la gauche<sup>2</sup><sup>5</sup>. </li> <li>Cliquez dessus pour voir la liste de toutes vos discussions pr\u00e9c\u00e9dentes. Vous pouvez en rouvrir une en un clic<sup>2</sup><sup>5</sup>.</li> <li>Attention : Si vous supprimez une discussion, elle ne pourra pas \u00eatre r\u00e9cup\u00e9r\u00e9e<sup>6</sup><sup>5</sup>.</li> </ul>"},{"location":"projects/llm-course/tutoriel/#4-faire-une-recherche-sur-le-web","title":"4. Faire une recherche sur le web","text":""},{"location":"projects/llm-course/tutoriel/#comment-proceder","title":"Comment proc\u00e9der","text":"<ul> <li>Tapez votre question dans la barre de recherche<sup>2</sup><sup>3</sup>.</li> <li>Perplexity va chercher sur Internet et vous affiche une r\u00e9ponse avec des sources \u00e0 jour<sup>2</sup><sup>3</sup>.</li> <li>Pour approfondir, cliquez sur les num\u00e9ros de sources en bas de la r\u00e9ponse<sup>3</sup>.</li> </ul>"},{"location":"projects/llm-course/tutoriel/#5-faire-une-recherche-academique","title":"5. Faire une recherche acad\u00e9mique","text":""},{"location":"projects/llm-course/tutoriel/#etapes-a-suivre","title":"\u00c9tapes \u00e0 suivre","text":"<ul> <li>Creer une nouvelle discussion en cliquant sur l'icone \u00e0 gauche \u2795. Puis, vous serez sur une nouvelle page</li> <li>Dans la barre de recherche, cliquez sur le menu d\u00e9roulant (souvent nomm\u00e9 \u00ab Mode \u00bb ou \u00ab Focus \u00bb)<sup>3</sup><sup>7</sup>. </li> <li>S\u00e9lectionnez Academique pour cibler les articles scientifiques, th\u00e8ses, etc.<sup>3</sup><sup>7</sup>.</li> <li>Posez votre question : l\u2019IA vous donnera des r\u00e9ponses issues de publications reconnues, avec des r\u00e9f\u00e9rences claires<sup>7</sup><sup>8</sup><sup>9</sup>.</li> </ul>"},{"location":"projects/llm-course/tutoriel/#6-ajouter-un-fichier-a-une-discussion","title":"6. Ajouter un fichier \u00e0 une discussion","text":"<p>Ic\u00f4ne \u00e0 utiliser : Capture d\u2019\u00e9cran sugg\u00e9r\u00e9e : Bouton \u00ab + Attacher \u00bb (alt=\"Bouton pour ajouter un fichier \u00e0 la discussion\")</p> <p></p>"},{"location":"projects/llm-course/tutoriel/#comment-faire","title":"Comment faire","text":"<ul> <li>Lors d\u2019une nouvelle question, cliquez sur le bouton + Attacher (\ud83d\udcce) \u00e0 droite de la barre de recherche<sup>2</sup><sup>10</sup><sup>11</sup>.</li> <li>S\u00e9lectionnez votre fichier (PDF, image, texte, etc.)<sup>10</sup><sup>11</sup><sup>12</sup>. </li> <li>L\u2019IA analysera le contenu du fichier pour r\u00e9pondre \u00e0 vos questions sp\u00e9cifiques<sup>11</sup><sup>12</sup>.</li> <li>Limite : 25 Mo par fichier, jusqu\u2019\u00e0 4 fichiers \u00e0 la fois pour les utilisateurs standards<sup>11</sup>. </li> </ul>"},{"location":"projects/llm-course/tutoriel/#7-exporter-une-discussion-ou-un-fichier","title":"7. Exporter une discussion ou un fichier","text":"<p>Ic\u00f4ne \u00e0 utiliser : Capture d\u2019\u00e9cran sugg\u00e9r\u00e9e : Menu d\u2019export (alt=\"Menu pour exporter ou enregistrer une discussion\")</p>"},{"location":"projects/llm-course/tutoriel/#exporter-une-discussion","title":"Exporter une discussion","text":"<ul> <li>Dans une discussion, cherchez en haut \u00e0 droite, l'icone <code>\u22ef</code> l\u2019option Exporter ou utilisez une extension comme \u00ab Save my Chatbot \u00bb pour sauvegarder la conversation au format Markdown ou PDF<sup>13</sup><sup>14</sup><sup>15</sup>. </li> <li>Pratique pour garder une trace ou partager vos \u00e9changes avec d\u2019autres personnes<sup>16</sup><sup>14</sup>.</li> <li>Vous pouvez \u00e9galement g\u00e9n\u00e9rer un lien et partager avec d'autres personnes </li> </ul>"},{"location":"projects/llm-course/tutoriel/#8-generer-les-fichiers-avec-la","title":"8. G\u00e9n\u00e9rer les fichiers avec l'A","text":"<p>Dans une question, vous pouvez demander \u00e0 l'AI de g\u00e9n\u00e9rer un fichier</p> <p></p> <p>Pour trouver les fichier, cliquer sur l'onglet \"Etapes\" </p> <p></p>"},{"location":"projects/llm-course/tutoriel/#tutoriels-avances-perplexity-rapports-approfondis-personnalisation-collaboration","title":"Tutoriels Avanc\u00e9s Perplexity : Rapports approfondis, Personnalisation, Collaboration","text":""},{"location":"projects/llm-course/tutoriel/#8-creer-des-rapports-de-recherche-approfondis-deep-research","title":"8. Cr\u00e9er des rapports de recherche approfondis (Deep Research)","text":""},{"location":"projects/llm-course/tutoriel/#a-quoi-ca-sert","title":"\u00c0 quoi \u00e7a sert\u202f?","text":"<p>Le mode \u00ab\u202fRecherche approfondie\u202f\u00bb (Deep Research) permet d\u2019obtenir des rapports d\u00e9taill\u00e9s et structur\u00e9s sur des sujets complexes, en quelques minutes seulement. L\u2019IA analyse des dizaines de sources, synth\u00e9tise l\u2019information et vous livre un document clair, id\u00e9al pour des \u00e9tudes, des dossiers ou des analyses pouss\u00e9es<sup>20</sup><sup>21</sup><sup>22</sup>.</p>"},{"location":"projects/llm-course/tutoriel/#etapes-simples_1","title":"\u00c9tapes simples","text":"<ul> <li>Acc\u00e9dez \u00e0 Perplexity et connectez-vous \u00e0 votre compte.</li> <li>Saisissez votre question dans la barre de recherche, puis s\u00e9lectionnez le mode \u00ab\u202fRecherche approfondie\u202f\u00bb ou \u00ab\u202fDeep Research\u202f\u00bb.</li> <li>Lancez la recherche : l\u2019IA va effectuer plusieurs recherches li\u00e9es, analyser les sources et r\u00e9diger un rapport complet. </li> <li>Consultez le rapport : vous pouvez voir le raisonnement \u00e9tape par \u00e9tape, les sources utilis\u00e9es et la synth\u00e8se finale. </li> <li>Exportez ou partagez le rapport\u202f: cliquez sur l\u2019option d\u2019export (PDF, Word, page partag\u00e9e) pour sauvegarder ou envoyer votre analyse \u00e0 d\u2019autres personnes<sup>20</sup><sup>21</sup><sup>22</sup>.</li> </ul> <p>Astuce\u202f: Ce mode est id\u00e9al pour les \u00e9tudes de march\u00e9, les analyses techniques ou la r\u00e9daction de m\u00e9moires universitaires. Par exemple, vous pouvez exporter les r\u00e9sultats sous la forme d'un blog </p> <p></p>"},{"location":"projects/llm-course/tutoriel/#9-personnaliser-les-resultats-de-lia","title":"9. Personnaliser les r\u00e9sultats de l\u2019IA","text":""},{"location":"projects/llm-course/tutoriel/#pourquoi-personnaliser","title":"Pourquoi personnaliser\u202f?","text":"<p>Adapter Perplexity \u00e0 vos besoins permet d\u2019obtenir des r\u00e9ponses plus pertinentes et adapt\u00e9es \u00e0 votre profil, vos centres d\u2019int\u00e9r\u00eat ou votre style de communication<sup>20</sup><sup>23</sup><sup>24</sup>.</p>"},{"location":"projects/llm-course/tutoriel/#comment-faire_1","title":"Comment faire\u202f?","text":"<ul> <li>Acc\u00e9dez aux param\u00e8tres\u202f: cliquez sur votre photo de profil (g\u00e9n\u00e9ralement en bas \u00e0 droite de l\u2019interface).  </li> <li>Remplissez votre profil\u202f:</li> <li>Centres d\u2019int\u00e9r\u00eat\u202f: indiquez vos sujets favoris (sciences, cuisine, voyages\u2026).</li> <li>Style de r\u00e9ponse\u202f: choisissez entre un ton formel ou d\u00e9contract\u00e9, des r\u00e9ponses courtes ou d\u00e9taill\u00e9es.</li> <li>Langue et localisation\u202f: s\u00e9lectionnez la langue de pr\u00e9f\u00e9rence et, si besoin, votre r\u00e9gion.</li> <li>Objectifs\u202f: pr\u00e9cisez ce que vous attendez de Perplexity (aide aux devoirs, veille professionnelle, etc.).</li> <li>Enregistrez vos pr\u00e9f\u00e9rences\u202f: ces r\u00e9glages s\u2019appliqueront \u00e0 toutes vos futures recherches, rendant l\u2019IA plus efficace et personnalis\u00e9e<sup>20</sup><sup>23</sup><sup>24</sup>.</li> </ul> <p>Astuce\u202f: Les abonn\u00e9s Pro peuvent aussi choisir le mod\u00e8le d\u2019IA utilis\u00e9 pour chaque recherche (ex\u202f: GPT-4o, Claude 3), pour encore plus de personnalisation<sup>24</sup>.</p>"},{"location":"projects/llm-course/tutoriel/#10-collaborer-dans-les-espaces-de-travail-spaces","title":"10. Collaborer dans les espaces de travail (Spaces)","text":""},{"location":"projects/llm-course/tutoriel/#quest-ce-quun-espace-de-travail","title":"Qu\u2019est-ce qu\u2019un espace de travail\u202f?","text":"<p>Un \u00ab\u202fSpace\u202f\u00bb est un espace collaboratif o\u00f9 vous pouvez organiser vos recherches, partager des fichiers et travailler \u00e0 plusieurs sur des projets communs (\u00e9tudes, travaux de groupe, veille d\u2019\u00e9quipe\u2026)<sup>25</sup>.</p>"},{"location":"projects/llm-course/tutoriel/#etapes-pour-collaborer","title":"\u00c9tapes pour collaborer","text":"<ul> <li>Dans le menu \u00e0 gauche, clickez sur le bouton espace </li> <li>Cr\u00e9ez un espace\u202f: cliquez sur \u00ab\u202fCr\u00e9er un espace\u202f\u00bb dans la barre lat\u00e9rale, donnez-lui un nom et une description.  </li> <li>Ajoutez des membres\u202f: invitez d\u2019autres utilisateurs par e-mail ou lien de partage.</li> <li>Centralisez vos documents\u202f: importez notes, fichiers, liens et ressources dans l\u2019espace.</li> <li>Posez des questions \u00e0 l\u2019IA\u202f: chaque membre peut interroger l\u2019IA sur les documents partag\u00e9s, g\u00e9n\u00e9rer des synth\u00e8ses, ou demander des plans de projet.</li> <li>Organisez la collaboration\u202f: cr\u00e9ez des guides d\u2019\u00e9tude, des calendriers partag\u00e9s, divisez les t\u00e2ches et suivez l\u2019avancement du groupe<sup>25</sup>.</li> </ul> <p>Exemples d\u2019usages\u202f:</p> <ul> <li>Pr\u00e9parer un expos\u00e9 ou un rapport \u00e0 plusieurs.</li> <li>Centraliser les ressources d\u2019une \u00e9quipe projet.</li> <li> <p>G\u00e9n\u00e9rer des outils d\u2019\u00e9tude personnalis\u00e9s pour une classe ou un groupe d\u2019apprentissage<sup>25</sup>.</p> </li> <li> <p>Cliquez sur Cr\u00e9er un espace dans la barre lat\u00e9rale<sup>17</sup><sup>19</sup>. </p> </li> <li> <p>Donnez un nom \u00e0 l\u2019espace, ajoutez une description et, si besoin, des instructions personnalis\u00e9es pour l\u2019IA<sup>17</sup><sup>19</sup>.</p> </li> <li> <p>Ajoutez vos fichiers et commencez \u00e0 poser des questions ou \u00e0 collaborer avec d\u2019autres utilisateurs<sup>18</sup><sup>19</sup>.</p> </li> </ul> \u2042"},{"location":"projects/llm-course/tutoriel/#guide-pour-utiliser-notebooklm","title":"Guide pour utiliser NotebookLM","text":"<p>Guide pour utiliser NotebookLM : Simplifiez vos t\u00e2ches et organisez vos informations</p> <p>Bienvenue dans ce guide d\u00e9taill\u00e9 pour utiliser NotebookLM ! Que vous soyez un jeune \u00e9tudiant, un professionnel exp\u00e9riment\u00e9 ou simplement curieux, ce guide vous aidera \u00e0 ma\u00eetriser NotebookLM pour all\u00e9ger vos t\u00e2ches administratives, stocker et classer vos recherches, et bien plus encore. Chaque section propose des instructions simples, des ic\u00f4nes pour rep\u00e9rer les actions, et des indications pour ins\u00e9rer vos propres captures d'\u00e9cran (pensez \u00e0 ajouter des balises <code>alt</code> pour l'accessibilit\u00e9).</p>"},{"location":"projects/llm-course/tutoriel/#1-creer-un-nouveau-carnet-de-notes-notebook","title":"1. Cr\u00e9er un nouveau carnet de notes (Notebook)","text":""},{"location":"projects/llm-course/tutoriel/#icone-a-utiliser","title":"Ic\u00f4ne \u00e0 utiliser","text":""},{"location":"projects/llm-course/tutoriel/#etapes-simples_2","title":"\u00c9tapes simples","text":"<ul> <li>Rendez-vous sur notebooklm.google depuis votre navigateur.</li> <li>Cliquez sur le bouton \"+ Nouveau carnet de notes\" (ou \"New notebook\")<sup>26</sup>.</li> <li>Donnez un nom \u00e0 votre carnet de notes (ex: \"Projet Alpha\", \"Recettes de cuisine\", \"Recherches historiques\").</li> <li>Cliquez sur \"Cr\u00e9er\" pour ouvrir votre nouveau carnet<sup>26</sup>.</li> </ul>"},{"location":"projects/llm-course/tutoriel/#2-ajouter-des-sources-a-votre-carnet-de-notes","title":"2. Ajouter des sources \u00e0 votre carnet de notes","text":"<p>NotebookLM est puissant gr\u00e2ce aux sources que vous lui fournissez. Il peut analyser diff\u00e9rents types de documents.</p>"},{"location":"projects/llm-course/tutoriel/#icone-a-utiliser_1","title":"Ic\u00f4ne \u00e0 utiliser","text":""},{"location":"projects/llm-course/tutoriel/#comment-ajouter-des-sources","title":"Comment ajouter des sources","text":"<ul> <li> <p>Dans votre carnet de notes, cherchez le bouton \"Ajouter des sources\" (g\u00e9n\u00e9ralement \"+ Add sources\" ou une ic\u00f4ne similaire).</p> </li> <li> <p>Vous avez plusieurs options pour ajouter des sources<sup>27</sup>:</p> </li> <li> <p>Depuis votre ordinateur : Cliquez sur \"T\u00e9l\u00e9charger un fichier\" et s\u00e9lectionnez des documents PDF, Google Docs, ou d'autres fichiers texte<sup>27</sup>.</p> </li> <li>Depuis Google Drive : Connectez votre compte Google Drive pour importer des documents directement<sup>27</sup>.</li> <li>Copier-coller du texte : Collez du texte directement dans la zone pr\u00e9vue \u00e0 cet effet (par exemple, un article de blog, une transcription)<sup>27</sup>.</li> <li> <p>Coller une URL de site web : Ajoutez l'URL d'une page web pour que NotebookLM en tire des informations<sup>27</sup>.</p> </li> <li> <p>Une fois les sources ajout\u00e9es, NotebookLM les analyse. Cela peut prendre quelques instants selon la taille et le nombre de fichiers<sup>27</sup>.</p> </li> </ul>"},{"location":"projects/llm-course/tutoriel/#3-discuter-avec-lia-et-obtenir-des-resumes","title":"3. Discuter avec l'IA et obtenir des r\u00e9sum\u00e9s","text":"<p>Une fois vos sources ajout\u00e9es, vous pouvez interroger NotebookLM pour obtenir des informations, des r\u00e9sum\u00e9s ou des id\u00e9es.</p>"},{"location":"projects/llm-course/tutoriel/#icone-a-utiliser_2","title":"Ic\u00f4ne \u00e0 utiliser","text":""},{"location":"projects/llm-course/tutoriel/#comment-discuter-avec-lia_1","title":"Comment discuter avec l'IA","text":"<ul> <li>Apr\u00e8s avoir ajout\u00e9 vos sources, vous verrez une barre de discussion ou une zone de texte en bas ou sur le c\u00f4t\u00e9 de l'interface<sup>28</sup>.</li> <li>Tapez votre question ou votre demande dans cette barre. Par exemple : \"R\u00e9sumez les points cl\u00e9s du document sur le projet X\", \"Quels sont les avantages de la solution Y mentionn\u00e9s dans ce fichier ?\", ou \"G\u00e9n\u00e9rez un plan d'action bas\u00e9 sur ces notes\"<sup>28</sup>.</li> <li>Appuyez sur Entr\u00e9e ou cliquez sur l'ic\u00f4ne d'envoi.</li> <li>L'IA g\u00e9n\u00e9rera une r\u00e9ponse bas\u00e9e uniquement sur les sources que vous avez fournies, ce qui garantit la pertinence et la fiabilit\u00e9 des informations<sup>28</sup>.</li> </ul>"},{"location":"projects/llm-course/tutoriel/#comprendre-les-citations-sources","title":"Comprendre les citations (Sources)","text":"<ul> <li>Les r\u00e9ponses de NotebookLM incluent des citations (par exemple, <code>[1]</code>, <code>[2]</code>) qui renvoient aux sources sp\u00e9cifiques que vous avez ajout\u00e9es<sup>29</sup>.</li> <li>Cliquez sur ces citations pour voir quelle partie de votre document a \u00e9t\u00e9 utilis\u00e9e pour g\u00e9n\u00e9rer la r\u00e9ponse. Cela vous aide \u00e0 v\u00e9rifier l'information et \u00e0 trouver rapidement la section originale<sup>29</sup>.</li> </ul>"},{"location":"projects/llm-course/tutoriel/#4-organiser-vos-notes-et-informations","title":"4. Organiser vos notes et informations","text":"<p>NotebookLM vous aide \u00e0 structurer vos pens\u00e9es et vos recherches.</p>"},{"location":"projects/llm-course/tutoriel/#icone-a-utiliser_3","title":"Ic\u00f4ne \u00e0 utiliser","text":""},{"location":"projects/llm-course/tutoriel/#comment-organiser-vos-informations","title":"Comment organiser vos informations","text":"<ul> <li>Ajouter des notes : Vous pouvez prendre des notes directement dans votre carnet, \u00e0 c\u00f4t\u00e9 de vos sources ou des discussions avec l'IA<sup>30</sup>.</li> <li>Cr\u00e9er des r\u00e9sum\u00e9s automatiques : Demandez \u00e0 l'IA de g\u00e9n\u00e9rer des r\u00e9sum\u00e9s pour des sections sp\u00e9cifiques de vos documents ou pour l'ensemble de votre carnet<sup>30</sup>.</li> <li>G\u00e9n\u00e9rer des plans et des id\u00e9es : Utilisez l'IA pour structurer des plans d'expos\u00e9, des \u00e9bauches d'articles, ou pour brainstormer des id\u00e9es en fonction de vos sources<sup>30</sup>.</li> <li>Mettre en \u00e9vidence les passages importants : S\u00e9lectionnez du texte dans vos sources et demandez \u00e0 NotebookLM de vous en extraire des points cl\u00e9s ou de les r\u00e9sumer<sup>31</sup>.</li> </ul>"},{"location":"projects/llm-course/tutoriel/#5-utiliser-les-guides-generes-par-lia","title":"5. Utiliser les guides g\u00e9n\u00e9r\u00e9s par l'IA","text":"<p>NotebookLM peut cr\u00e9er des \"Guides\" bas\u00e9s sur vos sources pour vous aider \u00e0 explorer des sujets en profondeur.</p>"},{"location":"projects/llm-course/tutoriel/#icone-a-utiliser_4","title":"Ic\u00f4ne \u00e0 utiliser","text":""},{"location":"projects/llm-course/tutoriel/#comment-utiliser-les-guides","title":"Comment utiliser les guides","text":"<ul> <li>Dans certains cas, NotebookLM peut sugg\u00e9rer de cr\u00e9er un guide bas\u00e9 sur le contenu de votre carnet<sup>32</sup>.</li> <li>Cliquez sur \"Cr\u00e9er un guide\" ou une option similaire.</li> <li>L'IA organisera les informations de vos sources en sections th\u00e9matiques, facilitant la navigation et la compr\u00e9hension des sujets complexes<sup>32</sup>.</li> <li>Les guides sont particuli\u00e8rement utiles pour \u00e9tudier, pr\u00e9parer des pr\u00e9sentations ou explorer de nouvelles th\u00e9matiques.</li> </ul>"},{"location":"projects/llm-course/tutoriel/#6-partager-vos-carnets-de-notes","title":"6. Partager vos carnets de notes","text":"<p>Vous pouvez partager vos carnets de notes avec d'autres personnes pour collaborer ou simplement diffuser vos recherches.</p>"},{"location":"projects/llm-course/tutoriel/#icone-a-utiliser_5","title":"Ic\u00f4ne \u00e0 utiliser","text":""},{"location":"projects/llm-course/tutoriel/#comment-partager-un-carnet","title":"Comment partager un carnet","text":"<ul> <li>Dans votre carnet de notes, cherchez l'ic\u00f4ne de partage (souvent un symbole de fl\u00e8che ou de trois points connect\u00e9s) ou le bouton \"Partager\"<sup>33</sup>.</li> <li>Vous pourrez alors g\u00e9n\u00e9rer un lien partageable.</li> <li>Attention : Assurez-vous de bien comprendre les options de partage (lecture seule, modification) avant d'envoyer le lien<sup>33</sup>.</li> </ul>"},{"location":"projects/llm-course/tutoriel/#7-gerer-et-retrouver-vos-carnets","title":"7. G\u00e9rer et retrouver vos carnets","text":""},{"location":"projects/llm-course/tutoriel/#ou-retrouver-vos-carnets-passes","title":"O\u00f9 retrouver vos carnets pass\u00e9s","text":"<ul> <li>Sur la page principale de NotebookLM, vous verrez la liste de tous vos carnets de notes.</li> <li>Cliquez sur un carnet pour l'ouvrir et reprendre votre travail l\u00e0 o\u00f9 vous l'avez laiss\u00e9.</li> <li>Vous pouvez rechercher des carnets par nom si vous en avez beaucoup.</li> </ul>"},{"location":"projects/llm-course/tutoriel/#attention","title":"Attention","text":"<ul> <li>Si vous supprimez un carnet de notes, toutes les sources et discussions qu'il contient seront d\u00e9finitivement effac\u00e9es<sup>34</sup>.</li> </ul> <p>En utilisant NotebookLM, vous transformez vos documents en une base de connaissances interactive et personnalis\u00e9e, pr\u00eate \u00e0 r\u00e9pondre \u00e0 toutes vos questions et \u00e0 vous aider \u00e0 organiser vos id\u00e9es.</p> <p>\\&lt;div style=\"text-align: center\"&gt;\u2042\\&lt;/div&gt;</p> <ol> <li> <p>https://infopreneur.blog/perplexity-login/ \u21a9\u21a9\u21a9</p> </li> <li> <p>https://reglo.ai/comment-utiliser-perplexity-ai/ \u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>https://gitmind.com/fr/perplexity-ai-guide-utilisation.html \u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>https://www.blogdumoderateur.com/perplexity-guide-bien-utiliser-moteur-recherche-assiste-ia/ \u21a9</p> </li> <li> <p>https://www.perplexity.ai/help-center/fr/articles/10354769-qu-est-ce-qu-un-fil-de-discussion \u21a9\u21a9\u21a9</p> </li> <li> <p>https://www.reddit.com/r/perplexity_ai/comments/1dyx9uy/need_help_with_managing_threads/?tl=fr \u21a9</p> </li> <li> <p>https://www.perplexity.ai/fr/hub/getting-started \u21a9\u21a9\u21a9</p> </li> <li> <p>https://www.perplexity.ai/help-center/fr/articles/10354975-commencer-a-utiliser-perplexity \u21a9</p> </li> <li> <p>https://www.elephorm.com/formation/code-data/no-code/maitriser-perplexity-ai-optimisez-vos-recherches-avec-lia \u21a9</p> </li> <li> <p>https://www.elephorm.com/formation/code-data/perplexity/maitriser-perplexity-ai-optimisez-vos-recherches-avec-lia/integration-dimages-et-de-fichiers-dans-vos-recherches \u21a9\u21a9</p> </li> <li> <p>https://app.studyraid.com/fr/read/18469/680245/importer-des-fichiers-pdf \u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>https://www.perplexity.ai/hub/faq/how-does-file-upload-work \u21a9\u21a9</p> </li> <li> <p>https://www.reddit.com/r/perplexity_ai/comments/16n2g3d/i_made_an_extension_to_export_perplexity_threads/?tl=fr \u21a9</p> </li> <li> <p>https://chromewebstore.google.com/detail/save-my-chatbot-ai-conver/agklnagmfeooogcppjccdnoallkhgkod \u21a9\u21a9</p> </li> <li> <p>https://github.com/Hugo-COLLIN/SaveMyPhind-conversation-exporter \u21a9</p> </li> <li> <p>https://www.perplexity.ai/hub/blog/introducing-perplexity-deep-research \u21a9</p> </li> <li> <p>https://www.elephorm.com/formation/code-data/perplexity/maitriser-perplexity-ai-optimisez-vos-recherches-avec-lia/explorez-la-fonctionnalite-space-de-perplexity \u21a9\u21a9</p> </li> <li> <p>https://anthemcreation.com/intelligence-artificielle/perplexity-spaces-recherche-en-ligne-et-locale-premium/ \u21a9</p> </li> <li> <p>https://action-commerciale.com/espace-perplexity-hub-projets/ \u21a9\u21a9\u21a9</p> </li> <li> <p>https://reglo.ai/comment-utiliser-perplexity-ai/ \u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>https://www.elephorm.com/formation/code-data/perplexity/maitriser-perplexity-ai-optimisez-vos-recherches-avec-lia/exploration-de-la-recherche-approfondie-avec-perplexity \u21a9\u21a9</p> </li> <li> <p>https://cedric.fm/guide-perplexity-redaction/ \u21a9\u21a9</p> </li> <li> <p>https://www.perplexity.ai/help-center/fr/articles/10354948-comment-dois-je-remplir-la-section-profil-de-mes-parametres \u21a9\u21a9</p> </li> <li> <p>https://www.blogdumoderateur.com/perplexity-guide-bien-utiliser-moteur-recherche-assiste-ia/ \u21a9\u21a9\u21a9</p> </li> <li> <p>https://www.perplexity.ai/fr/hub/blog/a-student-s-guide-to-using-perplexity-spaces \u21a9\u21a9\u21a9</p> </li> <li> <p>Google Workspace Updates: Build your own AI assistant with NotebookLM \u21a9\u21a9</p> </li> <li> <p>Google Chrome : Utiliser NotebookLM \u21a9\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>Google Workspace Updates: Get to know NotebookLM, a Google AI experiment \u21a9\u21a9\u21a9</p> </li> <li> <p>NotebookLM - Your AI-powered research assistant \u21a9\u21a9</p> </li> <li> <p>Google Workspace Updates: NotebookLM makes your research easier with AI-generated notes, outlines, and summaries \u21a9\u21a9\u21a9</p> </li> <li> <p>How to use NotebookLM - YouTube \u21a9</p> </li> <li> <p>Google Blog: How NotebookLM helps you learn anything \u21a9\u21a9</p> </li> <li> <p>How to Share NotebookLM Projects - YouTube \u21a9\u21a9</p> </li> <li> <p>NotebookLM support page \u21a9</p> </li> </ol>"},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/category/dev/","title":"dev","text":""},{"location":"blog/category/linux/","title":"linux","text":""},{"location":"blog/category/cli-tools/","title":"cli-tools","text":""},{"location":"blog/category/docker/","title":"docker","text":""},{"location":"blog/category/git/","title":"git","text":""},{"location":"blog/category/markdown/","title":"markdown","text":""},{"location":"blog/category/programming/","title":"programming","text":""},{"location":"blog/category/python/","title":"python","text":""},{"location":"blog/category/type-checking/","title":"type-checking","text":""},{"location":"blog/category/data-validation/","title":"data-validation","text":""},{"location":"blog/category/tools-comparison/","title":"tools-comparison","text":""},{"location":"blog/category/beginners/","title":"beginners","text":""},{"location":"blog/category/ai-tools/","title":"ai tools","text":""},{"location":"blog/category/remote-access/","title":"remote-access","text":""},{"location":"blog/category/ssh/","title":"ssh","text":""},{"location":"blog/category/security/","title":"Security","text":""},{"location":"blog/category/database-management/","title":"database-management","text":""},{"location":"blog/category/mysql/","title":"mysql","text":""},{"location":"blog/category/postgresql/","title":"postgresql","text":""},{"location":"blog/category/mongodb/","title":"mongodb","text":""},{"location":"blog/category/numerical-analysis/","title":"numerical-analysis","text":""},{"location":"blog/category/data-visualization/","title":"data-visualization","text":""},{"location":"blog/category/documentation/","title":"documentation","text":""},{"location":"blog/category/packaging/","title":"packaging","text":""},{"location":"blog/category/sphinx/","title":"sphinx","text":""},{"location":"blog/category/blog/","title":"Blog","text":""},{"location":"blog/category/authentication/","title":"Authentication","text":""},{"location":"blog/category/best-practices/","title":"Best Practices","text":""},{"location":"blog/category/osx/","title":"OSX","text":""},{"location":"blog/category/containers/","title":"containers","text":""},{"location":"blog/category/deployment/","title":"deployment","text":""},{"location":"blog/category/rdp/","title":"RDP","text":""},{"location":"blog/category/windows/","title":"windows","text":""},{"location":"blog/category/poetry/","title":"poetry","text":""},{"location":"blog/category/dependency-management/","title":"dependency-management","text":""},{"location":"blog/category/frameworks/","title":"frameworks","text":""},{"location":"blog/category/web/","title":"web","text":""},{"location":"blog/category/fullstack/","title":"fullstack","text":""},{"location":"blog/category/laravel/","title":"laravel","text":""},{"location":"blog/category/pwa/","title":"pwa","text":""},{"location":"blog/category/php/","title":"php","text":""},{"location":"blog/category/code-quality/","title":"code-quality","text":""},{"location":"blog/category/logging/","title":"logging","text":""},{"location":"blog/category/package-manager/","title":"package-manager","text":""},{"location":"blog/category/project-management/","title":"project-management","text":""},{"location":"blog/category/frontend/","title":"frontend","text":""},{"location":"blog/category/github-pages/","title":"github-pages","text":""},{"location":"blog/category/domain-migration/","title":"domain-migration","text":""},{"location":"blog/category/flask/","title":"flask","text":""},{"location":"blog/category/troubleshooting/","title":"troubleshooting","text":""},{"location":"blog/category/version-control/","title":"version-control","text":""},{"location":"blog/category/fastapi/","title":"fastapi","text":""},{"location":"blog/category/nodejs/","title":"nodejs","text":""},{"location":"blog/category/nextjs/","title":"nextjs","text":""},{"location":"blog/category/prisma/","title":"prisma","text":""},{"location":"blog/category/clerck/","title":"clerck","text":""},{"location":"blog/category/microservices/","title":"microservices","text":""},{"location":"blog/category/gprc/","title":"gprc","text":""},{"location":"blog/category/networking/","title":"networking","text":""},{"location":"blog/category/file-handling/","title":"file-handling","text":""},{"location":"blog/category/mkdocs/","title":"mkdocs","text":""},{"location":"blog/category/recovery/","title":"recovery","text":""},{"location":"blog/category/data/","title":"data","text":""},{"location":"blog/category/huggingface/","title":"huggingface","text":""},{"location":"blog/category/streamlit/","title":"streamlit","text":""},{"location":"blog/category/nginx/","title":"nginx","text":""},{"location":"blog/category/conversion-tools/","title":"conversion-tools","text":""},{"location":"blog/page/2/","title":"Blog","text":""},{"location":"blog/page/3/","title":"Blog","text":""},{"location":"blog/page/4/","title":"Blog","text":""},{"location":"blog/page/5/","title":"Blog","text":""},{"location":"blog/archive/2024/page/2/","title":"2024","text":""},{"location":"blog/archive/2023/page/2/","title":"2023","text":""},{"location":"blog/archive/2023/page/3/","title":"2023","text":""},{"location":"blog/category/beginners/page/2/","title":"beginners","text":""},{"location":"blog/category/deployment/page/2/","title":"deployment","text":""},{"location":"blog/category/python/page/2/","title":"python","text":""}]}